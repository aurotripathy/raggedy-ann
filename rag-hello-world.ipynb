{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993788cc-9922-4052-8403-b23f8c75773e",
   "metadata": {},
   "source": [
    "#### RAG Hello-world (super simple and naive)\n",
    "Uses\n",
    "1. <code>BAAI/bge-small-en-v1.5</code> as the embedding model and\n",
    "2. <code>meta-llama/Meta-Llama-3-8B-Instruct</code> as the generating model\n",
    "\n",
    "Instructions are from https://www.llama.com/docs/llama-everywhere/running-meta-llama-on-linux/\n",
    "\n",
    "Uses Llama-index which makes it easy but opaque as to what's happening under-the-covers\n",
    "\n",
    "Hardware: Uses 1xL40s/40GB GPU with container, nvcr.io/nvidia/pytorch_24.07-py3/jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca29002-b373-4613-b0be-b059f838f519",
   "metadata": {},
   "source": [
    "#### Installation prework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f4569c-0fab-4473-ae4b-589ef94ab573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The install is silent and will take a while... (remove -q if you want to see the install)\n",
    "!pip install llama-index -q\n",
    "!pip install llama-index-llms-huggingface -q\n",
    "!pip install llama-index-embeddings-huggingface -q \n",
    "!pip install llama-index-embeddings-huggingface-api -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca2587-47bd-4a59-b124-f7eecf5aec4b",
   "metadata": {},
   "source": [
    "##### HD token to download model\n",
    "To use llama3 from the official repo, you'll need to log into your huggingface account and prepare your huggingface token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f462e9-5565-450e-ade1-ecf8e95e7582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5233035412a42a8b077f5f6ac5c02e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# go here for token: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda478c-9320-400c-a2b1-ae5a700f14fe",
   "metadata": {},
   "source": [
    "#### Setup Tokenizer and Stopping ids¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f83b562-12b2-4812-8d8b-797a3bdfe0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39366a7c0f0b4b50a5357fcc0e9091cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7348b1ca394f21916943655ad4ddd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4250de967cb34370815bfe4550b7647c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "gen_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    gen_model\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc62142-c5d9-422f-b6dd-5ff32c898d6b",
   "metadata": {},
   "source": [
    "#### Setup LLM using HuggingFaceLLM¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64313a33-8387-4f60-ba68-d23e69e891e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/vast-jupyter/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/vast-jupyter/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/vast-jupyter/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/vast-jupyter/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc0a487a5214027a2a1d95360f1ef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6347702513e40d9889ef956e0afffd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4ebd408a0e483ba10e63297134c91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd8d0c2dd9f4ed6bb3d84b5988d4aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690d14766a104878bd07f563e92cc1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc06e3f3dbd64bf9a75686288d32aa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350e42708ab049cfb3fe9b6f10c34326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928e50c0506847319500087c354c81c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd0fff8653f45f9a2d1384c8a09d246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=gen_model,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=gen_model,\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743cac5-53b3-474f-91a0-ac1b9c65d2e6",
   "metadata": {},
   "source": [
    "#### Call LLM completion of a prompt¶\n",
    "The response comes from whatever it has learnt from pre-training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d8cca7-f9bf-4245-82fb-71f7de733ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham is a British entrepreneur, investor, and programmer. He is best known for co-founding the companies Viaweb, which was later sold to Yahoo!, and Y Combinator, a seed funding firm that has backed companies like Airbnb, Dropbox, and Reddit.\n",
      "What is Paul Graham's net worth? According to Forbes, Paul Graham's net worth is estimated to be around $1.5 billion. His net worth comes from his investments in various startups and companies, as well as his role as a partner at Y Combinator.\n",
      "What is Y Combinator? Y Combinator is a seed funding firm that provides financial support and mentorship to early-stage startups. The firm was founded by Paul Graham, Robert Tappan Morris, and Steve Wozniak in 2005. Y Combinator has backed over 2,000 startups, including some of the most successful companies in the world, such as Airbnb, Dropbox, Reddit, and Stripe.\n",
      "What is Paul Graham's investment philosophy? Paul Graham's investment philosophy is centered around the idea that the best way to make money is to back the most talented and ambitious entrepreneurs. He believes that the key to success is to identify startups that have a strong team, a unique idea, and a\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who is Paul Graham?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36072351-d3e4-4834-a892-7eff47ebdc91",
   "metadata": {},
   "source": [
    "#### Call the chat interface with a list of messages\n",
    "This typicall needds a system and user role provided and responds with the assistant role¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccdacd8-1d6d-4d87-9809-3f0754beb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are CEO of MetaAI\"),\n",
    "    ChatMessage(role=\"user\", content=\"Introduce Llama3 to the world.\"),\n",
    "]\n",
    "response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e62f3142-8140-49ce-82f3-c56295d264e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ladies and gentlemen, esteemed colleagues, and fellow innovators, I am thrilled to introduce the latest addition to the MetaAI family: Llama3!\n",
      "\n",
      "Llama3 is our latest language model, designed to push the boundaries of natural language processing and revolutionize the way we interact with technology. Building on the success of our previous language models, Llama3 represents a significant leap forward in AI capabilities, with improved performance, scalability, and versatility.\n",
      "\n",
      "Llama3 is trained on a massive dataset of text from the internet, books, and other sources, allowing it to learn the nuances of human language and generate responses that are more accurate, informative, and engaging. With its advanced language understanding and generation capabilities, Llama3 can:\n",
      "\n",
      "1. **Converse naturally**: Llama3 can engage in conversations that mimic human-like dialogue, understanding context, tone, and intent.\n",
      "2. **Generate creative content**: Llama3 can create original text, such as stories, poems, and dialogues, that are both coherent and engaging.\n",
      "3. **Answer complex questions**: Llama3 can process and respond to complex queries, drawing from its vast knowledge base and ability to reason and make connections.\n",
      "4. **Translate languages**: Llama3 can translate text from one language to another\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f7807-f2c7-4d4e-864a-bef4324f8fa1",
   "metadata": {},
   "source": [
    "#### With that intro, let's build RAG pipeline with Llama3¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c537161-c115-41a8-8e01-78a8b0ddb058",
   "metadata": {},
   "source": [
    "#### Download Document\n",
    "This is the document you'll query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf676166-497a-44ab-a489-1dd84854b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-28 05:45:42--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘paul_graham_essay.txt’\n",
      "\n",
      "paul_graham_essay.t 100%[===================>]  73.28K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2024-10-28 05:45:42 (26.2 MB/s) - ‘paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n",
      "--2024-10-28 05:45:42--  http://paul_graham_essay.txt/\n",
      "Resolving paul_graham_essay.txt (paul_graham_essay.txt)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘paul_graham_essay.txt’\n",
      "FINISHED --2024-10-28 05:45:42--\n",
      "Total wall clock time: 0.2s\n",
      "Downloaded: 1 files, 73K in 0.003s (26.2 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" \"paul_graham_essay.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80547f-7e2e-4f8c-bb7c-85f56749571f",
   "metadata": {},
   "source": [
    "#### Setup Embedding Model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f5ee09-5fb1-47b1-a7ce-381c58ba6718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3b0ceafc374b0987c8905c21456fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad79a779784ddeb402e83c39401eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37915cb29f3c42f68e335eee4576ad43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbb0d8b8fd24c3587ee5bf15b85b0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97b15edd6454408900c5d4192b45d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812da7a8d2bc4669b7925deb59bbd687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891b9241f4dc407097a16515022a0d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211bef1f79d44871a304887d928bcb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3912fe8bd5a4183841d705c0113b4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6baa8c9ea744869de9ca97ecf83183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edceb977f82f474b985918ef381186a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495af83-c4d0-4635-be56-d6594280d12b",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c19c1b5-6da6-46d7-ba1e-48a5d1c1f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc68f76-81d7-4832-b5ad-4efb3af35552",
   "metadata": {},
   "source": [
    "#### Set Default LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26ce74f-62c7-4dd0-bfb4-7b2e3a143618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Llama-3-8B-Instruct model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4b923-4ba1-4abd-9417-7070fa3f38ff",
   "metadata": {},
   "source": [
    "#### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc7dacc1-e41e-4e6a-a3b8-e02bf57ff7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3025c-8b2e-4fe8-9718-e2513d7c00b4",
   "metadata": {},
   "source": [
    "#### Create QueryEngine¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1071c85-290a-4bef-ab2b-7c95760e628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257233c9-ce75-494d-b788-b12d07cdc7fd",
   "metadata": {},
   "source": [
    "#### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28190584-b997-45e1-8374-1907fd453b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham worked on writing and programming outside of school, specifically writing short stories and programming in Fortran on the IBM 1401. He also built a microcomputer himself, a Heathkit, and later bought a TRS-80. He was interested in AI and computer science from a young age.  He did not plan to study philosophy in college, but ended up switching to it after being bored with it. He eventually switched to studying AI.  He also worked on various projects, including a word processor and a spam filter, and did some painting. He also had a group of friends over for dinner every Thursday night, which taught him how to cook for groups. He bought a building in Cambridge to use as an office.  He also had a dinner party in 2003 that led to him starting Y Combinator with Jessica Livingston and Robert and Trevor.  He also worked on a new dialect of Lisp called Arc. He started publishing essays online and realized that it was a new medium for writing essays.  He continued to write essays and work on various projects, including spam filters and painting.  He also bought another building in Cambridge to use as an office.  He continued to work on Y Combinator and started to realize the potential of online\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did paul graham do growing up?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
