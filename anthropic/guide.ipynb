{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "LLMs excels at a wide range of tasks, but struggle with queries specific to your unique business context. This is where Retrieval Augmented Generation (RAG) becomes invaluable. RAG enables the LLM to leverage your internal knowledge bases or customer support documents, significantly enhancing its ability to answer domain-specific questions. Enterprises are increasingly building RAG applications to improve workflows in customer support, Q&A over internal company documents, financial & legal analysis, and much more.\n",
    "\n",
    "In this guide, we'll demonstrate how to build and optimize a RAG system using the Anthropic documentation as our knowledge base. We'll walk you through:\n",
    "\n",
    "1. Embeddings are from the `intfloat/multilingual-e5-large-instruct` model, where input is truncated to at most 512 tokens\n",
    "2. In-memory vector database class is from Anthropic\n",
    "3. Building a robust evaluation suite. We'll go beyond 'vibes' based evals and show you how to measure the retrieval pipeine & end to end performance independently\n",
    "4. Implementing advanced techniques to improve RAG including summary indexing and re-ranking with Claude.\n",
    "\n",
    "Through a series of targeted improvements, we achieved significant performance gains on the following metrics compared to a basic RAG pipeline (we'll explain what all these metrics *mean* in a bit)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1) Setup\n",
    "2) Level 1 - Basic RAG\n",
    "3) Building an Evaluation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need a few libraries and models:\n",
    "\n",
    "1. `intfloat/multilingual-e5-large-instruct` to generate high quality embeddings\n",
    "2. `openai`,  the LLM for generation\n",
    "4. `pandas`, `numpy`, `matplotlib`, and `scikit-learn` for data manipulation and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## silent setup (-q)\n",
    "!pip install openai -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downlaod the Embeddings model and run a quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df2436f5daf40e4aa68e9de8defdac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0964eb956ab7489f870c640b8239792e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6423c439b4164422932c1d8914782b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b93aae4b364f1c87b6965d1a5480cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75f73b9621f4c02a326521769dba9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abda5fdd0e434a1cac53a05beeb3aaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56a13d2c3694140b2e67b26fcf7fe4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f083748a8c74176a9a2f3a8789f1242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572a9ef86b5c4798951a7cda36c8ae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c52a3b9f67b43b584bb51b709bb3185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91.92853546142578, 67.58030700683594], [70.38142395019531, 92.1330795288086]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, '南瓜的家常做法')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"\n",
    "]\n",
    "input_texts = queries + documents\n",
    "\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n",
    "\n",
    "embeddings = model.encode(input_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())\n",
    "# [[91.92853546142578, 67.5802993774414], [70.38143157958984, 92.13307189941406]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Vector DB Class\n",
    "\n",
    "In this example, we're using an in-memory vector DB, but for a production application, you may want to use a hosted solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_vec_db_in_memory(self, data):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_vec_db()\n",
    "            return\n",
    "\n",
    "        texts = [f\"Heading: {item['chunk_heading']}\\n\\n Chunk Text:{item['text']}\" for item in data]\n",
    "        self._embed_and_store(texts, data)\n",
    "        self.save_db()\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            # query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            query_embedding = model.encode(query)\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_vec_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_vec_in_memory to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 - Basic RAG\n",
    "\n",
    "To get started, we'll set up a basic RAG pipeline using a bare bones approach. This is sometimes called 'Naive RAG' by many in the industry. A basic RAG pipeline includes the following 3 steps:\n",
    "\n",
    "1) Chunk documents by heading - containing only the content from each subheading\n",
    "\n",
    "2) Embed each document\n",
    "\n",
    "3) Use Cosine similarity to retrieve documents in order to answer query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database loaded and saved.\n",
      "([{'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases', 'chunk_heading': 'Creating Test Cases', 'text': 'Creating Test Cases\\n\\n\\nWhen you first access the Evaluation screen, you’ll see a single row:\\n\\nTo add more test cases:\\nClick the ‘Add Test Case’ button.\\nFill in values for each variable in your prompt.\\nRepeat to create multiple scenarios.\\nHere’s an example of a populated Evaluation screen with several test cases:\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n'}, 'similarity': np.float32(0.8731841)}, {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#accessing-the-evaluate-feature', 'chunk_heading': 'Accessing the Evaluate Feature', 'text': 'Accessing the Evaluate Feature\\n\\n\\nTo get started with the Evaluation tool:\\nOpen the Anthropic Console and navigate to the prompt editor.\\nAfter composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n'}, 'similarity': np.float32(0.8708937)}, {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#2-develop-your-test-cases', 'chunk_heading': '2. Develop your test cases', 'text': '2. Develop your test cases\\n\\n\\nTo run your classification evaluation, you will need test cases to run it on. Take a look at our guide to developing test cases.\\n'}, 'similarity': np.float32(0.8676537)}], '\\nCreating Test Cases\\n\\n\\nWhen you first access the Evaluation screen, you’ll see a single row:\\n\\nTo add more test cases:\\nClick the ‘Add Test Case’ button.\\nFill in values for each variable in your prompt.\\nRepeat to create multiple scenarios.\\nHere’s an example of a populated Evaluation screen with several test cases:\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\n\\nAccessing the Evaluate Feature\\n\\n\\nTo get started with the Evaluation tool:\\nOpen the Anthropic Console and navigate to the prompt editor.\\nAfter composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n\\n\\n2. Develop your test cases\\n\\n\\nTo run your classification evaluation, you will need test cases to run it on. Take a look at our guide to developing test cases.\\n\\n')\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. Access the Evaluation screen, where you will see a single test case row.\n",
      "2. Click the ‘Add Test Case’ button to create additional test cases.\n",
      "3. Fill in the values for each variable in your prompt for each new test case.\n",
      "4. Repeat the process to create as many scenarios as needed.\n",
      "\n",
      "If you need to update your original prompt text, you can re-run the entire evaluation suite against the new prompt to see how the changes affect performance across all test cases.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "\n",
    "def retrieve_similar(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n{chunk['text']}\\n\"\n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):\n",
    "    # query = \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context(query, context):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load the Anthropic documentation\n",
    "with open('data/anthropic_docs.json', 'r') as f:\n",
    "    anthropic_docs = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "db = VectorDB(\"anthropic_docs\")\n",
    "db.load_vec_db_in_memory(anthropic_docs)\n",
    "\n",
    "# test\n",
    "query = \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\"\n",
    "context = \"\"'Creating Test Cases\\n\\n\\nWhen you first access the Evaluation screen, you’ll see a single row:\\n\\nTo add more test cases:\\nClick the ‘Add Test Case’ button.\\nFill in values for each variable in your prompt.\\nRepeat to create multiple scenarios.\\nHere’s an example of a populated Evaluation screen with several test cases:\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n'\n",
    "print(retrieve_similar(query, db))\n",
    "print(answer_query_from_context(query, context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Setup\n",
    "\n",
    "When evaluating RAG applications, it's critical to evaluate the performance of the retrieval system and end to end system separately.\n",
    "\n",
    "We synthetically generated an evaluation dataset consisting of 100 samples which include the following:\n",
    "- A question\n",
    "- Chunks from our docs which are relevant to that question. This is what we expect our retrieval system to retrieve when the question is asked\n",
    "- A correct answer to the question.\n",
    "\n",
    "This is a relatively challenging dataset. Some of our questions require synthesis between more than one chunk in order to be answered correctly, so it's important that our system can load in more than one chunk at a time. You can inspect the dataset by opening `evaluation/docs_evaluation_dataset.json`\n",
    "\n",
    "Run the next cell to see a preview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 4 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1d6210b8\",\n",
      "    \"question\": \"What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/claude-for-sheets#why-use-claude-for-sheets\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts#how-to-chain-prompts\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=4):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db)\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'response_text:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  16%|█▌        | 16/100 [00:00<00:02, 35.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.4333, Avg Recall: 0.7000, Avg MRR: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  24%|██▍       | 24/100 [00:00<00:02, 36.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/100 items. Current Avg Precision: 0.3333, Avg Recall: 0.5500, Avg MRR: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  36%|███▌      | 36/100 [00:01<00:01, 37.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.3778, Avg Recall: 0.6000, Avg MRR: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  44%|████▍     | 44/100 [00:01<00:01, 37.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 items. Current Avg Precision: 0.4083, Avg Recall: 0.6250, Avg MRR: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  56%|█████▌    | 56/100 [00:01<00:01, 37.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4067, Avg Recall: 0.6300, Avg MRR: 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  64%|██████▍   | 64/100 [00:01<00:00, 37.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/100 items. Current Avg Precision: 0.4056, Avg Recall: 0.6361, Avg MRR: 0.7833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  76%|███████▌  | 76/100 [00:02<00:00, 38.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.3952, Avg Recall: 0.6167, Avg MRR: 0.7548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  84%|████████▍ | 84/100 [00:02<00:00, 38.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 items. Current Avg Precision: 0.4208, Avg Recall: 0.6583, Avg MRR: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  96%|█████████▌| 96/100 [00:02<00:00, 38.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4185, Avg Recall: 0.6556, Avg MRR: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:02<00:00, 37.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/100 items. Current Avg Precision: 0.3933, Avg Recall: 0.6183, Avg MRR: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:05<08:56,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of the process for creating multiple test cases in the Anthropic Evaluation tool, including defining evaluation criteria, creating diverse test cases, formatting them, using batch processing, running evaluations, analyzing results, and iterating based on findings. However, it does not mention the specific action of clicking the 'Add Test Case' button and filling in values for each variable in the prompt, which is a critical piece of information from the correct answer. Therefore, while the generated answer contains relevant information, it lacks the specific steps outlined in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:07<05:45,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that Anthropic recommends Pinecone as the embeddings provider, while the correct answer specifies Voyage AI as the recommended provider. This is a critical piece of information that directly contradicts the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:12<06:29,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of key success metrics for evaluating Claude's performance on a classification task, including accuracy, precision, recall, F1 score, ROC-AUC, latency, and throughput. It also discusses the importance of balancing these metrics when choosing the right model to reduce latency, which aligns with the correct answer's emphasis on speed and output quality. However, the generated answer does not explicitly mention \"consistency,\" \"structure,\" \"bias,\" and \"fairness,\" which are included in the correct answer. While the generated answer is detailed and covers many important aspects, the omission of these specific metrics means it does not fully align with the correct answer. Therefore, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:15<05:53,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides valid points about how Claude for Sheets can improve prompt engineering workflows, such as streamlined interaction and enhanced contextual awareness. However, it does not mention the ability to test prompts across evaluation suites in parallel, which is a key aspect of the correct answer. Therefore, while the generated answer contains relevant information, it is missing a critical piece of information regarding the speed advantage of parallel testing over sequential chained prompts. Thus, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:17<05:11,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not mention that the absence of the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns will result in an API error, which is a critical piece of information provided in the correct answer. Instead, it focuses on the potential for confusion and lack of clarity in responses, which does not address the specific consequence of an API error. Therefore, the generated answer is missing a key aspect of the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:19<04:26,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that using tools in the Claude API results in additional tokens, which impacts pricing. However, it fails to mention that tool use requests are priced the same as regular API requests based on total input and output tokens, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing important context regarding the pricing structure.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:21<03:48,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the new features are expected to be available in early 2024, while the correct answer specifies a concrete date of June 27th, 2024. This is a critical piece of information that is missing in the generated answer, as it does not provide the exact date and instead gives a vague timeframe. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:23<03:36,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two key factors to consider when deciding whether to use chain-of-thought (CoT): the complexity of the task and latency requirements. It emphasizes the need for in-depth reasoning for complex tasks and acknowledges the potential impact of CoT on latency, which aligns with the correct answer. Therefore, the generated answer captures the essential points and is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:28<04:29,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed process for using Claude to digest long PDF documents, including steps for text extraction, summarization, and asking specific questions. However, it does not explicitly mention the ability to upload PDFs directly to Claude for summarization, which is a key point in the correct answer. Therefore, while the generated answer contains useful information, it is missing the critical aspect of directly uploading PDFs for summarization.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:31<04:37,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that you can view your organization's current API rate limits in the \"Usage\" section of the Anthropic Console, while the correct answer specifies that this information is found in the Rate Limits tab of the Developer Console. Since the generated answer refers to a different section (\"Usage\") instead of the specified \"Rate Limits tab,\" it is missing critical information and does not accurately reflect the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/100 questions. Current Accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [00:36<05:32,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of metrics to evaluate the performance of the ticket classification system beyond accuracy, including precision, recall, F1 score, confusion matrix, ROC-AUC score, log loss, class distribution analysis, cross-validation, error analysis, and user feedback. However, it does not mention the 95th percentile response time and average cost per classification, which are specifically highlighted in the correct answer as important metrics for assessing production-readiness. Therefore, the generated answer is missing critical information that is present in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [00:40<05:25,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to specify a system prompt using both the Text Completions API and the Messages API. It mentions that the Text Completions API uses a single input string for the prompt, which aligns with the correct answer's description of adding the system prompt before the first \"\\n\\nHuman:\" turn. Additionally, it accurately states that the Messages API uses a structured array of messages with a role of \"system\" for the prompt. Therefore, the generated answer captures the essential information and is consistent with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 600\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [00:46<06:21,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed explanation of how to use XML tags to structure prompts for Claude, emphasizing the importance of defining the structure, encouraging step-by-step reasoning, using attributes for additional context, and iterating to refine prompts. However, it lacks the specific mention of using <thinking> and <answer> tags as highlighted in the correct answer, which is a critical piece of information for combining XML tags with chain of thought reasoning. Therefore, the generated answer is missing essential details that are present in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [00:48<05:31,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it mentions precision and recall as key metrics instead of the accuracy, 95th percentile response time, and average cost per request routing, which are the metrics specified in the correct answer. Additionally, it does not provide the specific results for accuracy, precision, and recall, which are critical pieces of information that were included in the correct answer. Therefore, the generated answer does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [00:51<04:47,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes some relevant points such as understanding goals and objectives, defining a target audience, and having evaluation criteria. However, it misses the specific mention of having a clear definition of success criteria, ways to empirically test against those criteria, and a first draft prompt to improve, which are critical components outlined in the correct answer. Therefore, the generated answer is not fully aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [00:55<05:16,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly describes the differences between the Messages API and the Text Completions API regarding mid-response prompting. It accurately states that the Messages API allows for interactive dialogues and can adjust its output based on user input, while the Text Completions API processes the input at once without mid-response adjustments. However, it does not explicitly mention the specific mechanism of how the Messages API allows for continuation of responses by using the \"assistant\" role, nor does it mention the ability to pre-fill part of Claude's response in the Text Completions API. These details are critical to fully understanding the differences. Therefore, while the essence of the answer is correct, it lacks important specifics, leading to a conclusion of incorrectness.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:00<05:28,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that Claude's response is tailored based on the assigned role, emphasizing the depth and specificity expected from a role-based prompt. It also notes that without a specific role, the response may be more general and less focused. However, it lacks the specific mention of the structured, actionable nature of the analysis and the specific recommendations that the correct answer highlights. While the generated answer captures the essence of the difference, it does not fully convey the level of insight and actionable recommendations that the correct answer specifies. Therefore, it is missing critical information regarding the structured and actionable aspects of the role-based response.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:05<06:07,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several quantitative metrics for evaluating a sentiment analysis model, including accuracy, precision, recall, F1 score, AUC-ROC, and the confusion matrix. It also discusses how to determine specific targets for these metrics, aligning well with the correct answer's emphasis on industry benchmarks, prior experiments, and expert knowledge. Therefore, the generated answer captures the essential points and is consistent with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  19%|█▉        | 19/100 [01:07<04:58,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that it does not have access to the specific documentation and cannot provide a power user tip, which is a critical piece of information missing from the correct answer. The correct answer provides a specific tip about combining XML tags with other prompt engineering techniques, which is not addressed in the generated answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [01:12<05:22,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive and detailed approach to using an LLM like Claude to grade the outputs of other LLMs based on a rubric. It includes defining the grading rubric, inputting outputs, prompt design, batch processing, analyzing results, iterative improvement, and human oversight. These steps align well with the essence of the correct answer, which emphasizes using Claude to evaluate outputs against a rubric and providing a simple 'correct' or 'incorrect' result. Although the generated answer is more detailed, it does not omit any critical information and effectively captures the main idea of the correct answer. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 20/100 questions. Current Accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [01:16<05:20,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a general overview of accessing and deploying Voyage embeddings on AWS Marketplace, including steps like visiting the marketplace, selecting the product, subscribing, launching, configuring deployment, and accessing the embeddings. However, it lacks specific details mentioned in the correct answer, such as copying the Product ARN, creating a JupyterLab space in SageMaker Studio, and uploading Voyage's notebook. These are critical steps for deploying the model package that are not addressed in the generated answer. Therefore, it is missing essential information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [01:22<05:51,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive approach to obtaining JSON output from Claude, including defining the schema, using structured prompts, providing examples, and validating the output. However, it lacks the specific instruction to provide a single tool and to set the tool_choice explicitly, which are key elements mentioned in the correct answer. Therefore, it is missing critical information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [01:26<05:31,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the key differences between the Claude Instant 1.2 model and the Claude 3 Haiku model, including improvements in model architecture, natural language understanding, response quality, performance metrics, adaptability, and training data. It aligns with the correct answer's points about performance, intelligence, and up-to-date training data. However, it does not explicitly mention the vision capabilities of Claude 3 Haiku, which is a critical piece of information present in the correct answer. Therefore, while the generated answer is largely correct, it is missing this important detail.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [01:27<04:29,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that using examples helps clarify the desired output format and context, which aligns with the correct answer's emphasis on reducing misinterpretation of instructions. Both answers highlight the benefit of achieving more accurate outputs from Claude. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [01:29<03:43,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that prompt engineering allows for quicker adaptations to new domains or tasks without extensive retraining, which aligns with the key advantage mentioned in the correct answer. It emphasizes the efficiency and reduced computational resources needed compared to fine-tuning, which is consistent with the essence of the correct answer. Therefore, the generated answer is correct as it conveys the same fundamental idea.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [01:33<03:54,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed step-by-step process for getting started with the Claude for Sheets extension, including installing the extension, accessing it, choosing a template, customizing it, and utilizing its features. However, it does not explicitly mention making a copy of the provided Claude for Sheets workbook template, which is a critical piece of information in the correct answer. Therefore, the generated answer is missing this key detail and does not align fully with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [01:39<04:53,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the general idea that the \"index\" field relates to the order of the text being streamed, but it lacks the specific detail that multiple deltas with the same index consecutively stream the text for a single content block. This critical piece of information is essential for understanding how the \"index\" functions in the context of streaming responses. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [01:41<04:06,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is mostly correct, but it is missing the specific requirement to include the image as a base64-encoded image in an \"image\" content block within the \"messages\" array. Additionally, it does not mention the WebP format as a supported image format, which is included in the correct answer. Therefore, it lacks critical information and is not fully accurate.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [01:44<04:03,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the relationship between time to first token (TTFT) and latency, emphasizing that TTFT is a measure of the time taken to generate the first token after a prompt, which is indeed a specific aspect of latency. It also discusses the implications of TTFT and latency on user experience, aligning with the correct answer's focus on TTFT as an important component of overall latency and responsiveness. Therefore, the generated answer is correct and captures the essential points made in the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [01:48<04:05,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly captures the essence of the correct answer by explaining how providing examples of handling edge cases like implicit requests and emotional prioritization can enhance Claude's performance in routing support tickets. It discusses the importance of recognizing implicit requests and emotional cues, which aligns with the points made in the correct answer. Both answers emphasize the improvement in ticket categorization and prioritization, as well as the overall customer experience. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 30/100 questions. Current Accuracy: 0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [01:52<04:11,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes the significance of the \"tool_use\" stop_reason in the context of integrating external tools with Claude. It emphasizes the enhancement of Claude's capabilities through the use of external tools and the transition from internal processing to tool utilization. However, it lacks specific details about the process that follows the \"tool_use\" stop_reason, such as the need for the user to extract tool input, run the tool code client-side, and send the results back to Claude. This omission is critical as it describes the complete workflow and interaction required after the \"tool_use\" signal. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [01:54<03:43,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that the error event is \"rate_limit_exceeded\" with an HTTP error code of 429, while the correct answer specifies that the error event is \"overloaded_error\" with an HTTP error code of 529. This is a critical piece of information that is missing in the generated answer, and there is a direct contradiction between the two answers regarding the specific error event and corresponding HTTP error code.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [01:56<03:03,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that the two types of deltas are \"insert\" and \"delete,\" while the correct answer specifies \"text_delta\" and \"input_json_delta.\" Since the generated answer does not match the correct answer and presents different terms, it is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [01:57<02:41,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that both Claude 3.5 Sonnet and tool use became generally available on March 13, 2024, while the correct answer specifies that Claude 3.5 Sonnet became available on June 20, 2024, and tool use on May 30, 2024. This is a direct contradiction regarding the dates of availability.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [01:59<02:29,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not specify the order of the launches in terms of the timeline (May 2024 for Europe and June 2024 for Canada) and implies a different sequence by stating that Claude.ai was launched first without clarifying the specific regions or months. The correct answer clearly states the order and timing of the launches, which the generated answer fails to capture accurately.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [02:03<02:45,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the correct answer by explaining that a stop_reason of \"tool_use\" indicates the model's need to use an external tool to continue the conversation. However, it lacks specific details about extracting the tool name and input from Claude's request, executing the tool code client-side, and sending a new user message with the tool result back to Claude. These critical steps are essential for properly continuing the conversation after a \"tool_use\" stop reason. Therefore, the generated answer is missing important information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [02:05<02:30,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the anthropic Python library, which is specifically stated in the correct answer as the library used to interact with the Claude AI model. Instead, it lists other libraries that are not mentioned in the correct answer, which could lead to confusion about the specific tools used in the example code snippet.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [02:07<02:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it omits the option of using the default AWS credential providers, such as the ~/.aws/credentials file or the AWS environment variables (AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID), which is a critical piece of information mentioned in the correct answer. Additionally, while it mentions AWS IAM roles, it does not explicitly state that this is an alternative to providing access keys directly, which is part of the correct answer's context.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [02:10<02:37,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two important factors: security and privacy, and usability and performance. However, it does not explicitly mention the potential reduction in prompt leaks or the risk of degraded model performance due to added complexity, which are critical components of the correct answer. Therefore, while the generated answer addresses relevant themes, it lacks the specific balance between prompt leaks and model performance degradation that is central to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [02:14<02:54,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains that selecting the appropriate Claude model can help reduce latency by optimizing for specific tasks and balancing performance and speed. It mentions the impact of model size and architecture on processing speed, which aligns with the correct answer's emphasis on choosing the right model for speed and output quality. Both answers convey the importance of matching the model's capabilities to the application's needs to minimize latency. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 40/100 questions. Current Accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [02:17<02:55,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a correct method for streaming responses from the Anthropic API using the Python SDK. It mentions using the `stream=True` parameter when making a request, which aligns with the correct answer's emphasis on streaming responses. Both answers convey the essential idea of iterating over the response to handle streamed data. Therefore, the generated answer is correct as it captures the necessary information for streaming responses.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [02:19<02:36,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly mentions using the \"max_tokens\" parameter to limit the length of the output, which aligns with the correct answer. However, it fails to mention that the pre-filled part of the response should be placed in the last position of the input messages list, which is a critical piece of information missing from the generated answer. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [02:24<03:17,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer presents a nuanced view on the importance of test cases with automated grading versus those graded by humans, discussing the benefits of both approaches. However, it ultimately contradicts the correct answer, which clearly states that it is better to prioritize a larger volume of test cases with automated grading over fewer high-quality human-graded cases. The generated answer suggests that the importance depends on specific goals and implies a balance between the two, which is not aligned with the correct answer's emphasis on prioritizing automated grading. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [02:26<02:41,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it states that the required fields are \"text\" and \"timestamp,\" while the Correct Answer specifies \"index\" and \"delta\" as the required fields. This is a critical piece of information that is missing in the Generated Answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: not well-formed (invalid token): line 3, column 119\n",
      "Evaluating End-to-End:  45%|████▌     | 45/100 [02:28<02:29,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides alternative interactive learning methods (interactive tutorials and live Q&A sessions) that are not mentioned in the Correct Answer. However, it fails to mention the specific resources highlighted in the Correct Answer, such as the Anthropic Cookbook and the Developer Console. Since the Generated Answer does not include critical pieces of information regarding the specific tools available for learning about Claude's capabilities, it cannot be considered correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [02:35<03:30,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive explanation of why breaking a task into subtasks improves Claude's accuracy, covering aspects such as clarity, incremental problem solving, error isolation, structured reasoning, and enhanced contextual understanding. These points align well with the essence of the correct answer, which emphasizes that focusing on distinct subtasks allows Claude to give full attention to each part, thereby reducing errors. Since the generated answer captures the main idea and expands on it without contradicting the correct answer, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [02:38<03:19,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that Messages streaming responses are structured for interactive dialogue and include metadata, while Text Completions streaming responses are more linear and focused on generating text completions. However, it does not explicitly mention that Messages responses can contain multiple content blocks of varying types, which is a key aspect of the correct answer. Therefore, while the generated answer captures the essence of the differences, it lacks a critical piece of information regarding the complexity of the Messages streaming format compared to Text Completions. Thus, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [02:40<02:46,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two ways to experiment with Claude: using the Claude API and accessing Claude through a web-based interface. However, it does not mention visiting claude.ai or using Anthropic's web Console, which are specifically stated in the correct answer. Therefore, the generated answer is missing critical information and is not fully aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [02:43<02:42,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how chain prompts break down complex tasks into smaller components, allowing for focused attention and reducing errors. It also mentions the iterative refinement process, which enhances the quality of the output, aligning with the idea of minimizing inconsistencies. Overall, the generated answer conveys the same fundamental concepts as the correct answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [02:45<02:17,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the overloaded_error event corresponds to the HTTP status code 429, while the correct answer specifies that it corresponds to HTTP 529. Since these two status codes are different and the generated answer contradicts the correct answer, it is deemed incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 50/100 questions. Current Accuracy: 0.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [02:47<02:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the specific ways to specify the format of embeddings as described in the correct answer. The correct answer specifies that you can either leave the encoding_format parameter unspecified or set it to \"base64\", while the generated answer refers to using query parameters and the `Accept` header, which does not align with the details provided in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [02:51<02:16,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not accurately capture the specifics of how the input JSON deltas for tool_use content blocks are sent and parsed. It mentions incremental updates and maintaining local state, but it fails to specify that the deltas are sent as partial JSON strings in multiple content_block_delta events and that the complete JSON object is parsed after receiving a content_block_stop event. Additionally, it does not mention the use of libraries like Pydantic or helpers provided in Anthropic's SDKs for parsing. These omissions are critical to understanding the correct implementation, making the generated answer incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [02:53<02:14,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly names the tutorials as \"Prompting 101\" and \"Prompting 102,\" whereas the correct answer specifies a GitHub prompting tutorial and a Google Sheets prompting tutorial. Additionally, the generated answer does not mention the specific platforms or tools used in the tutorials, which is a critical piece of information. Therefore, the generated answer is not correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [02:58<02:37,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of Claude's capabilities for enterprise use, including integration features, data privacy, scalability, customizability, advanced processing capabilities, user-friendly interface, and real-time processing. However, it fails to mention the specific 200K token context window and multimodal input capabilities that are highlighted in the correct answer. These are key aspects that contribute to Claude's suitability for high-trust industries and processing large volumes of sensitive data. Therefore, the generated answer is missing critical information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [03:00<02:11,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide any information regarding the specific regions where Anthropic's Claude.ai API and iOS app are available as of June 2024. It states a lack of access to this information and suggests consulting the official website instead. This is a critical omission, as the correct answer specifies that the services are available in the United States, Canada, and Europe. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [03:05<02:39,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer introduces two different approaches (API Integration and Chatbot Integration) that are not mentioned in the correct answer, which specifies push-based and pull-based approaches. While the generated answer discusses scalability and ease of implementation, it does not align with the specific approaches outlined in the correct answer. Therefore, it lacks critical information and does not accurately reflect the content of the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [03:07<02:13,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that the prompt generator tool was released on March 1, 2024, while the correct answer states the release date is May 10, 2024. Additionally, the generated answer mentions the tool is available through the Claude interface, whereas the correct answer specifies it is available through the Developer Console. These discrepancies indicate that the generated answer does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [03:10<02:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not explicitly identify the Claude 3 Sonnet model as the one that balances intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing. Instead, it discusses the general trade-offs between speed and intelligence without naming the specific model that provides the best balance. This omission of the critical piece of information makes the generated answer incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [03:13<02:04,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that the similarity between two Voyage embedding vectors can be calculated using the cosine similarity measure, which is equivalent to the dot product of the two vectors since they are normalized to length 1. It also explains the meaning of the cosine similarity values, which adds clarity. Therefore, the generated answer is consistent with the correct answer and contains all the necessary information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [03:15<01:50,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer. It discusses how examples in prompts provide clear context, demonstrate expected formats, and help reduce ambiguity, which aligns with the correct answer's emphasis on reducing misinterpretation and enforcing consistency. Both answers highlight that examples serve as a guide for the desired output and improve Claude's ability to handle complex tasks. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 60/100 questions. Current Accuracy: 0.2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [03:19<02:04,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it describes two types of deltas that are not mentioned in the Correct Answer. The Correct Answer specifies \"text deltas\" and \"input JSON deltas,\" while the Generated Answer refers to \"Content Block Delta\" and \"Tool Use Delta,\" which do not match. Additionally, the details provided in the Generated Answer do not align with the specific fields mentioned in the Correct Answer, such as the \"text\" field and \"partial_json\" field. Therefore, there is a critical piece of information missing and a contradiction in the types of deltas described.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [03:22<01:50,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two key capabilities of Claude: advanced natural language understanding and the ability to learn from user interactions. These capabilities align with the correct answer's emphasis on question answering and text analysis, as both involve understanding language and adapting to user preferences. Therefore, the generated answer captures the essence of the correct answer without missing any critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [03:27<02:09,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not accurately reflect the key event types included in a raw HTTP stream response when using message streaming as outlined in the correct answer. It mentions connection establishment, response headers, data chunks, end of stream, and error handling, which are not the specific event types described in the correct answer. The correct answer specifies events like message_start, content_block_start, content_block_delta, content_block_stop, message_delta, and message_stop, which are critical to understanding the streaming process. Therefore, the generated answer is missing key information and does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [03:28<01:48,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide any specific information about the maximum number of images that can be included in a single request using the Anthropic API or the claude.ai interface. It states that it does not have access to those details, which is a critical piece of information missing compared to the correct answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [03:32<01:48,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer suggests prompting Claude again to continue from where it left off, which is a valid approach. However, it does not mention the critical step of increasing the max_tokens value, which is essential to ensure that the full response can be received. Therefore, the generated answer is missing a key piece of information compared to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [03:35<01:42,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the critical steps of developing test cases and reviewing Anthropic's guide to developing test cases. Instead, it refers to preparing the dataset and defining evaluation metrics, which are not the same as the steps outlined in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [03:37<01:32,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a broader context on how to influence Claude's response by discussing the use of context, tone, and specific instructions. However, it does not explicitly mention that the content parameter should be in the last position of the messages list with the \"assistant\" role to pre-fill part of Claude's response, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing essential details that are present in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [03:41<01:39,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the preservation of general knowledge as an advantage of prompt engineering over fine-tuning, which aligns with the correct answer. However, it introduces a new point about flexibility and adaptability, which is not mentioned in the correct answer. The correct answer specifically emphasizes the effectiveness of prompt engineering in helping models understand and utilize external content, which is not addressed in the generated answer. Therefore, while the generated answer contains valid points, it misses a critical aspect of the correct answer regarding the utilization of external content, leading to a lack of completeness. Thus, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [03:45<01:50,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a general overview of the steps to get started with the Bedrock API, mentioning setting up the environment and making API requests. However, it lacks specific details about installing and configuring the AWS CLI and the need to install an SDK, such as the Python SDK, which are critical steps mentioned in the correct answer. Therefore, the generated answer is missing important information and is not fully aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 573\n",
      "Evaluating End-to-End:  70%|███████   | 70/100 [03:49<01:47,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it suggests using the `aws sagemaker list-models` command, which is not the correct command for listing Claude models. The correct command is `aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`, which specifically targets the Bedrock service for listing foundation models provided by Anthropic. The generated answer does not mention the Bedrock service and could lead to confusion about how to correctly check for Claude models in a specific AWS region.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 70/100 questions. Current Accuracy: 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [03:51<01:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately conveys the information provided in the Correct Answer. Both answers state that the `input_type` argument can be used to specify whether the input text is a \"query\" or a \"document.\" There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [03:56<01:43,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a general overview of the differences between tool_use content blocks and text content blocks, focusing on their structure and the type of data they encapsulate. However, it fails to mention the specific detail that tool_use content block deltas contain partial JSON strings for the input field and that there may be delays between streaming events as the model emits one complete key-value pair at a time. These critical pieces of information are essential to understanding the differences in delta formats. Therefore, the generated answer is missing key information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [03:58<01:22,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide the specific image file size limits for the API and claude.ai, which are critical pieces of information included in the correct answer. Instead, it suggests that the details are not provided in the documents, which is misleading. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [04:00<01:10,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that the model's size and complexity are important factors for achieving low latency, which aligns with the correct answer's emphasis on balancing speed and output quality. Both answers highlight the need to consider the specific requirements of the use case, although the generated answer focuses more on model size. Overall, the essence of both answers is similar, and the generated answer does not omit any critical information. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [04:02<01:02,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that Anthropic recommends the \"Claude\" embedding model for code retrieval, whereas the correct answer specifies the \"voyage-code-2\" embedding model. Additionally, the generated answer does not mention the specific performance improvement of 17% over alternatives, which is a critical piece of information. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [04:05<01:10,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides two valid ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs: step-by-step tutorials and code examples. However, it does not mention the interactive Jupyter notebooks or the specific use case of uploading PDFs and working with embeddings, which are key aspects of the Correct Answer. Therefore, the Generated Answer is missing critical information and is not fully aligned with the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [04:10<01:15,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer regarding the impact of the context window size on retrieval augmented generation (RAG). Both answers emphasize that a larger context window allows for more retrieved information to be incorporated, which enhances the model's ability to generate accurate and relevant responses. The generated answer also discusses the limitations of a smaller context window, which aligns with the correct answer's implication about improved performance with a larger context window. Therefore, the generated answer is correct as it conveys the same essential information without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [04:13<01:13,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the essence of the Correct Answer by discussing the Evaluation tool's role in assessing prompt effectiveness, refining prompts, and improving user experience. It mentions the iterative process of prompt development and the importance of aligning prompts with desired outcomes. However, it lacks specific references to identifying edge cases, ensuring consistent performance across inputs, and reviewing results across test cases to spot patterns, which are critical elements mentioned in the Correct Answer. Therefore, while the Generated Answer is largely correct, it misses some key aspects that are essential for a complete understanding.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [04:15<01:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide the specific information that the Claude 3 Haiku model has the fastest comparative latency, which is a critical piece of information present in the correct answer. Instead, it states a lack of access to the comparison tables and does not affirmatively identify any model. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [04:21<01:17,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes the process of maintaining a multi-turn conversation using the Anthropic Messages API in Python. It emphasizes the need to keep a list of messages representing the conversation history, which aligns with the correct answer's point about sending the full conversation history with each request. Both answers convey that the API is stateless and that the entire context must be provided with each request. Therefore, the generated answer is correct and captures the essential information from the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 80/100 questions. Current Accuracy: 0.2375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [04:24<01:07,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how using XML tags to provide a specific role or context can enhance Claude's analysis of a legal contract. It emphasizes the importance of focusing Claude's attention on relevant aspects of the contract and how this targeted approach can lead to better risk assessments and informed decision-making. While the generated answer does not explicitly mention the potential financial savings (millions of dollars) highlighted in the correct answer, it captures the essence of the benefits of using a role prompt. Therefore, it is considered correct as it conveys the same fundamental idea without critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [04:27<01:01,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer misrepresents the approaches of Claude 3 Opus and Claude 3 Sonnet. It states that Claude 3 Opus adopts a proactive approach by inferring missing information, which contradicts the correct answer that states Opus is more likely to ask the user for missing information. Additionally, the generated answer suggests that Sonnet is conservative and may request clarification, which aligns with the correct answer but does not accurately reflect the emphasis on inference for Opus. Therefore, the generated answer is incorrect as it contains critical misinterpretations of both models' behaviors regarding missing information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [04:32<01:06,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of steps for deploying an automated ticket routing system using Claude, covering aspects such as requirements gathering, system design, data preparation, model training, testing, performance evaluation, monitoring, deployment strategy, user training, feedback loop, maintenance plan, and compliance and security. However, it lacks specific mention of implementing retry logic, error handling, and a gradual rollout process, which are critical components highlighted in the correct answer. Therefore, while the generated answer is thorough, it misses key elements necessary for ensuring reliability in deployment, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs = evaluate_retrieval(retrieve_similar, eval_data, db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context, db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_detailed.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_one.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Basic RAG\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Basic RAG\",\n",
      "  \"average_precision\": 0.3933333333333335,\n",
      "  \"average_recall\": 0.6183333333333334,\n",
      "  \"average_f1\": 0.48081274025260856,\n",
      "  \"average_mrr\": 0.7333333333333334,\n",
      "  \"end_to_end_accuracy\": 0.27\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/json_results/evaluation_results_one.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,retrieval_precision,retrieval_recall,retrieval_mrr,e2e_correct\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?,0.3333333333333333,0.5,1.0,False\n",
      "\"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",0.6666666666666666,1.0,1.0,False\n",
      "\"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",0.6666666666666666,1.0,1.0,False\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?,0.3333333333333333,0.5,1.0,False\n",
      "\"What happens if a prompt for the Text Completions API is missing the \"\"\\n\\nHuman:\"\" and \"\"\\n\\nAssistant:\"\" turns?\",0.6666666666666666,1.0,1.0,False\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?,0.3333333333333333,0.5,1.0,False\n",
      "\"When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\",0.3333333333333333,1.0,1.0,False\n",
      "\"When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\",0.3333333333333333,0.5,1.0,True\n",
      "How can I use Claude to more easily digest the content of long PDF documents?,0.0,0.0,0.0,True\n",
      "\"According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\",0.6666666666666666,1.0,1.0,False\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?,0.0,0.0,0.0,False\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?,0.3333333333333333,0.5,1.0,True\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?,0.0,0.0,0.0,False\n",
      "\"When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\",0.0,0.0,0.0,False\n",
      "\"Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\",0.3333333333333333,0.5,1.0,False\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?,0.6666666666666666,1.0,1.0,False\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?,0.0,0.0,0.0,False\n",
      "\"What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\",0.3333333333333333,1.0,0.5,True\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?,0.3333333333333333,0.5,1.0,False\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?,0.3333333333333333,0.5,0.5,True\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?,0.3333333333333333,1.0,1.0,False\n",
      "\"When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\",0.3333333333333333,0.5,1.0,False\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?,1.0,1.0,1.0,False\n",
      "What is one key benefit of using examples when prompt engineering with Claude?,0.3333333333333333,1.0,1.0,True\n",
      "\"According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\",0.3333333333333333,0.5,1.0,True\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?,0.6666666666666666,1.0,1.0,False\n",
      "\"How does the \"\"index\"\" field in the \"\"content_block_delta\"\" event relate to the text being streamed in a response?\",0.3333333333333333,0.5,1.0,False\n",
      "\"How can you include an image as part of a Claude API request, and what image formats are currently supported?\",0.0,0.0,0.0,False\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?,1.0,1.0,1.0,True\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?,0.3333333333333333,0.5,1.0,True\n",
      "\"How does the stop_reason of \"\"tool_use\"\" relate to the overall workflow of integrating external tools with Claude?\",0.3333333333333333,0.5,1.0,False\n",
      "\"According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\",1.0,1.0,1.0,False\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?,0.3333333333333333,0.5,1.0,False\n",
      "\"On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\",0.6666666666666666,1.0,1.0,False\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?,0.6666666666666666,1.0,1.0,False\n",
      "\"When the API response from Claude has a stop_reason of \"\"tool_use\"\", what does this indicate and what should be done next to continue the conversation?\",0.3333333333333333,0.5,1.0,False\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?,0.0,0.0,0.0,False\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?,0.6666666666666666,1.0,1.0,False\n",
      "\"When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\",0.6666666666666666,1.0,1.0,True\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?,0.3333333333333333,0.5,1.0,True\n",
      "How can you stream responses from the Anthropic API using the Python SDK?,0.6666666666666666,1.0,1.0,False\n",
      "\"How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\",0.0,0.0,0.0,False\n",
      "\"What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\",0.3333333333333333,0.5,1.0,False\n",
      "What are the two required fields in a content_block_delta event for a text delta type?,0.6666666666666666,1.0,1.0,False\n",
      "\"What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\",0.0,0.0,0.0,False\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?,0.6666666666666666,1.0,1.0,True\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?,0.3333333333333333,1.0,1.0,False\n",
      "\"What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\",0.0,0.0,0.0,False\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?,0.6666666666666666,1.0,1.0,True\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?,0.6666666666666666,1.0,1.0,False\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?,0.3333333333333333,1.0,1.0,False\n",
      "\"When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\",0.6666666666666666,1.0,1.0,False\n",
      "\"What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\",0.3333333333333333,0.5,1.0,False\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?,0.3333333333333333,1.0,1.0,False\n",
      "\"As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\",0.6666666666666666,0.6666666666666666,1.0,False\n",
      "\"What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\",0.6666666666666666,1.0,1.0,False\n",
      "\"When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\",0.0,0.0,0.0,False\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?,0.0,0.0,0.0,False\n",
      "\"How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\",0.6666666666666666,1.0,1.0,True\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?,0.3333333333333333,0.5,1.0,True\n",
      "\"What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\",0.6666666666666666,0.5,1.0,False\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?,0.0,0.0,0.0,True\n",
      "\"What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\",0.6666666666666666,1.0,1.0,False\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?,0.3333333333333333,0.5,1.0,False\n",
      "\"When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\",0.3333333333333333,1.0,0.5,False\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?,0.0,0.0,0.0,False\n",
      "How can you use the content parameter in the messages list to influence Claude's response?,0.0,0.0,0.0,False\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?,0.3333333333333333,0.5,1.0,False\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?,0.3333333333333333,0.5,0.3333333333333333,False\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?,0.6666666666666666,1.0,1.0,False\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?,0.6666666666666666,1.0,1.0,False\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?,0.6666666666666666,1.0,1.0,False\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?,0.3333333333333333,1.0,1.0,False\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?,0.6666666666666666,1.0,1.0,True\n",
      "\"What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\",0.6666666666666666,1.0,1.0,False\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?,0.6666666666666666,1.0,0.5,False\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?,0.6666666666666666,1.0,1.0,True\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?,0.3333333333333333,0.5,1.0,False\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?,0.6666666666666666,1.0,1.0,False\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?,0.6666666666666666,1.0,1.0,True\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?,0.0,0.0,0.0,True\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?,0.0,0.0,0.0,False\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?,0.6666666666666666,1.0,1.0,True\n",
      "How should you evaluate a model's performance on a ticket routing classifier?,0.3333333333333333,0.5,1.0,True\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?,0.3333333333333333,0.5,1.0,False\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?,0.6666666666666666,1.0,0.5,False\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?,0.3333333333333333,0.3333333333333333,1.0,True\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?,0.6666666666666666,1.0,1.0,True\n",
      "\"What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\",0.3333333333333333,1.0,0.5,False\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?,0.6666666666666666,1.0,1.0,False\n",
      "\"When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\",0.3333333333333333,0.5,1.0,True\n",
      "What does the temperature parameter do when working with large language models?,0.3333333333333333,0.5,1.0,True\n",
      "What are two ways to specify API parameters when calling the Claude API using Claude for Sheets?,0.3333333333333333,0.3333333333333333,0.5,False\n",
      "How does prefilling the response with an opening curly brace ({ ) affect Claude's output when extracting structured data from text?,0.0,0.0,0.0,False\n",
      "What are some helpful resources provided by Anthropic to dive deeper into building with images using Claude?,0.0,0.0,0.0,False\n",
      "How do you specify the API key when creating a new Anthropic client in the Python and TypeScript SDK examples?,0.0,0.0,0.0,True\n",
      "What are two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application?,0.3333333333333333,0.5,1.0,False\n",
      "\"What are the key differences between a pretrained language model like Claude's underlying model, and the final version of Claude available through Anthropic's API?\",0.0,0.0,0.0,False\n",
      "What is the IPv6 address range used by Anthropic?,0.3333333333333333,1.0,0.5,False\n",
      "\"When using the Python SDK to create a message with Claude, what are two ways you can specify your API key?\",0.0,0.0,0.0,False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/csvs/evaluation_results_detailed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
