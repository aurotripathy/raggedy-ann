{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "LLMs excels at a wide range of tasks, but struggle with queries specific to your unique business context. This is where Retrieval Augmented Generation (RAG) becomes invaluable. RAG enables the LLM to leverage your internal knowledge bases or customer support documents, significantly enhancing its ability to answer domain-specific questions. Enterprises are increasingly building RAG applications to improve workflows in customer support, Q&A over internal company documents, financial & legal analysis, and much more.\n",
    "\n",
    "In this guide, we'll demonstrate how to build and optimize a RAG system using the Anthropic documentation as our knowledge base. We'll walk you through:\n",
    "\n",
    "1. Embeddings are from the `intfloat/multilingual-e5-large-instruct` model, where input is truncated to at most 512 tokens\n",
    "2. In-memory vector database class is from Anthropic\n",
    "3. Building a robust evaluation suite. We'll go beyond 'vibes' based evals and show you how to measure the retrieval pipeine & end to end performance independently\n",
    "4. Implementing advanced techniques to improve RAG including summary indexing and re-ranking with Claude.\n",
    "\n",
    "Through a series of targeted improvements, we achieved significant performance gains on the following metrics compared to a basic RAG pipeline (we'll explain what all these metrics *mean* in a bit)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1) Setup\n",
    "2) Level 1 - Basic RAG\n",
    "3) Building an Evaluation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need a few libraries and models:\n",
    "\n",
    "1. `intfloat/multilingual-e5-large-instruct` to generate high quality embeddings\n",
    "2. `openai`,  the LLM for generation\n",
    "4. `pandas`, `numpy`, `matplotlib`, and `scikit-learn` for data manipulation and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-learn in /opt/vast-jupyter/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/vast-jupyter/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/vast-jupyter/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/vast-jupyter/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/vast-jupyter/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "## silent setup\n",
    "!pip install openai -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downlaod the Embeddings model and run a quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987f6fbb0a3c4e64a2a8e41fddec12ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7992ee9751d04dc7aa95e37ec8364ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470d76b997324d2faf8ef6cedf5e5103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f871fd9abfe043728d8561c7e7f0d157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d65f4485fa48ac9d86f4ff8dbf3914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37746d53fe5949388c4fa5c901bee0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa51c9c0c30445528020d2236a700618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa9d294c8bb4e28bcdfa5ae4694e890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3db937d4eac4b448c64a466909e0c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327b33ea5e394d57b967b10dd6c871be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91.92853546142578, 67.58030700683594], [70.38142395019531, 92.1330795288086]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, '南瓜的家常做法')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"\n",
    "]\n",
    "input_texts = queries + documents\n",
    "\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n",
    "\n",
    "embeddings = model.encode(input_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())\n",
    "# [[91.92853546142578, 67.5802993774414], [70.38143157958984, 92.13307189941406]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Vector DB Class\n",
    "\n",
    "In this example, we're using an in-memory vector DB, but for a production application, you may want to use a hosted solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_vec_db_in_memory(self, data):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_vec_db()\n",
    "            return\n",
    "\n",
    "        texts = [f\"Heading: {item['chunk_heading']}\\n\\n Chunk Text:{item['text']}\" for item in data]\n",
    "        self._embed_and_store(texts, data)\n",
    "        self.save_db()\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            # query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            query_embedding = model.encode(query)\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_vec_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_vec_in_memory to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 - Basic RAG\n",
    "\n",
    "To get started, we'll set up a basic RAG pipeline using a bare bones approach. This is sometimes called 'Naive RAG' by many in the industry. A basic RAG pipeline includes the following 3 steps:\n",
    "\n",
    "1) Chunk documents by heading - containing only the content from each subheading\n",
    "\n",
    "2) Embed each document\n",
    "\n",
    "3) Use Cosine similarity to retrieve documents in order to answer query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database loaded and saved.\n",
      "([{'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases', 'chunk_heading': 'Creating Test Cases', 'text': 'Creating Test Cases\\n\\n\\nWhen you first access the Evaluation screen, you’ll see a single row:\\n\\nTo add more test cases:\\nClick the ‘Add Test Case’ button.\\nFill in values for each variable in your prompt.\\nRepeat to create multiple scenarios.\\nHere’s an example of a populated Evaluation screen with several test cases:\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n'}, 'similarity': np.float32(0.8731841)}, {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#accessing-the-evaluate-feature', 'chunk_heading': 'Accessing the Evaluate Feature', 'text': 'Accessing the Evaluate Feature\\n\\n\\nTo get started with the Evaluation tool:\\nOpen the Anthropic Console and navigate to the prompt editor.\\nAfter composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n'}, 'similarity': np.float32(0.8708937)}, {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#2-develop-your-test-cases', 'chunk_heading': '2. Develop your test cases', 'text': '2. Develop your test cases\\n\\n\\nTo run your classification evaluation, you will need test cases to run it on. Take a look at our guide to developing test cases.\\n'}, 'similarity': np.float32(0.8676537)}], '\\nCreating Test Cases\\n\\n\\nWhen you first access the Evaluation screen, you’ll see a single row:\\n\\nTo add more test cases:\\nClick the ‘Add Test Case’ button.\\nFill in values for each variable in your prompt.\\nRepeat to create multiple scenarios.\\nHere’s an example of a populated Evaluation screen with several test cases:\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\n\\nAccessing the Evaluate Feature\\n\\n\\nTo get started with the Evaluation tool:\\nOpen the Anthropic Console and navigate to the prompt editor.\\nAfter composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\nEnsure your prompt includes at least 1-2 dynamic variables using the double brace syntax: {{variable}}. This is required for creating eval test sets.\\n\\n\\n2. Develop your test cases\\n\\n\\nTo run your classification evaluation, you will need test cases to run it on. Take a look at our guide to developing test cases.\\n\\n')\n",
      "To create multiple test cases for an evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. Access the Evaluation screen where you will see a single row for a test case.\n",
      "2. Click the ‘Add Test Case’ button to add more test cases.\n",
      "3. Fill in the values for each variable in your prompt for each test case.\n",
      "4. Repeat the process to create as many test cases as needed.\n",
      "\n",
      "Once you have added multiple test cases, you can also update the original prompt text and re-run the entire evaluation suite to see how the changes affect the performance across all test cases.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "\n",
    "def retrieve_similar(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n{chunk['text']}\\n\"\n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):\n",
    "    # query = \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context(query, context):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load the Anthropic documentation\n",
    "with open('data/anthropic_docs.json', 'r') as f:\n",
    "    anthropic_docs = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "db = VectorDB(\"anthropic_docs\")\n",
    "db.load_vec_db_in_memory(anthropic_docs)\n",
    "\n",
    "# test\n",
    "query = \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\"\n",
    "context = \"\"'Creating Test Cases\\n\\n\\nWhen you first access the Evaluation screen, you’ll see a single row:\\n\\nTo add more test cases:\\nClick the ‘Add Test Case’ button.\\nFill in values for each variable in your prompt.\\nRepeat to create multiple scenarios.\\nHere’s an example of a populated Evaluation screen with several test cases:\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\nIf you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.\\n'\n",
    "print(retrieve_similar(query, db))\n",
    "print(answer_query_from_context(query, context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Setup\n",
    "\n",
    "When evaluating RAG applications, it's critical to evaluate the performance of the retrieval system and end to end system separately.\n",
    "\n",
    "We synthetically generated an evaluation dataset consisting of 100 samples which include the following:\n",
    "- A question\n",
    "- Chunks from our docs which are relevant to that question. This is what we expect our retrieval system to retrieve when the question is asked\n",
    "- A correct answer to the question.\n",
    "\n",
    "This is a relatively challenging dataset. Some of our questions require synthesis between more than one chunk in order to be answered correctly, so it's important that our system can load in more than one chunk at a time. You can inspect the dataset by opening `evaluation/docs_evaluation_dataset.json`\n",
    "\n",
    "Run the next cell to see a preview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 4 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1d6210b8\",\n",
      "    \"question\": \"What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/claude-for-sheets#why-use-claude-for-sheets\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts#how-to-chain-prompts\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=4):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db)\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'response_text:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  12%|█▏        | 12/100 [00:00<00:00, 110.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.4333, Avg Recall: 0.7000, Avg MRR: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  24%|██▍       | 24/100 [00:00<00:01, 44.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/100 items. Current Avg Precision: 0.3333, Avg Recall: 0.5500, Avg MRR: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  36%|███▌      | 36/100 [00:00<00:01, 39.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.3778, Avg Recall: 0.6000, Avg MRR: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  46%|████▌     | 46/100 [00:01<00:01, 38.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 items. Current Avg Precision: 0.4083, Avg Recall: 0.6250, Avg MRR: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  55%|█████▌    | 55/100 [00:01<00:01, 37.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4067, Avg Recall: 0.6300, Avg MRR: 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  67%|██████▋   | 67/100 [00:01<00:00, 37.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/100 items. Current Avg Precision: 0.4056, Avg Recall: 0.6361, Avg MRR: 0.7833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  75%|███████▌  | 75/100 [00:01<00:00, 37.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.3952, Avg Recall: 0.6167, Avg MRR: 0.7548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  87%|████████▋ | 87/100 [00:02<00:00, 37.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 items. Current Avg Precision: 0.4208, Avg Recall: 0.6583, Avg MRR: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  95%|█████████▌| 95/100 [00:02<00:00, 36.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4185, Avg Recall: 0.6556, Avg MRR: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:02<00:00, 38.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/100 items. Current Avg Precision: 0.3933, Avg Recall: 0.6183, Avg MRR: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:05<09:10,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of the process for creating multiple test cases in the Anthropic Evaluation tool, including defining evaluation criteria, creating test cases, utilizing templates, and reviewing them. However, it does not mention the specific action of clicking the 'Add Test Case' button, which is a critical piece of information from the correct answer. Therefore, while the generated answer contains relevant information, it lacks a key step that is explicitly mentioned in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:07<05:40,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that Anthropic recommends OpenAI as the embeddings provider, while the correct answer specifies that Anthropic recommends Voyage AI. This is a critical piece of information that directly contradicts the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:15<09:09,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of key success metrics for evaluating Claude's performance on a classification task, including accuracy, precision, recall, F1 score, and AUC-ROC, which are all relevant. It also mentions latency as a critical factor in model selection, aligning with the correct answer's emphasis on choosing the right model to reduce latency. However, the generated answer does not explicitly mention \"consistency,\" \"structure,\" \"bias,\" and \"fairness,\" which are included in the correct answer. While the generated answer is detailed and covers many important aspects, the omission of these specific metrics means it does not fully align with the correct answer. Therefore, it is marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:19<07:49,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two valid points about how Claude for Sheets can improve prompt engineering workflows compared to chained prompts. It discusses integrated data handling and simplified workflow management, which align with the essence of the correct answer. However, it does not mention the ability to test prompts across evaluation suites in parallel, which is a key aspect of the correct answer. Therefore, while the generated answer contains relevant information, it is missing a critical piece regarding the speed advantage of parallel testing, making it incomplete.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:22<06:41,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer discusses the potential issues that may arise from missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, such as ambiguity and less coherent completions. However, it does not explicitly state that this will result in an API error, which is a critical piece of information provided in the correct answer. Therefore, the generated answer is missing a key aspect of the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:25<05:57,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that using tools leads to higher costs compared to standard requests. However, the correct answer specifies that tool use requests are priced the same as regular API requests, but they do consume additional tokens that contribute to the total token count and thus the cost. This critical piece of information about the pricing being the same is missing from the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:28<05:27,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide the specific date of June 27th, 2024, when the new features will be available, which is a critical piece of information present in the correct answer. Instead, it suggests checking the official website for updates, which does not address the question directly. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:33<06:11,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two key factors: the complexity of the task and latency requirements, which align with the considerations mentioned in the correct answer. Both answers emphasize the need for in-depth thinking for complex tasks and the impact of increased output length on latency. Therefore, the generated answer captures the essence of the correct answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:37<06:03,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive set of strategies for using Claude to digest long PDF documents, including summarization, question answering, content extraction, and more. While the correct answer specifically mentions the ability to upload PDFs and have Claude summarize their content, the generated answer expands on this by offering additional methods for interacting with the content. Since the core idea of using Claude for summarization is present in both answers, and the generated answer does not contradict the correct answer, it can be considered correct overall.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:39<04:54,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that you can view your organization's current API rate limits in the \"API Usage\" section of the Anthropic Console, while the correct answer specifies that this information is found in the \"Rate Limits\" tab of the Developer Console. Since the generated answer refers to a different section and does not mention the \"Rate Limits\" tab, it is missing critical information and is therefore incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/100 questions. Current Accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [00:46<06:29,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of metrics to evaluate the performance of the ticket classification system, including precision, recall, F1 score, confusion matrix, ROC-AUC score, log loss, categorical performance, error analysis, user feedback, and temporal analysis. However, it does not mention the 95th percentile response time and average cost per classification, which are critical metrics highlighted in the correct answer. Since these specific metrics are essential for assessing production-readiness and were explicitly mentioned in the correct answer, the generated answer is missing critical information. Therefore, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [00:48<05:45,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to specify a system prompt using both the Text Completions API and the Messages API. It mentions that the Text Completions API embeds the prompt in the input text and that the Messages API uses structured message objects to define roles, including the system prompt. This aligns with the correct answer, which states that the system prompt is added as text before the first \"\\n\\nHuman:\" turn for the Text Completions API and specified using a separate \"system\" parameter for the Messages API. Therefore, the generated answer captures the essential differences and is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 683\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [00:55<06:46,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to use XML tags to structure prompts for Claude and emphasizes the importance of clear labeling and step-by-step reasoning. It also includes examples of how to use XML for context and dynamic adjustments, which aligns with the correct answer's focus on combining XML tags with chain of thought reasoning. However, it does not explicitly mention the specific XML tags like <thinking> and <answer> as part of the structured prompts, which is a critical piece of information from the correct answer. Therefore, while the generated answer is largely correct in its approach, it lacks this specific detail, making it incomplete.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [00:57<05:45,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it lists precision, recall, and F1 score as the key metrics, which are not the same as the accuracy, 95th percentile response time, and average cost per request routing mentioned in the Correct Answer. Additionally, the Generated Answer does not provide any results for the claude-3-haiku-20240307 model, which is a critical piece of information that is present in the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:00<04:58,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures some of the key ideas from the correct answer, such as understanding the goals for the prompt and the context in which the model will operate. However, it lacks the specific mention of having a clear definition of success criteria, ways to empirically test against those criteria, and a first draft prompt to improve, which are critical components outlined in the correct answer. Therefore, it is missing essential information and does not fully align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:03<04:50,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the differences between the Messages API and the Text Completions API, emphasizing the conversational continuity of the Messages API and the more static nature of the Text Completions API. However, it does not mention the specific mechanism by which the Messages API allows for mid-response prompting (i.e., making the last input message have the \"assistant\" role) or how the Text Completions API allows pre-filling part of the response directly in the prompt string. These are critical pieces of information that are present in the correct answer but missing from the generated answer. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:08<05:14,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the correct answer by explaining that having a specific role leads to more tailored and focused responses, with detailed analysis and recommendations. It also contrasts this with the broader and less specialized responses when no specific role is given. However, it lacks the specific mention of the role of CFO and the structured, actionable nature of the analysis that the correct answer highlights. While the generated answer is generally correct in its reasoning, it misses some critical details about the specific role and the nature of the insights provided. Therefore, it is not fully aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:13<05:42,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies several quantitative metrics for evaluating a sentiment analysis model, including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and MSE/MAE. It also discusses how to set specific targets for these metrics based on industry benchmarks, historical performance, and stakeholder input, which aligns with the Correct Answer's emphasis on using benchmarks and prior experiments to determine targets. Therefore, the Generated Answer is consistent with the Correct Answer in substance and detail.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  19%|█▉        | 19/100 [01:15<04:50,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that it does not have access to the specific content of the documentation and suggests referring directly to the documentation for power user tips. This is a critical omission, as it fails to mention the specific techniques of combining XML tags with multishot prompting and chain of thought, which are essential components of the correct answer. Therefore, the generated answer does not convey the necessary information and is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [01:20<05:21,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive and detailed approach to using an LLM like Claude for grading outputs from other LLMs, including defining a rubric, preparing outputs, structuring prompts, automating the process, and analyzing results. It aligns well with the correct answer's essence, which emphasizes using a rubric and instructing the LLM to evaluate outputs. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 20/100 questions. Current Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [01:25<05:43,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a general overview of the steps to access and deploy Voyage embeddings on AWS Marketplace, but it lacks specific details mentioned in the correct answer. Notably, it does not mention copying the Product ARN for the selected region or creating a JupyterLab space in SageMaker Studio, which are critical steps in the deployment process. Therefore, the generated answer is missing essential information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [01:30<05:50,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive approach to getting Claude to produce JSON output, including schema definition, structured prompts, examples, contextual information, feedback loops, and validation. However, it does not mention the specific requirement of providing a single tool and setting the tool_choice to explicitly instruct the model to use that tool, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing a key aspect of the tool setup and prompting process.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [01:37<06:40,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed comparison between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model, highlighting improvements in architecture, contextual understanding, response quality, learning and adaptation, error handling, efficiency, and versatility. However, it does not mention the specific vision capabilities of Claude 3 Haiku, its increased speed, performance, intelligence, or the fact that it has more up-to-date training data, which are key points in the correct answer. Therefore, the generated answer is missing critical information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [01:38<05:14,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that using examples helps clarify the desired output and provides context, which aligns with the correct answer's emphasis on reducing misinterpretation of instructions. Both answers convey the idea that examples lead to more accurate outputs from Claude. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [01:40<04:20,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that prompt engineering allows for quicker adaptation to new domains or tasks without the need for extensive retraining, which aligns with the key advantage mentioned in the correct answer. Both answers emphasize the ease of adapting AI models through prompt engineering compared to the complexity of fine-tuning. Therefore, the generated answer captures the essential substance of the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [01:45<04:40,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed process for getting started with the Claude for Sheets extension, including installation, accessing a template, and utilizing the extension. However, it does not explicitly mention making a copy of Anthropic's provided Claude for Sheets workbook template, which is a critical piece of information in the correct answer. Therefore, the generated answer is missing a key element that is essential for quickly getting started with the extension.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [01:48<04:13,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the \"index\" field's purpose in tracking the order of text segments in a streamed response. However, it lacks the specific detail that multiple deltas with the same index consecutively stream the text for a single content block, which is a critical piece of information from the correct answer. Therefore, the generated answer is not fully correct as it misses this important aspect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [01:50<03:54,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that images need to be encoded in base64 format and included in the request payload. However, it fails to mention that the image should be included in an \"image\" content block within the \"messages\" array, which is a critical piece of information. Additionally, the generated answer lists only JPEG, PNG, and GIF as supported formats, omitting WebP, which is included in the correct answer. Therefore, the generated answer is missing key details and is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [01:54<03:55,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the relationship between time to first token (TTFT) and latency, emphasizing that TTFT is a measure of the time taken to generate the first token after receiving input, while latency refers to the total time until the entire response is produced. It also correctly notes that a lower TTFT indicates a more responsive model, which enhances user experience, particularly in interactive applications. The generated answer aligns well with the correct answer, covering the essential points without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [01:58<04:20,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly captures the essence of the correct answer by explaining how providing examples of edge cases like implicit requests and emotional prioritization can enhance Claude's performance in routing support tickets. It discusses the importance of recognizing implicit requests and prioritizing emotional cues, which aligns with the points made in the correct answer. Both answers emphasize the improvement in routing accuracy and the overall user experience. Therefore, the generated answer is correct as it conveys the same fundamental ideas without omitting critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 30/100 questions. Current Accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [02:03<04:28,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the \"tool_use\" stop_reason and its significance in the workflow of integrating external tools with Claude. However, it lacks critical details about the specific actions that follow the \"tool_use\" indication, such as the need for the user to extract tool input, run the tool code client-side, and send the results back to Claude. These steps are essential for understanding the complete workflow and are missing from the generated answer. Therefore, it does not fully align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [02:05<03:42,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that the error event is \"rate limit exceeded\" with an HTTP error code of 429, while the correct answer specifies \"overloaded_error\" with an HTTP error code of 529. This is a critical piece of information that is missing in the generated answer, leading to a direct contradiction regarding the specific error event and code. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [02:06<03:08,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that the two types of deltas are \"insert\" and \"delete\", while the correct answer specifies that they are \"text_delta\" and \"input_json_delta\". This is a critical piece of information that is missing in the generated answer, leading to a direct contradiction in the content regarding the types of deltas.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [02:08<02:43,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that both Claude 3.5 Sonnet and tool use became generally available on March 20, 2024, which contradicts the correct answer that states Claude 3.5 Sonnet became available on June 20, 2024, and tool use on May 30, 2024. Since there is a direct contradiction in the dates provided, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [02:10<02:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not specify the order of the launches in terms of dates and locations, which is critical information. The correct answer states that both Claude.ai and the Claude iOS app were launched in Europe in May 2024, followed by their launch in Canada in June 2024. The generated answer fails to mention these specific dates and the sequence of launches, making it incomplete and misleading.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [02:12<02:31,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not accurately capture the specific actions required after receiving a stop_reason of \"tool_use\". While it mentions that the model has invoked an external tool, it fails to specify the need to extract the tool name and input, execute the tool code client-side, and send a new user message with the tool result. These critical steps are missing, making the generated answer incomplete and incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [02:15<02:25,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer mentions several libraries such as `nltk`, `textblob`, `spaCy`, and `transformers`, which are not mentioned in the correct answer. The correct answer specifically states that the anthropic Python library is used to interact with the Claude AI model, which is a critical piece of information missing from the generated answer. Therefore, the generated answer does not align with the correct answer and is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [02:18<02:48,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the option of using the default AWS credential providers, such as the ~/.aws/credentials file or the AWS environment variables (AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID). Instead, it focuses on using IAM roles, which is not one of the two main ways specified in the correct answer. Therefore, it misses a critical piece of information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [02:22<02:58,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two key factors: security and privacy, and usability and performance. These factors align with the essence of the correct answer, which emphasizes balancing the reduction in prompt leaks against the risk of degraded model performance. While the wording differs, the substance of both answers is consistent in highlighting the need to balance security with performance. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [02:26<03:12,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how selecting the appropriate Claude model can reduce latency by optimizing model size and complexity for specific tasks. It emphasizes the importance of choosing a model that balances performance and response time, which aligns with the correct answer's focus on selecting the right model based on speed and output quality. Both answers highlight the trade-off between model complexity and latency, and the generated answer adds detail about aligning the model with performance requirements and optimizing for hardware, which enhances the explanation. Therefore, the generated answer is correct and captures the essence of the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 40/100 questions. Current Accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [02:30<03:31,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a method for streaming responses from the Anthropic API using the Python SDK, which aligns with the correct answer's intent. It mentions using asynchronous features and provides an example code snippet that demonstrates how to stream responses. However, it does not explicitly mention the `client.messages.stream()` method or the `stream.text_stream` attribute, which are key components in the correct answer. Therefore, while the generated answer is generally correct in its approach, it lacks critical details that are present in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [02:32<03:03,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct as it mentions using the \"max_tokens\" parameter to limit the length of the response. However, it incorrectly states that the \"prompt\" parameter is used to guide Claude's response, while the correct answer specifies that the pre-filled part should be in the last position of the input messages list. This critical detail about the position of the pre-filled response is missing in the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [02:38<03:46,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer discusses the importance of both automated grading and human grading, emphasizing that the choice depends on specific evaluation goals. However, it does not align with the correct answer's assertion that prioritizing a larger number of automated test cases is better than having fewer high-quality human-graded cases. The generated answer suggests a more balanced approach without clearly stating that a larger volume of automated cases is preferred, which is a critical piece of information missing from it. Therefore, the generated answer is not correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [02:40<03:03,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it omits the \"index\" field, which is one of the two required fields in a content_block_delta event for a text delta type. The correct answer specifies both \"index\" and \"delta\" as required fields, while the generated answer only mentions \"delta\" and \"text\", which is insufficient and misleading.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [02:42<02:47,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides alternative methods for learning about Claude's capabilities, such as interactive tutorials and online sandbox environments. However, it does not mention the Anthropic Cookbook or the Developer Console, which are specific resources highlighted in the correct answer. Therefore, while the generated answer suggests valid learning methods, it lacks critical information about the specific tools mentioned in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [02:47<03:18,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive explanation of why breaking a task into subtasks improves Claude's accuracy, covering aspects such as clarity, cognitive load management, intermediary checking, layered context building, and error localization. These points align well with the essence of the correct answer, which emphasizes that focusing on distinct subtasks allows Claude to reduce errors compared to handling a complex task all at once. Therefore, the generated answer captures the critical reasoning behind the improvement in accuracy and does not omit any essential information. </explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [02:51<03:10,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies that Messages streaming responses are designed for more interactive and structured communication compared to Text Completions. It also mentions the handling of multi-turn conversational contexts and the inclusion of metadata, which aligns with the complexity described in the Correct Answer. However, it does not explicitly mention that Messages streaming responses can contain multiple content blocks of varying types, which is a critical piece of information that distinguishes it from Text Completions. Therefore, while the Generated Answer captures the essence of the differences, it lacks a key detail that is present in the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [02:54<02:59,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two methods for experimenting with Claude: using the Claude API and interacting via a chat interface. However, it does not mention visiting claude.ai or using Anthropic's web Console, which are the specific methods outlined in the correct answer. Therefore, the generated answer is missing critical information and does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [02:58<03:12,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the essence of the correct answer by explaining how chain prompts break down complex tasks into smaller parts, allowing Claude to focus on each subtask. It emphasizes the benefits of clarity, reduced ambiguity, and maintaining context, which aligns with the idea of reducing errors and inconsistencies. Therefore, the generated answer is correct as it conveys the same fundamental concepts without omitting critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [03:01<02:55,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that an `overloaded_error` event corresponds to an HTTP status code 429, while the correct answer specifies that it corresponds to an HTTP 529 status code. Since these two status codes are different, the generated answer is incorrect as it contradicts the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 50/100 questions. Current Accuracy: 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [03:03<02:30,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the specific ways to specify the format of the embeddings as described in the correct answer. The correct answer specifies that the format can be set by either leaving the encoding_format parameter unspecified or setting it to \"base64\". The generated answer, however, talks about using query parameters and headers, which does not align with the specific methods mentioned in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [03:08<02:45,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of how input JSON deltas for tool_use content blocks are sent and processed in streaming API requests. It mentions that the deltas are sent incrementally and describes the process of accumulating and parsing these deltas on the client side. However, it lacks the specific detail that the input JSON deltas are sent as partial JSON strings in multiple content_block_delta events and that a content_block_stop event indicates the completion of the JSON object. Additionally, it does not mention the use of libraries like Pydantic or helpers provided in Anthropic's SDKs for parsing. These omissions are critical, as they provide important context for how the client should handle the incoming data. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [03:11<02:42,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly names the tutorials as \"Prompt Engineering for ChatGPT\" and \"Prompt Engineering for Claude,\" which are not mentioned in the correct answer. The correct answer specifies a GitHub prompting tutorial and a Google Sheets prompting tutorial, highlighting their different formats and purposes. Therefore, the generated answer is missing critical information and does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [03:16<03:01,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of Claude's capabilities for enterprise use cases, including integration, data privacy, scalability, advanced NLP, customization, real-time processing, and multi-modal data handling. However, it does not mention the specific 200K token context window or the unique positioning of Claude for high-trust industries, which are critical elements highlighted in the correct answer. Therefore, while the generated answer contains relevant information, it lacks some key details that are essential for a complete understanding of Claude's suitability for enterprise applications.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [03:18<02:30,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide any specific information about the regions where Anthropic's Claude.ai API and iOS app are available as of June 2024. It states that it cannot access the specific content and suggests referring to official sources, which means it lacks the critical information that the correct answer provides. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [03:22<02:34,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it describes two different approaches (direct API integration and middleware integration) that do not match the correct answer's two main approaches (push-based using webhooks and pull-based). The generated answer fails to address the specific approaches mentioned in the correct answer and does not discuss their scalability and ease of implementation in the same context. Therefore, it lacks critical information and does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [03:24<02:09,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that the prompt generator tool was released on October 29, 2023, while the correct answer states it was released on May 10, 2024. This is a critical piece of information that directly contradicts the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [03:28<02:14,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not explicitly mention the Claude 3 Sonnet model, which is identified in the correct answer as the model that provides the best balance of intelligence and speed for high-throughput tasks. While the generated answer discusses the importance of balancing intelligence and speed and suggests looking for middle-tier models, it fails to specify that the Claude 3 Sonnet is the recommended model for the tasks mentioned. Therefore, it is missing a critical piece of information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [03:31<02:08,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to calculate the similarity between two Voyage embedding vectors using the dot product, which is equivalent to cosine similarity due to the normalization of the embeddings. It also accurately describes the range of the cosine similarity values and their meanings. Therefore, the generated answer is consistent with the correct answer and contains all the necessary information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [03:33<01:58,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer. It discusses how examples in prompts provide context, clarify expected output, reduce ambiguity, and help Claude understand complex relationships, which aligns with the correct answer's emphasis on reducing misinterpretation and enforcing consistent structure. Therefore, the generated answer is correct as it captures the essence of the correct answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 60/100 questions. Current Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [03:36<01:58,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the specific types of content block deltas (text deltas and input JSON deltas) as stated in the correct answer. Instead, it refers to \"delta\" and \"final,\" which are not the same as the specified types. Additionally, it fails to describe the contents of each delta type accurately, missing critical information about the \"text\" field and the \"partial_json\" field. Therefore, there are significant discrepancies between the generated answer and the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [03:39<01:46,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies key capabilities of Claude that enable it to build interactive systems and personalized user experiences, specifically mentioning advanced natural language understanding and maintaining context during conversations. These aspects align with the correct answer's focus on question answering and text analysis capabilities, as both emphasize understanding user intent and providing tailored responses. Therefore, the generated answer captures the essence of the correct answer without missing critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [03:42<01:49,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not accurately reflect the key event types included in a raw HTTP stream response when using message streaming as described in the correct answer. It focuses on general HTTP response structure rather than the specific events related to message streaming, such as message_start, content blocks, message_delta, and message_stop. Additionally, it does not mention the ping events that may occur throughout the stream. Therefore, it is missing critical information and does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [03:44<01:37,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide any specific information about the maximum number of images that can be included in a single request for either the Anthropic API or the claude.ai interface. It states that it is unable to answer the question based on the provided context, which is incorrect because the correct answer clearly specifies that the Messages API allows up to 20 images and the claude.ai interface allows up to 5 images. Therefore, the generated answer is missing critical information and is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [03:48<01:50,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer suggests prompting Claude to complete the tool use block directly, while the correct answer emphasizes increasing the max_tokens value to retrieve the full response. The critical piece of information missing in the generated answer is the recommendation to adjust the max_tokens limit, which is essential for obtaining the complete response. Therefore, the generated answer is not correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [03:50<01:34,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the two steps needed before running a classification evaluation on Claude are not provided in the available context, which is incorrect. The correct answer clearly outlines the two specific steps: developing test cases and reviewing Anthropic's guide to developing test cases. Since the generated answer fails to mention these critical steps, it is deemed incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [03:53<01:36,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how the content parameter can influence Claude's response by providing context and framing. It emphasizes the importance of crafting the content to guide Claude's output, which aligns with the correct answer's focus on pre-filling part of the response. However, it does not explicitly mention that the content should be in the last position of the messages list with the \"assistant\" role, which is a critical detail from the correct answer. Therefore, while the generated answer captures the essence of influencing Claude's response, it lacks a key piece of information regarding the specific placement and role of the content parameter.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [03:57<01:35,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the preservation of general knowledge as an advantage of prompt engineering over fine-tuning, which aligns with the correct answer. However, it introduces a third point about flexibility and adaptability, which, while relevant, is not explicitly mentioned in the correct answer. The correct answer specifically highlights the effectiveness of prompt engineering in helping models understand and utilize external content, which is not addressed in the generated answer. Therefore, while the generated answer contains valid points, it misses a critical aspect of the correct answer regarding the utilization of external content, leading to a conclusion that it is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [03:59<01:30,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it omits the crucial step of installing and configuring the AWS CLI, which is explicitly mentioned in the correct answer. While it includes obtaining an API key and making an API call, it does not cover the necessary setup of the AWS CLI and the installation of an SDK, which are essential steps to get started with making requests to Claude models on Anthropic's Bedrock API.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 70/100 [04:02<01:28,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it suggests using the `aws comprehend list-endpoints` command to check for available Claude models, which is not the correct command for this purpose. The correct command is `aws bedrock list-foundation-models`, as stated in the correct answer. The generated answer does not provide the right method to list Claude models and therefore fails to address the question accurately.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 70/100 questions. Current Accuracy: 0.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [04:04<01:16,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that the argument is `is_query`, while the correct answer specifies the argument as `input_type`. This is a critical piece of information that changes the meaning of the answer, as it refers to a different parameter. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [04:08<01:24,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a general overview of the differences between tool_use content blocks and text content blocks, focusing on the structure and fields used. However, it misses critical details from the correct answer, specifically that tool_use content block deltas contain partial JSON strings for the input field and that there may be delays between streaming events as the model emits one complete key-value pair at a time. These specific points are essential to understanding the differences in delta formats, making the generated answer incomplete and therefore incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [04:10<01:13,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the image file size limits are not specified in the provided documents, which is incorrect. The correct answer clearly specifies the limits: 5MB for the API and 10MB for claude.ai. Since the generated answer fails to provide this critical information, it is marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [04:13<01:07,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the importance of balancing model size and inference speed to meet low-latency requirements, which aligns with the essence of the correct answer. Both answers emphasize the need to balance speed and output quality based on specific use case requirements. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [04:15<01:03,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that Anthropic recommends the \"Claude\" embedding model for code retrieval, while the correct answer specifies the \"voyage-code-2\" embedding model. Additionally, the generated answer does not mention the 17% performance improvement over alternatives or the state-of-the-art results on general-purpose corpora, which are critical pieces of information. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [04:19<01:08,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides valid ways the Anthropic Cookbook can help developers learn to use the APIs, focusing on step-by-step examples and best practices. However, it does not mention the specific interactive Jupyter notebooks or the example of uploading PDFs and working with embeddings, which are key elements in the correct answer. Therefore, it is missing critical information that is present in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [04:22<01:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer regarding the impact of the context window size on a language model's ability to utilize retrieval augmented generation (RAG). Both answers emphasize that a larger context window allows for more retrieved information to be incorporated, which improves the accuracy and coherence of the generated responses. The generated answer also discusses the consequences of a small context window, which aligns with the correct answer's focus on the importance of context in generating informed responses. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [04:26<01:11,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the essence of the Correct Answer by discussing how the Evaluation tool helps in assessing and refining prompt effectiveness, identifying better phrasing, and optimizing input strategies. It also mentions the iterative process of evaluation and refinement, which aligns with the idea of improving reliability and user satisfaction. However, it lacks specific mention of identifying edge cases, rating individual results, ensuring consistent performance, and reviewing results across test cases to spot patterns, which are critical components of the Correct Answer. Therefore, while the Generated Answer is largely correct, it misses some key details that are essential for a complete understanding.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [04:28<01:02,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide the specific information that the Claude 3 Haiku model has the fastest comparative latency, which is a critical piece of information present in the correct answer. Instead, it suggests consulting the documents directly without confirming the specific model. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [04:34<01:17,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to maintain a multi-turn conversation using the Anthropic Messages API by emphasizing the need to keep a history of messages and send the entire context with each request. It also provides a detailed implementation example, which aligns with the correct answer's key point about the API being stateless and requiring the full conversation history. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 80/100 questions. Current Accuracy: 0.2375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [04:38<01:14,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how using XML tags to specify a role or context can enhance Claude's analysis of a legal contract. It emphasizes the importance of structured information and how roles guide Claude in focusing on relevant aspects of the contract. The generated answer also mentions the potential for a more nuanced and contextually appropriate analysis, which aligns with the correct answer's emphasis on catching critical legal issues and risks. While the generated answer does not explicitly mention the potential financial savings, it captures the essence of the correct answer regarding the benefits of using role prompts. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [04:42<01:07,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed comparison of how Claude 3 Opus and Claude 3 Sonnet handle missing information during tool calls. It correctly identifies that Opus is more aggressive in retrieving or inferring missing information, while Sonnet is more cautious and verification-oriented. However, the generated answer introduces the idea of Opus making educated guesses and using supplementary context, which is not explicitly mentioned in the correct answer. Additionally, it suggests that Sonnet may only proceed if enough context is available, which is a slight deviation from the correct answer's emphasis on asking the user for missing information. While the overall essence is similar, the generated answer introduces nuances that could lead to misunderstandings about the specific behaviors of each model. Therefore, it lacks some critical clarity and precision compared to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [04:47<01:13,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive set of steps for deploying an automated ticket routing system using Claude, covering aspects such as requirement analysis, system design, testing, security review, monitoring, user training, deployment strategy, and maintenance. While the correct answer emphasizes specific points like implementing retry logic, error handling, and gradual rollout, these are implicitly included in the broader steps outlined in the generated answer. Therefore, the generated answer captures the essence of ensuring a reliable deployment, even if it does not explicitly mention every detail from the correct answer. There are no critical omissions or contradictions, so the generated answer can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [04:52<01:10,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive evaluation of a model's performance on a ticket routing classifier, including accuracy, precision, recall, F1 score, confusion matrix, ROC-AUC, class distribution analysis, and cross-validation. While the correct answer mentions accuracy, cost, and speed, it does not elaborate on the specifics of how to evaluate performance. The generated answer covers accuracy and expands on it with additional relevant metrics, which enhances the evaluation. Therefore, the generated answer is correct in substance, as it aligns with the intent of evaluating model performance effectively.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [04:54<00:54,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the specific methods recommended by Anthropic, which are the interactive GitHub prompting tutorial and the Google Sheets prompting tutorial. Instead, it refers to exploring the documentation and experimenting with the model, which are not the same as the recommended methods. This omission of critical information makes the generated answer incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [04:59<00:58,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed comparison between pretrained large language models and Claude, touching on aspects such as training data, architecture, fine-tuning, capabilities, and ethical considerations. It aligns with the correct answer in emphasizing that Claude has been fine-tuned with RLHF to enhance its helpfulness and capabilities. However, it does not explicitly mention that pretrained LLMs are not inherently good at answering questions or following instructions without prompt engineering, which is a critical point made in the correct answer. This omission is significant as it highlights a key limitation of pretrained LLMs compared to Claude. Therefore, the generated answer is missing a critical piece of information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [05:05<01:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key advantages of prompt engineering over fine-tuning as outlined in the correct answer. It mentions cost-effectiveness, time efficiency, preservation of model integrity, flexibility, lower data requirements, avoidance of overfitting, real-time applications, and ease of use. All these points align with the essence of the correct answer, which emphasizes speed, cost, resource efficiency, and flexibility. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [05:11<01:02,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of the authentication process with GCP before accessing Claude models on Vertex AI. It includes the correct command `gcloud auth application-default login`, which is mentioned in the correct answer, and elaborates on additional steps such as setting up a GCP project, enabling the Vertex AI API, creating a service account, and using service account keys. While the generated answer is more detailed, it does not contradict the correct answer and includes the essential information needed for authentication. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [05:14<00:47,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not provide any specific information about the new capabilities and features introduced by Anthropic on May 10th, 2024, such as the \"Prompt Generator\" tool. It also fails to mention how this tool enables users to create and tailor prompts for specific tasks. Instead, it suggests looking for information elsewhere, which does not address the question directly. Therefore, the generated answer is missing critical information compared to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [05:15<00:35,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on March 11, 2024, while the correct answer states that they became available on June 20th, 2024. This is a direct contradiction regarding the date of availability, making the generated answer incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 90/100 questions. Current Accuracy: 0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [05:17<00:27,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that to limit Claude's response to a single token, you can use the header `max_tokens` and set its value to `1`, which aligns with the correct answer. Both answers convey the same essential information without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [05:20<00:23,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the function of the temperature parameter in large language models, including how it affects randomness and the trade-off between creativity and reliability. It expands on the correct answer by providing additional context about the effects of different temperature values, which does not contradict the original statement. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [05:23<00:19,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the specific method of passing API parameters as additional arguments in the CLAUDE() function, which is a critical piece of information provided in the correct answer. Instead, it introduces the concept of using named ranges, which is not mentioned in the correct answer and does not align with the specified methods. Therefore, the generated answer lacks essential details and does not accurately reflect the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [05:26<00:18,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains that prefilling the response with an opening curly brace ({) signals to Claude to output structured data, which aligns with the correct answer's emphasis on skipping the preamble and directly outputting a JSON object. However, it does not explicitly mention the result of this action being a more concise response that is easier for programs to parse, which is a critical piece of information in the correct answer. Therefore, while the generated answer captures some key aspects, it lacks completeness regarding the benefits of this approach.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [05:28<00:13,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not mention the specific multimodal cookbook or the API reference documentation for the Messages API, which are critical pieces of information provided in the correct answer. While it suggests visiting Anthropic's official website and community forums for resources, it lacks the specific details that the correct answer provides. Therefore, the generated answer is missing essential information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [05:31<00:11,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to specify the API key when creating a new Anthropic client in both Python and TypeScript. It mentions that the API key can be set directly in the code or as an environment variable, which aligns with the correct answer's information about the default behavior of using the ANTHROPIC_API_KEY environment variable if no API key is provided. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [05:34<00:08,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two benefits of the Anthropic Evaluation tool: improved prompt effectiveness and enhanced safety and alignment. While these points are relevant, they do not directly address the specific benefits mentioned in the correct answer, which focuses on identifying edge cases and ensuring consistent performance across test cases. The generated answer lacks the emphasis on refining prompts for reliability and does not mention the identification of edge cases, which is a critical aspect of the correct answer. Therefore, the generated answer is not correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [05:40<00:07,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of the differences between the pretrained model and the final version of Claude, touching on aspects such as training process, customization, deployment enhancements, safety and alignment, and user interface. However, it does not explicitly mention the critical point that the pretrained model is not inherently good at answering questions or following instructions, which is a key aspect of the correct answer. This omission is significant as it highlights the necessity of fine-tuning and reinforcement learning from human feedback (RLHF) to create the final version. Therefore, the generated answer lacks a critical piece of information and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [05:42<00:03,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that it does not have access to specific details regarding the IPv6 address range used by Anthropic, which is a critical piece of information that is missing. The correct answer explicitly provides the IPv6 address range (2607:6bc0::/48), which is not mentioned in the generated answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [05:44<00:00,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it mentions the environment variable as `CLAUDE_API_KEY`, which is not the correct name; the correct name is `ANTHROPIC_API_KEY`. This is a critical piece of information that directly contradicts the correct answer. Additionally, while it does mention passing the API key as a parameter, it lacks the clarity and specificity found in the correct answer regarding how to do this. Therefore, the generated answer does not accurately reflect the information provided in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 100/100 questions. Current Accuracy: 0.2700\n",
      "Detailed results saved to evaluation/csvs/evaluation_results_detailed.csv\n",
      "Average Precision: 0.3933\n",
      "Average Recall: 0.6183\n",
      "Average MRR: 0.7333\n",
      "Average F1: 0.4808\n",
      "End-to-End Accuracy: 0.2700\n",
      "Evaluation complete. Results saved to evaluation/json_results/evaluation_results_one.json, evaluation/csvs/evaluation_results_detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs = evaluate_retrieval(retrieve_similar, eval_data, db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context, db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_detailed.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_one.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Basic RAG\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Basic RAG\",\n",
      "  \"average_precision\": 0.3933333333333335,\n",
      "  \"average_recall\": 0.6183333333333334,\n",
      "  \"average_f1\": 0.48081274025260856,\n",
      "  \"average_mrr\": 0.7333333333333334,\n",
      "  \"end_to_end_accuracy\": 0.27\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/json_results/evaluation_results_one.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,retrieval_precision,retrieval_recall,retrieval_mrr,e2e_correct\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?,0.3333333333333333,0.5,1.0,False\n",
      "\"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",0.6666666666666666,1.0,1.0,False\n",
      "\"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",0.6666666666666666,1.0,1.0,False\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?,0.3333333333333333,0.5,1.0,False\n",
      "\"What happens if a prompt for the Text Completions API is missing the \"\"\\n\\nHuman:\"\" and \"\"\\n\\nAssistant:\"\" turns?\",0.6666666666666666,1.0,1.0,False\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?,0.3333333333333333,0.5,1.0,False\n",
      "\"When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\",0.3333333333333333,1.0,1.0,False\n",
      "\"When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\",0.3333333333333333,0.5,1.0,True\n",
      "How can I use Claude to more easily digest the content of long PDF documents?,0.0,0.0,0.0,True\n",
      "\"According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\",0.6666666666666666,1.0,1.0,False\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?,0.0,0.0,0.0,False\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?,0.3333333333333333,0.5,1.0,True\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?,0.0,0.0,0.0,False\n",
      "\"When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\",0.0,0.0,0.0,False\n",
      "\"Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\",0.3333333333333333,0.5,1.0,False\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?,0.6666666666666666,1.0,1.0,False\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?,0.0,0.0,0.0,False\n",
      "\"What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\",0.3333333333333333,1.0,0.5,True\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?,0.3333333333333333,0.5,1.0,False\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?,0.3333333333333333,0.5,0.5,True\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?,0.3333333333333333,1.0,1.0,False\n",
      "\"When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\",0.3333333333333333,0.5,1.0,False\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?,1.0,1.0,1.0,False\n",
      "What is one key benefit of using examples when prompt engineering with Claude?,0.3333333333333333,1.0,1.0,True\n",
      "\"According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\",0.3333333333333333,0.5,1.0,True\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?,0.6666666666666666,1.0,1.0,False\n",
      "\"How does the \"\"index\"\" field in the \"\"content_block_delta\"\" event relate to the text being streamed in a response?\",0.3333333333333333,0.5,1.0,False\n",
      "\"How can you include an image as part of a Claude API request, and what image formats are currently supported?\",0.0,0.0,0.0,False\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?,1.0,1.0,1.0,True\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?,0.3333333333333333,0.5,1.0,True\n",
      "\"How does the stop_reason of \"\"tool_use\"\" relate to the overall workflow of integrating external tools with Claude?\",0.3333333333333333,0.5,1.0,False\n",
      "\"According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\",1.0,1.0,1.0,False\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?,0.3333333333333333,0.5,1.0,False\n",
      "\"On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\",0.6666666666666666,1.0,1.0,False\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?,0.6666666666666666,1.0,1.0,False\n",
      "\"When the API response from Claude has a stop_reason of \"\"tool_use\"\", what does this indicate and what should be done next to continue the conversation?\",0.3333333333333333,0.5,1.0,False\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?,0.0,0.0,0.0,False\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?,0.6666666666666666,1.0,1.0,False\n",
      "\"When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\",0.6666666666666666,1.0,1.0,True\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?,0.3333333333333333,0.5,1.0,True\n",
      "How can you stream responses from the Anthropic API using the Python SDK?,0.6666666666666666,1.0,1.0,False\n",
      "\"How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\",0.0,0.0,0.0,False\n",
      "\"What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\",0.3333333333333333,0.5,1.0,False\n",
      "What are the two required fields in a content_block_delta event for a text delta type?,0.6666666666666666,1.0,1.0,False\n",
      "\"What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\",0.0,0.0,0.0,False\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?,0.6666666666666666,1.0,1.0,True\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?,0.3333333333333333,1.0,1.0,False\n",
      "\"What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\",0.0,0.0,0.0,False\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?,0.6666666666666666,1.0,1.0,True\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?,0.6666666666666666,1.0,1.0,False\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?,0.3333333333333333,1.0,1.0,False\n",
      "\"When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\",0.6666666666666666,1.0,1.0,False\n",
      "\"What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\",0.3333333333333333,0.5,1.0,False\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?,0.3333333333333333,1.0,1.0,False\n",
      "\"As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\",0.6666666666666666,0.6666666666666666,1.0,False\n",
      "\"What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\",0.6666666666666666,1.0,1.0,False\n",
      "\"When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\",0.0,0.0,0.0,False\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?,0.0,0.0,0.0,False\n",
      "\"How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\",0.6666666666666666,1.0,1.0,True\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?,0.3333333333333333,0.5,1.0,True\n",
      "\"What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\",0.6666666666666666,0.5,1.0,False\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?,0.0,0.0,0.0,True\n",
      "\"What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\",0.6666666666666666,1.0,1.0,False\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?,0.3333333333333333,0.5,1.0,False\n",
      "\"When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\",0.3333333333333333,1.0,0.5,False\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?,0.0,0.0,0.0,False\n",
      "How can you use the content parameter in the messages list to influence Claude's response?,0.0,0.0,0.0,False\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?,0.3333333333333333,0.5,1.0,False\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?,0.3333333333333333,0.5,0.3333333333333333,False\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?,0.6666666666666666,1.0,1.0,False\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?,0.6666666666666666,1.0,1.0,False\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?,0.6666666666666666,1.0,1.0,False\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?,0.3333333333333333,1.0,1.0,False\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?,0.6666666666666666,1.0,1.0,True\n",
      "\"What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\",0.6666666666666666,1.0,1.0,False\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?,0.6666666666666666,1.0,0.5,False\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?,0.6666666666666666,1.0,1.0,True\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?,0.3333333333333333,0.5,1.0,False\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?,0.6666666666666666,1.0,1.0,False\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?,0.6666666666666666,1.0,1.0,True\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?,0.0,0.0,0.0,True\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?,0.0,0.0,0.0,False\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?,0.6666666666666666,1.0,1.0,True\n",
      "How should you evaluate a model's performance on a ticket routing classifier?,0.3333333333333333,0.5,1.0,True\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?,0.3333333333333333,0.5,1.0,False\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?,0.6666666666666666,1.0,0.5,False\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?,0.3333333333333333,0.3333333333333333,1.0,True\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?,0.6666666666666666,1.0,1.0,True\n",
      "\"What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\",0.3333333333333333,1.0,0.5,False\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?,0.6666666666666666,1.0,1.0,False\n",
      "\"When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\",0.3333333333333333,0.5,1.0,True\n",
      "What does the temperature parameter do when working with large language models?,0.3333333333333333,0.5,1.0,True\n",
      "What are two ways to specify API parameters when calling the Claude API using Claude for Sheets?,0.3333333333333333,0.3333333333333333,0.5,False\n",
      "How does prefilling the response with an opening curly brace ({ ) affect Claude's output when extracting structured data from text?,0.0,0.0,0.0,False\n",
      "What are some helpful resources provided by Anthropic to dive deeper into building with images using Claude?,0.0,0.0,0.0,False\n",
      "How do you specify the API key when creating a new Anthropic client in the Python and TypeScript SDK examples?,0.0,0.0,0.0,True\n",
      "What are two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application?,0.3333333333333333,0.5,1.0,False\n",
      "\"What are the key differences between a pretrained language model like Claude's underlying model, and the final version of Claude available through Anthropic's API?\",0.0,0.0,0.0,False\n",
      "What is the IPv6 address range used by Anthropic?,0.3333333333333333,1.0,0.5,False\n",
      "\"When using the Python SDK to create a message with Claude, what are two ways you can specify your API key?\",0.0,0.0,0.0,False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/csvs/evaluation_results_detailed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
