{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a765c99-cd22-4427-9c75-18dbef726eff",
   "metadata": {},
   "source": [
    "#### This app demos a Retrieval Augmented Generation (RAG) pattern:\n",
    "RAG is well known pattern to allow a LLM (such as Llama 3) to answer \\\n",
    "questions on private data (which most likely not part of the training set). \n",
    "\n",
    "RAG is one way to prevent hallucination as it's grounded by the retrieved \\\n",
    "data (and much cheaper that fine-tuning)\n",
    "\n",
    "The demo includes how-tos for the following:\n",
    "- download Llama 3.1 from HF\n",
    "- use LangChain to ask Llama general questions and follow up questions using memory\n",
    "- use LangChain to load content (a recent web page - Hugging Face's blog post on Llama 3.1) and chat about it.\n",
    "- use an embeddings model, `sentence-transformers/all-mpnet-base-v2` for generating embeddings\n",
    "\n",
    "For now, this demo needs a L40S (and above in compute), although it should 'fit' in a 4090 (but it doesnt)\n",
    "\n",
    "Uses the following container, `image:nvcr.io/nvidia/pytorch:24.07-py3` from PyTorch NGC\n",
    "\n",
    "Roughly based on https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/RAG/hello_llama_cloud.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff223e9e-a011-44da-a50e-4d4d2578ce16",
   "metadata": {},
   "source": [
    "#### Install packages\n",
    "Let's start by installing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5989409-6e3e-49e5-b097-afd1d6a83358",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install faiss-cpu\n",
    "!pip install bs4\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba336758-d720-4673-89e4-2a74e8331877",
   "metadata": {},
   "source": [
    "#### Login to HF and download 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259da9fa-4584-4683-935c-300d432a9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface-hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49048b0-58f2-4ea3-98f8-ffe6aa0071df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go here for token: https://huggingface.co/settings/tokens\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e40064-4b60-4dea-a22b-9e6154ff669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936c643-62f6-4a93-847e-f68869d93b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print (f'allocated (GB): {torch.cuda.memory_allocated() / (1024 **3)}')\n",
    "# no memory should be allocated at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0eacd-e88f-4128-9e6e-b8ffc452a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    device=0,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"top_k\": 5,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85583dcc-a6d0-48eb-baf3-167a87c738c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check GPU memory allocation again\n",
    "print (f'allocated (GB): {torch.cuda.memory_allocated() / (1024 **3)}')  # ~29.9 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322f5ef-0d44-4779-acd3-cf9c936f580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "print(f'Output:\\n{llm.invoke(\"Hugging Face is\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee91ef9-a24e-49ab-8ba6-24766d225be9",
   "metadata": {},
   "source": [
    "#### Q&A w/model (on model memory, *not* on your data)\n",
    "With the model set up, you are now ready to ask some questions. \\\n",
    "Here is an example of the simplest way to ask the model some general questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8834ba-710c-4a88-86cc-a5bd2f8fa6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who wrote the book Innovator's dilemma?\"\n",
    "answer = llm.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1952131-9029-47f8-b5ad-44d5396ab451",
   "metadata": {},
   "source": [
    "##### Does multi-turn conversations work?\n",
    "We'll now ask a follow-up question, more information on the book.\\\n",
    "Since the chat history is not passed on, Llama doesn't have the context and doesn't know this is more about the book. \\\n",
    "Thus it treats this as new query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c5c81-aaf3-49d4-8055-8aa0ad7ebb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask one more question\n",
    "# chat history hasn't been passed, so Llama doesn't have the context and doesn't know this is more about the book\n",
    "followup = \"tell me more\"\n",
    "followup_answer = llm.invoke(followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b0dad-c272-46c6-acec-c669b5179a02",
   "metadata": {},
   "source": [
    "#### Q&A w/model (with conversation history, still *not* your data)\n",
    "To get around this we will need to provide the model with history of the chat.\\\n",
    "To do this, we will use `ConversationBufferMemory` to pass the chat history to the model \\\n",
    "and give it the capability to handle follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f338ce7-89c4-47aa-883d-a6fc7abf6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ConversationBufferMemory to pass memory (chat history) for follow up questions\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20758a2e-ce6a-4de6-9128-5bd7939b7ea7",
   "metadata": {},
   "source": [
    "##### Repeating with converational memory\n",
    "Once this is set up, let us repeat the steps from before and ask the model a simple question.\\\n",
    "Then we pass the question plus the answer back into the model for context along with the follow up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f80d5c-19a9-4452-b4e4-2a280f852185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart from the original question\n",
    "answer = conversation.predict(input=question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a9af0-f0a2-43ff-8879-5afdbb11b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass context (previous question and answer) along with the follow up \"tell me more\" to Llama who now knows more of what\n",
    "memory.save_context({\"input\": question},\n",
    "                    {\"output\": answer})\n",
    "followup_answer = conversation.predict(input=followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c00acc-f85f-4212-b62e-48086e63bf38",
   "metadata": {},
   "source": [
    "#### Finally, a small RAG pattern\n",
    "Next, let's explore using Llama 3.1 to answer questions using documents for context. \\\n",
    "This allows us to not rely on Llama 3.1's knowledge but provide better context without needing to finetune.\n",
    "\n",
    "##### Import the HF and LangChain components\n",
    "We will import the `HuggingFaceEmbeddings` and `RecursiveCharacterTextSplitter` to assist in storing the documents.\\\n",
    "The embedding model is, `sentence-transformers/all-mpnet-base-v2`\n",
    "\n",
    "##### The Vector Store\n",
    "We need to store our document in a vector store. \\\n",
    "There are more than <B>30 vector stores (DBs)</B> supported by LangChain. \\\n",
    "For this example we will use FAISS, a popular open source vector store by Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254ae7e-97c6-409c-917d-e30226e32734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\"https://huggingface.co/blog/llama3\"])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec27ab-668a-4b6b-8b68-8a049cdc7a87",
   "metadata": {},
   "source": [
    "#### Chunk the docs, build the Vector DB\n",
    "To store the documents, we will need to split them into chunks using `RecursiveCharacterTextSplitter` and create vector representations \\\n",
    "of these chunks using `HuggingFaceEmbeddings` on them before storing them into our vector database. \\\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. \\\n",
    "You may need to experiment with different chunk sizes and overlap values to find out the best numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8e337-d467-4094-89dd-85cbd6eb80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1f642-7000-4a13-acc4-f2f3c9073175",
   "metadata": {},
   "source": [
    "#### Retrieve using Semantic Search\n",
    "\n",
    "We then use `RetrievalQA` to retrieve the documents from the vector database and give the model more context on Llama 3.1, thereby reducing its hallucination. \\\n",
    "LLama 3.1 also really shines with the new 128k context!\n",
    "\n",
    "For each question, LangChain performs a semantic similarity search of it in the vector db, then passes the search results as the context to Llama to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba12173-8078-40f6-88e6-05b752c04030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama 3 with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"What's new with Llama 3?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(f\"Response:\\n{result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2fcde-f481-4003-a4cf-87c4f6f542c4",
   "metadata": {},
   "source": [
    "#### Followup without memory\n",
    "Now, lets bring it all together by incorporating follow up questions.\\\n",
    "First we ask a follow up questions without giving the model context of the previous conversation. \\\n",
    "Without this context, the answer we get does not relate to our original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f868b-0468-4a1b-be63-99ccf1be814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no context passed so Llama 3 doesn't have enough context to answer so it lets its imagination run wild\n",
    "result = qa_chain({\"query\": \"Based on what architecture?\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8e382-24f7-4d65-83dc-2feb99952068",
   "metadata": {},
   "source": [
    "As we did before, let us use the `ConversationalRetrievalChain` package to give the model context of our \\\n",
    "previous question so we can add follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef82eaa-9c4c-413f-860e-629b1fbe6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ConversationalRetrievalChain to pass chat history for follow up questions\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242750e9-d052-49c2-971a-9245b6f42894",
   "metadata": {},
   "source": [
    "#### Followup with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda732a-fd90-4c3e-b583-da72f736e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's ask the original question What's new with Llama 3?\" again\n",
    "result = chat_chain({\"question\": question, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9589f-34f7-430d-95fe-07cb06ec4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we pass chat history along with the follow up so good things should happen\n",
    "chat_history = [(question, result[\"answer\"])]\n",
    "followup = \"Based on what architecture?\"\n",
    "followup_answer = chat_chain({\"question\": followup, \"chat_history\": chat_history})\n",
    "print(followup_answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5957b-cb28-4281-be31-1cb5494d9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further follow ups can be made possible by updating chat_history like this:\n",
    "chat_history.append((followup, followup_answer[\"answer\"]))\n",
    "more_followup = \"What changes in vocabulary size?\"\n",
    "more_followup_answer = chat_chain({\"question\": more_followup, \"chat_history\": chat_history})\n",
    "print(more_followup_answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356e227-7aab-4985-8339-d116f76a90f2",
   "metadata": {},
   "source": [
    "<B>Additional Note</B>: If results get cut off, you can set \"max_new_tokens\" in the Replicate call above to a larger number (like shown below) to avoid the cut off.\n",
    "\n",
    "model_kwargs={\"temperature\": 0.01, \"top_p\": 1, \"max_new_tokens\": 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b09f0-224e-4cb6-9c0e-b5bf8f96f7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
