{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Intro\n",
    "This example shows you how to use Approximate Nearest Neighbor Search (ANN) using [Hnswlib](https://github.com/nmslib/hnswlib/).\n",
    "\n",
    "Install it with: `pip install hnswlib` (along with other requirements)\n",
    "\n",
    "The embeddings model is `msmarco-distilbert-base-v4` from [SBERT](https://sbert.net/examples/applications/semantic-search/README.html)\n",
    "\n",
    "The dataset we use is our own, (at ../dish-washer-data.csv); that gives us a way to compare with naive search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hnswlib\n",
    "!pip install pandas\n",
    "!pip3 install torch\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import hnswlib\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'msmarco-distilbert-base-v4'\n",
    "embeddings_model = SentenceTransformer(model_name)\n",
    "embedding_size = 768    #Size of embeddings\n",
    "top_k_hits = 5         #Output k hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to get the embeddings from the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: SentenceTransformer) -> list[float]:\n",
    "    embeddings = model.encode([text])\n",
    "    return embeddings\n",
    "\n",
    "def get_doc_embedding(text: str, model: SentenceTransformer) -> list[float]:\n",
    "    return get_embedding(text, model)\n",
    "\n",
    "def get_query_embedding(text: str, model: SentenceTransformer) -> list[float]:\n",
    "    return get_embedding(text, model)\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame, model: SentenceTransformer) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe.\n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \"), model) for idx, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the first run, generate the embeddings, if not, then load the previously generated embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cache_path = f\"dishwasher-repair-manual-embeddings-{model_name.replace('/', '_')}.pkl\"\n",
    "\n",
    "#Check if embeddings cache path exists\n",
    "if not os.path.exists(embeddings_cache_path):\n",
    "    df = pd.read_csv('../dish-washer-data.csv')\n",
    "    df[\"tokens\"] = pd.to_numeric(df[\"tokens\"])  # convert column \"tokens\" of a DataFrame\n",
    "    df = df.set_index([\"title\", \"heading\"])\n",
    "    print(f\"{len(df)} rows in the data.\")\n",
    "    print(df.sample(10))\n",
    "    # TODO get some stats on max/min content length\n",
    "    corpus_size = df.shape[0]\n",
    "\n",
    "    # This could take a bit of time\n",
    "    print(\"Encoding the corpus. This might take a while...\")\n",
    "    corpus_embeddings = compute_doc_embeddings(df, embeddings_model)\n",
    "\n",
    "    print(\"Store file...\")\n",
    "    with open(embeddings_cache_path, \"wb\") as fOut:\n",
    "        pickle.dump({'sentences': df, 'embeddings': corpus_embeddings}, fOut)\n",
    "else:\n",
    "    print(\"Loading pre-computed embeddings from disc...\")\n",
    "    with open(embeddings_cache_path, \"rb\") as f_in:\n",
    "        cache_data = pickle.load(f_in)\n",
    "        corpus_sentences = cache_data['sentences']\n",
    "        corpus_embeddings = cache_data['embeddings']\n",
    "        corpus_embeddings = [v for k, v in corpus_embeddings.items()]\n",
    "        print(\"Loading done.\")\n",
    "        print(f'Length, corpus sentences:{len(corpus_sentences)}')\n",
    "        print(f'Length, corpus embeddings:{len(corpus_embeddings)}')\n",
    "        print(f'one vector dimension: {len(corpus_embeddings[0][0])}')\n",
    "\n",
    "        print(f\"Sentence samples\\n {corpus_sentences.sample(5)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our hnswlib index\n",
    "index_path = \"./hnswlib.index\"\n",
    "index = hnswlib.Index(space = 'cosine', dim = embedding_size)\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    print(\"Loading index...\")\n",
    "    index.load_index(index_path)\n",
    "    print('Loading index done.')\n",
    "else:\n",
    "    ### Create the HNSWLIB index\n",
    "    print(\"Start creating HNSWLIB index\")\n",
    "    # TODO check is ef_consturction and M and appropriate for this dataset\n",
    "    index.init_index(max_elements = len(corpus_embeddings), ef_construction = 400, M = 64)\n",
    "\n",
    "    # create ids for the embeddings, ids are optional N-size numpy array of integer labels for all elements in data. \n",
    "    ids = np.arange(len(corpus_embeddings))\n",
    "    print(f'ids:\\n{ids}')\n",
    "    \n",
    "    # Then we train the index to find a suitable clustering\n",
    "    corpus_embeddings = np.array(corpus_embeddings).squeeze()\n",
    "    print('shape', corpus_embeddings.shape)\n",
    "    print(f'corpus embeddings:\\n{corpus_embeddings}')\n",
    "    index.add_items(corpus_embeddings, ids)\n",
    "    # ?? index.add_items(corpus_embeddings, list(corpus_sentences.index))\n",
    "\n",
    "    print(\"Saving index to:\", index_path)\n",
    "    index.save_index(index_path)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "index.set_ef(50)  # ef should always be > top_k_hits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose questions\n",
    "# inp_question = \"Why is my dishwasher leaking?\"\n",
    "inp_question = \"Why do we use a dishwasher?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "question_embedding = embeddings_model.encode(inp_question)\n",
    "\n",
    "# Use hnswlib knn_query method to find the top_k_hits\n",
    "corpus_ids, distances = index.knn_query(question_embedding, k=top_k_hits)\n",
    "\n",
    "# Extract corpus ids and scores for the query\n",
    "hits = [{'corpus_id': id, 'score': 1 - score} for id, score in zip(corpus_ids[0], distances[0])]\n",
    "hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "end_time = time.time()\n",
    "print(f'Hits\\n, {hits}')\n",
    "\n",
    "print(\"Input question:\", inp_question)\n",
    "print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n",
    "for hit in hits[0:top_k_hits]:\n",
    "    print(f\"\\t{hit['score']:.3f}\\t{corpus_sentences.iloc[hit['corpus_id']].content}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate Nearest Neighbor (ANN) is not exact, it might miss entries with high cosine similarity\n",
    "# Here, we compute the recall of ANN compared to the exact results\n",
    "correct_hits = util.semantic_search(torch.tensor(question_embedding), \n",
    "                                    torch.tensor(corpus_embeddings).squeeze(), \n",
    "                                    top_k=top_k_hits)[0]\n",
    "\n",
    "print(f'Correct hits:\\n {correct_hits}')\n",
    "\n",
    "for hit in correct_hits[0:top_k_hits]:\n",
    "    print(f\"\\t{hit['score']:.3f}\\t{corpus_sentences.iloc[hit['corpus_id']].content}\")\n",
    "\n",
    "correct_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n",
    "\n",
    "ann_corpus_ids = set([hit['corpus_id'] for hit in hits])\n",
    "if len(ann_corpus_ids) != len(correct_hits_ids):\n",
    "    print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n",
    "\n",
    "recall = len(ann_corpus_ids.intersection(correct_hits_ids)) / len(correct_hits_ids)\n",
    "print(\"\\nApproximate Nearest Neighbor Recall@{}: {:.2f}\".format(top_k_hits, recall * 100))\n",
    "\n",
    "if recall < 1:\n",
    "    print(\"Missing results:\")\n",
    "    for hit in correct_hits[0:top_k_hits]:\n",
    "        if hit['corpus_id'] not in ann_corpus_ids:\n",
    "            print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n",
    "print(\"\\n\\n========\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c5ba64e8a2a59a6be0dd5b8a0149005a96dade111bc9e6b7c5d65266a44d405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
