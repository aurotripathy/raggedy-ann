{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Intro\n",
    "The dataset is scraped from https://www.appliancerepair.net/dishwasher-repair-1.html and follow-on pages.\n",
    "\n",
    "Update Oct 23, the page is no longer available, but the chunking is already done, so we're OK\n",
    "\n",
    "Scraper code is at scraper.py.\n",
    "\n",
    "The embeddings are generated using the [SentenceTransformer](https://www.sbert.net/) package (no finetuning)\n",
    "\n",
    "The overall flow is from \n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\n",
    "\n",
    "The difference is, instead of using OpenAI Curie to create the embeddings, I create embeddings using SBERT and use cosine simialrity to retrieve relevant pieces of text.\n",
    "\n",
    "You'll need an OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas -q\n",
    "%pip install openai -q\n",
    "%pip install getpass4\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "print(f'openai api version: {openai.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: SentenceTransformer) -> list[float]:\n",
    "    embeddings = model.encode([text])\n",
    "    return embeddings\n",
    "\n",
    "def get_doc_embedding(text: str, model: SentenceTransformer) -> list[float]:\n",
    "    return get_embedding(text, model)\n",
    "\n",
    "def get_query_embedding(text: str, model: SentenceTransformer) -> list[float]:\n",
    "    return get_embedding(text, model)\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame, model: SentenceTransformer) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe.\n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \"), model) for idx, r in df.iterrows()\n",
    "    }\n",
    "\n",
    "def load_embeddings(fname: str) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Read the document embeddings and their keys from a CSV.\n",
    "    Again, we have hosted the embeddings for you so you don't have to re-calculate them from scratch.\n",
    "    \n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scraper generated the csv file below\n",
    "df = pd.read_csv('dish-washer-data.csv')\n",
    "df[\"tokens\"] = pd.to_numeric(df[\"tokens\"])  # convert column \"tokens\" of a DataFrame\n",
    "df = df.set_index([\"title\", \"heading\"])\n",
    "print(f\"{len(df)} rows in the data.\")\n",
    "df.sample(10)\n",
    "# TODO get some stats on max/min content length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_model_path = 'msmarco-distilbert-base-v4'\n",
    "embeddings_model = SentenceTransformer(embeddings_model_path)\n",
    "print(f'Default sequence length:{embeddings_model.max_seq_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could take a bit of time\n",
    "document_embeddings = compute_doc_embeddings(df, embeddings_model)\n",
    "\n",
    "# An example embedding:\n",
    "example_entry = list(document_embeddings.items())[0]\n",
    "# print(example_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total documents, {len(document_embeddings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    We use cosine similarity \n",
    "    \"\"\"\n",
    "    return util.cos_sim(x,y)\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array], model: SentenceTransformer) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_query_embedding(query, model)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a question and get top_k relevant text\n",
    "top_k = 5\n",
    "by_semantic_relevance = order_document_sections_by_query_similarity(\"Why is my dishwasher leaking?\", document_embeddings, embeddings_model)[:top_k]\n",
    "for i in range(top_k):\n",
    "    index = by_semantic_relevance[i][1]  # index\n",
    "    print(f\"index: {index} {[i]}:  {df.loc[index]['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask another question\n",
    "top_k = 5\n",
    "by_semantic_relevance = order_document_sections_by_query_similarity(\"Why is my dish washer not cleaning well?\", document_embeddings, embeddings_model)[:top_k]\n",
    "for i in range(top_k):\n",
    "    index = by_semantic_relevance[i][1]  # [0] similarity [1]index\n",
    "    print(f\"{[i]} {df.loc[index]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO what is this?\n",
    "from transformers import GPT2TokenizerFast\n",
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "separator_len = len(tokenizer.tokenize(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame, embeddings_model: SentenceTransformer) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings, embeddings_model)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        \n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index]\n",
    "\n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            # print(f'Enough context---run out of length of {MAX_SECTION_LEN}')\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section['content'].replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    # print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    # print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    # The context\n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the prompt\n",
    "prompt = construct_prompt(\n",
    "    \"Why is my dish wasker leaking?\",\n",
    "    document_embeddings,\n",
    "    df,\n",
    "    embeddings_model\n",
    ")\n",
    "\n",
    "print(\"===\\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering the question from a context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_query_from_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings: dict[(str, str), np.array],\n",
    "    show_prompt: bool = False\n",
    ") -> str:\n",
    "    prompt = construct_prompt(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df,\n",
    "        embeddings_model\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    print(completion.choices[0].message.content)\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = answer_query_with_context(\"Who won the 2020 Summer Olympics men's high jump?\", df, document_embeddings)\n",
    "from scipy.__config__ import show\n",
    "\n",
    "\n",
    "response = answer_query_from_context(\"What are the ways to prevent water leakage?\", df, document_embeddings, show_prompt=True)\n",
    "\n",
    "answer = response[0:len(response) + 1].split('A:')[-1].strip()\n",
    "\n",
    "print(f'====Answer\\n\" {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer_query_from_context(\"Explain the various cycles?\", df, document_embeddings, show_prompt=True)\n",
    "\n",
    "answer = response[0:len(response) + 1].split('A:')[-1].strip()\n",
    "\n",
    "print(f'====Answer\\n {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = answer_query_from_context(\"Why is my dishwasher leaking?\", df, document_embeddings, show_prompt=True)\n",
    "\n",
    "answer = response[0:len(response) + 1].split('A:')[-1].strip()\n",
    "\n",
    "print(f'====Answer\\n\" {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer_query_from_context(\"What's the biggest thing to worry about?\", df, document_embeddings, show_prompt=True)\n",
    "\n",
    "answer = response[0:len(response) + 1].split('A:')[-1].strip()\n",
    "\n",
    "print(f'====Answer\\n\" {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer_query_from_context(\"What are the sources of water leaks?\", df, document_embeddings, show_prompt=True)\n",
    "\n",
    "answer = response[0:len(response) + 1].split('A:')[-1].strip()\n",
    "\n",
    "print(f'====Answer\\n\" {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer_query_from_context(\"Tell me a bit about the warranty\", df, document_embeddings, show_prompt=True)\n",
    "\n",
    "answer = response[0:len(response) + 1].split('A:')[-1].strip()\n",
    "\n",
    "print(f'====Answer\\n\" {answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c5ba64e8a2a59a6be0dd5b8a0149005a96dade111bc9e6b7c5d65266a44d405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
