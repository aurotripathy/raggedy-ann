{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4834a618-cb3c-4f71-b4f1-a18063afcc11",
   "metadata": {},
   "source": [
    "# RAG Retrieval Enhanced with Document Summaries\n",
    "In this section, we'll implement an improved approach to our retrieval system by incorporating document summaries. Instead of embedding chunks directly from the documents, we'll create a concise summary for each chunk and use this summary along with the original content in our embedding process.\n",
    "\n",
    "This approach aims to capture the essence of each document chunk more effectively, potentially leading to improved retrieval performance.\n",
    "\n",
    "Key steps in this process:\n",
    "\n",
    "1. We load the original document chunks.\n",
    "2. For each chunk, we generate a 2-3 sentence summary using OpenAI (or an OpenAI compatible API).\n",
    "3. We store both the original content and the summary for each chunk in a new json file: data/anthropic_summary_indexed_docs.json\n",
    "\n",
    "This summary-enhanced approach is designed to provide more context during the embedding and retrieval phases, potentially improving the system's ability to understand and match the most relevant documents to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b80e4-3558-445c-a17c-5a4b8db4cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## silent setup (-q), may take a while\n",
    "!pip install openai -q\n",
    "!pip install --upgrade tiktoken -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af37b9a7-0878-4b8d-ae76-ad694cb512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "embeddings_model = \"intfloat/multilingual-e5-large-instruct\"; generation_model = \"gpt-4o-mini\"; judge_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d1e786-d81b-411c-b2e0-8d618a5f5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262fa9ab-559b-41be-9de3-4ae757c2fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84b2907eab349478e8bb33854c57217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf38f4caff1f4ff0b2d7399b9d13dff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3664ce31970e42a7bb579e71e0effc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fa0a6d7f874ffb92d4e4ef698d0a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ae04912c2141c8af387bf59ec0e910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8657fe0bab734def90e41637cddd37c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa92e0e5cf641ff8bf671dd08ded81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400abdb22a8a4776bf832713ea41d842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b3514e436946fab527f1a7bffd5aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9639db8dceaf4653a4bdfdc7e7af2a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddings_model = SentenceTransformer(embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54311b-bba4-40d4-b4fe-f17e54991e10",
   "metadata": {},
   "source": [
    "### Generating the Summaries and Storing Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b741b86c-f53e-4220-9c5b-bbbe6b7db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, this is for Claud-3-haiku, need to be changed to OpenAI or Llama\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summaries(input_file, output_file):\n",
    " \n",
    "    # Load the original documents\n",
    "    with open(input_file, 'r') as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    # Prepare the context about the overall knowledge base\n",
    "    knowledge_base_context = \"This is documentation for Anthropic's, a frontier AI lab building Claude, an LLM that excels at a variety of general purpose tasks. These docs contain model details and documentation on Anthropic's APIs.\"\n",
    "\n",
    "    summarized_docs = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Generating summaries\"):\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with creating a short summary of the following content from Anthropic's documentation. \n",
    "\n",
    "        Context about the knowledge base:\n",
    "        {knowledge_base_context}\n",
    "\n",
    "        Content to summarize:\n",
    "        Heading: {doc['chunk_heading']}\n",
    "        {doc['text']}\n",
    "\n",
    "        Please provide a brief summary of the above content in 2-3 sentences. The summary should capture the key points and be concise. We will be using it as a key part of our search pipeline when answering user queries about this content. \n",
    "\n",
    "        Avoid using any preamble whatsoever in your response. Statements such as 'here is the summary' or 'the summary is as follows' are prohibited. You should get straight into the summary itself and be concise. Every word matters.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=150,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        summary = response.content[0].text.strip()\n",
    "\n",
    "        summarized_doc = {\n",
    "            \"chunk_link\": doc[\"chunk_link\"],\n",
    "            \"chunk_heading\": doc[\"chunk_heading\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"summary\": summary\n",
    "        }\n",
    "        summarized_docs.append(summarized_doc)\n",
    "\n",
    "    # Save the summarized documents to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(summarized_docs, f, indent=2)\n",
    "\n",
    "    print(f\"Summaries generated and saved to {output_file}\")\n",
    "    \n",
    "# this is already available, so the call is commented out\n",
    "# generate_summaries('data/anthropic_docs.json', 'data/anthropic_summary_indexed_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cfea-b81d-4d06-9c8f-337f9ecc95c9",
   "metadata": {},
   "source": [
    "### Summary-Enhanced Vector Database Creation (heading + summary + chunk)\n",
    "Here, we're creating a new vector database that incorporates our summary-enhanced document chunks. This approach combines the original text, the chunk heading, and the newly generated summary into a single text for embedding.\n",
    "\n",
    "Key features of this process:\n",
    "\n",
    "1. We create embeddings for the combined text (heading + summary + original content) using the Voyage AI API.\n",
    "2. The embeddings and full metadata (including summaries) are stored in our vector database.\n",
    "3. We implement caching mechanisms to improve efficiency in repeated queries.\n",
    "4. The database is saved to disk for persistence and quick loading in future sessions.\n",
    "\n",
    "This summary-enhanced approach aims to create more informative embeddings, potentially leading to more accurate and contextually relevant document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351e638a-d09a-4295-8de6-6c54ac6e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class SummaryEnhancedVectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/summary_indexed_vector_db.pkl\"\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        \"\"\"not called for now\"\"\"\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "        \n",
    "    def load_data(self, data_file):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Loading vector database from file: {self.db_path}.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "            \n",
    "        # well, if not...\n",
    "        print(f'file {self.db_path} does not exist')\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        texts = [f\"{item['chunk_heading']}\\n\\n{item['text']}\\n\\n{item['summary']}\" for item in data]  # Embed Chunk Heading + Text + Summary Together\n",
    "        # Embed more than 128 documents with a for loop\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data  # Store the entire item as metadata\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=3, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            # print(f'found in cache!')\n",
    "            query_embedding = np.array(self.query_cache[query])  #\n",
    "            # print(f'type:{type(query_embedding)}')\n",
    "        else:\n",
    "            query_embedding = embeddings_model.encode(query)\n",
    "            # print(f'query embedding:\\n {query_embedding}')\n",
    "            self.query_cache[query] = query_embedding.tolist()\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        print(f'Saving DB in: {self.db_path}')\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c306c5-1d54-4ca0-9a14-4947caae1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 3 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=3):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4e912-1419-4078-a063-c83d30b9a9de",
   "metadata": {},
   "source": [
    "### Enhanced Retrieval Using Summary-Enhanced Embeddings\n",
    "In this section, we implement the retrieval process using our new summary-enhanced vector database. This approach leverages the enhanced embeddings we created, which incorporate document summaries along with the original content.\n",
    "\n",
    "Key aspects of this updated retrieval process:\n",
    "\n",
    "1. We search the vector database using the query embedding, retrieving the top k most similar documents.\n",
    "2. For each retrieved document, we include the chunk heading, summary, and full text in the context provided to the LLM.\n",
    "3. This enriched context is then used to generate an answer to the user's query.\n",
    "\n",
    "By including summaries in both the embedding and retrieval phases, we aim to provide the LLM with a more comprehensive and focused context. This could potentially lead to more accurate and relevant answers, as the LLM has access to both a concise overview (the summary) and the detailed information (the full text) for each relevant document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a875906-ca83-4bb2-bdce-d8508e45025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl does not exist\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n",
      "Vector database loaded and saved.\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n",
      "ith:0\n",
      " {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic', 'chunk_heading': 'How to get embeddings with Anthropic', 'text': 'How to get embeddings with Anthropic\\n\\n\\nAnthropic does not offer its own embedding model. One embeddings provider that has a wide variety of options and capabilities encompassing all of the above considerations is Voyage AI.\\nVoyage AI makes state-of-the-art embedding models and offers customized models for specific industry domains such as finance and healthcare, or bespoke fine-tuned models for individual customers.\\nThe rest of this guide is for Voyage AI, but we encourage you to assess a variety of embeddings vendors to find the best fit for your specific use case.\\n', 'summary': 'Anthropic does not offer its own embedding model. Voyage AI is recommended as a provider of state-of-the-art embedding models, including customized and fine-tuned options for specific use cases.'}, 'similarity': np.float32(0.8849898)}\n",
      "ith:1\n",
      " {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/intro-to-claude#model-options', 'chunk_heading': 'Model options', 'text': 'Model options\\n\\n\\nEnterprise use cases often mean complex needs and edge cases. Anthropic offers a range of models across the Claude 3 and Claude 3.5 families to allow you to choose the right balance of intelligence, speed, and cost.\\n', 'summary': 'Anthropic offers a range of Claude 3 and Claude 3.5 models to cater to the complex needs and edge cases of enterprise use cases, allowing users to choose the right balance of intelligence, speed, and cost.'}, 'similarity': np.float32(0.8613833)}\n",
      "ith:2\n",
      " {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings', 'chunk_heading': 'Before implementing embeddings', 'text': 'Before implementing embeddings\\n\\n\\nWhen selecting an embeddings provider, there are several factors you can consider depending on your needs and preferences:\\nDataset size & domain specificity: size of the model training dataset and its relevance to the domain you want to embed. Larger or more domain-specific data generally produces better in-domain embeddings\\nInference performance: embedding lookup speed and end-to-end latency. This is a particularly important consideration for large scale production deployments\\nCustomization: options for continued training on private data, or specialization of models for very specific domains. This can improve performance on unique vocabularies\\n', 'summary': 'When selecting an embeddings provider, consider the dataset size and domain specificity, inference performance, and customization options. Larger or more domain-specific training data, faster embedding lookup, and the ability to fine-tune models can improve the quality and relevance of the embeddings for your use case.'}, 'similarity': np.float32(0.8572701)}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "def retrieve_similar_level_two(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n <document> \\n {chunk['chunk_heading']}\\n\\nText\\n {chunk['text']} \\n\\nSummary: \\n {chunk['summary']} \\n </document> \\n\" #show model all 3 items\n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context_level_two(query, db):\n",
    "    documents, context = retrieve_similar_level_two(query, db)\n",
    "    completion = client.chat.completions.create(\n",
    "    model=generation_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Initialize the SummaryEnhancedVectorDB\n",
    "level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "level_two_db.save_db()\n",
    "\n",
    "# # Load the Anthropic documentation\n",
    "# with open('data/anthropic_docs.json', 'r') as f:\n",
    "#     anthropic_docs = json.load(f)\n",
    "\n",
    "# test\n",
    "query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\n",
    "test_results, test_contexts = retrieve_similar_level_two(query, level_two_db)\n",
    "for i, test_result in enumerate(test_results):\n",
    "    print(f'ith:{i}\\n {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daa157-bd0c-4462-be43-f2a7d1f06bc4",
   "metadata": {},
   "source": [
    "### Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b51398e7-2da9-47ca-90f8-e4e565f6108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"For OpenAI models, returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db) # ??\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_tokens = num_tokens_from_string(comparision_prompt, \"o200k_base\")  # note, this encoding name for gpt-4o, gpt-4o-mini\n",
    "        # print(f'Number of tokens: {nb_tokens}')\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'Number of query tokens: {nb_tokens}, Query:\\n{query}')\n",
    "            print(f'__Correct answer__:\\n{correct_answer}')\n",
    "            print(f'__Generated answer__:\\n{generated_answer}')\n",
    "            print(f'__Response text from judge LLM__:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8852c4-15df-422c-bdb7-cccf25f8da08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from file: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 10/10 [00:00<00:00, 36.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/10 items. Current Avg Precision: 0.4667, Avg Recall: 0.7500, Avg MRR: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 1/10 [00:04<00:37,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 506, Query:\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\n",
      "__Correct answer__:\n",
      "To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\n",
      "__Generated answer__:\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. **Access the Evaluation Tool**: Open the Anthropic Console and navigate to the prompt editor. After composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\n",
      "\n",
      "2. **Prepare Your Prompt**: Ensure your prompt includes at least 1-2 dynamic variables using the double brace syntax (e.g., {{variable}}). This is necessary for creating evaluation test sets.\n",
      "\n",
      "3. **Add Test Cases**: On the Evaluation screen, you will see a single row for your initial test case. To add more test cases, click the ‘Add Test Case’ button.\n",
      "\n",
      "4. **Fill in Variables**: For each new test case, fill in the values for each variable in your prompt. Repeat this process to create multiple scenarios.\n",
      "\n",
      "5. **Update and Re-run**: If you update your original prompt text, you can re-run the entire evaluation suite against the new prompt to see how changes affect performance across all test cases.\n",
      "\n",
      "By following these steps, you can effectively create and manage multiple test cases for your evaluation.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly outlines the steps to create multiple test cases in the Anthropic Evaluation tool, including accessing the tool, preparing the prompt with dynamic variables, adding test cases, filling in variables, and updating the prompt. It includes all the essential information found in the correct answer and provides additional context that enhances understanding. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 2/10 [00:07<00:28,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 336, Query:\n",
      "What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\n",
      "__Correct answer__:\n",
      "Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\n",
      "__Generated answer__:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models and provides options for customized models tailored to specific industry domains, such as finance and healthcare, as well as bespoke fine-tuned models for individual customers.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer. It mentions that Anthropic recommends Voyage AI for embedding models and highlights the capabilities of Voyage AI, including customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 3/10 [00:15<00:39,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 637, Query:\n",
      "What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\n",
      "__Correct answer__:\n",
      "When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\n",
      "__Generated answer__:\n",
      "Key success metrics to consider when evaluating Claude's performance on a classification task include:\n",
      "\n",
      "1. **Accuracy**: Measures the proportion of correct predictions made by the model. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the accuracy of the positive class predictions and the model's ability to identify all relevant instances.\n",
      "\n",
      "3. **Consistency**: Evaluates whether the model's outputs are stable and logical across similar inputs, ensuring reliability in predictions.\n",
      "\n",
      "4. **Structure**: Assesses whether the model's outputs adhere to expected formats, such as JSON, which facilitates easier parsing and interpretation.\n",
      "\n",
      "5. **Speed**: Refers to the model's response time, which is crucial for applications requiring low latency. This metric directly impacts user experience and operational efficiency.\n",
      "\n",
      "6. **Bias and Fairness**: Important for ensuring that the model does not exhibit biases based on sensitive attributes like gender or ethnicity, which could lead to unfair classifications.\n",
      "\n",
      "When choosing the right model to reduce latency, the speed metric becomes particularly important. Smaller models, such as Claude 3 Haiku, are typically preferred for their efficiency and faster response times. In contrast, larger models like Sonnet or Opus may be necessary for tasks requiring more complex reasoning or specialized knowledge, but they often come with increased latency. Therefore, the choice of model involves a trade-off between accuracy, speed, and cost, with a focus on selecting a model that meets the specific requirements of the classification task while minimizing latency.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the key success metrics for evaluating Claude's performance on a classification task, including accuracy, F1 score, consistency, structure, speed, and bias and fairness. It also discusses the importance of choosing the right model to reduce latency, emphasizing the trade-off between model size, speed, and accuracy. While it provides additional context about model types (Claude 3 Haiku, Sonnet, Opus), this does not contradict the correct answer. Therefore, the generated answer is consistent with the correct answer in substance and detail.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 4/10 [00:20<00:31,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 413, Query:\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\n",
      "__Correct answer__:\n",
      "Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\n",
      "__Generated answer__:\n",
      "Claude for Sheets can improve prompt engineering workflows compared to using chained prompts in two significant ways:\n",
      "\n",
      "1. **Parallel Testing of Prompts**: Claude for Sheets enables users to test multiple prompts across evaluation suites simultaneously. This capability allows for more efficient experimentation and optimization of prompts, as users can quickly compare results and iterate on their designs without the sequential limitations of chained prompts.\n",
      "\n",
      "2. **Resource for Learning and Exploration**: The Claude for Sheets prompting examples workbench serves as a comprehensive resource that houses example prompts and prompt engineering structures. This facilitates learning and exploration of effective prompt engineering techniques, which can enhance the quality and effectiveness of prompts beyond what might be achieved through simple chaining.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts: the ability to test prompts in parallel and the resource for learning and exploration. However, it introduces a new point about the prompting examples workbench, which is not mentioned in the correct answer. The correct answer focuses on the speed of testing and the suitability for office tasks, which are not addressed in the generated answer. Therefore, while the generated answer contains valid points, it misses critical information from the correct answer, leading to a conclusion that it is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 5/10 [00:23<00:22,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 342, Query:\n",
      "What happens if a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns?\n",
      "__Correct answer__:\n",
      "If a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error.\n",
      "__Generated answer__:\n",
      "If a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. The API requires these specific markers to properly identify the roles of the participants in the conversation. Without them, the API cannot process the prompt correctly.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that if a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. It also adds that these markers are necessary for the API to identify the roles of the participants in the conversation, which is additional relevant information. Therefore, the generated answer is correct and provides a more comprehensive explanation without contradicting the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 6/10 [00:31<00:22,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 505, Query:\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?\n",
      "__Correct answer__:\n",
      "Tool use requests in the Claude API are priced the same as regular API requests, based on the total input and output tokens. However, tool use requests have additional tokens beyond the regular input and output, including the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt that enables tool use, which add to the total tokens and cost.\n",
      "__Generated answer__:\n",
      "The additional tokens required for tool use in Claude API requests impact pricing by increasing the total number of tokens counted for each request. Tool use requests are priced based on the total number of input tokens sent to the model, which includes tokens from the tools parameter (such as tool names, descriptions, and schemas), as well as tokens from tool_use and tool_result content blocks. \n",
      "\n",
      "Moreover, there is a specific number of tokens added for the tool use system prompt, which varies by model. For instance, the Claude 3.5 Sonnet model requires an additional 294 tokens for tool choice and 261 tokens for tool use, while other models have different token counts ranging from 159 to 530 tokens. \n",
      "\n",
      "These additional tokens are added to the normal input and output tokens to calculate the total cost of a request. Therefore, using tools will generally result in higher costs compared to regular API requests, as the total token count will be greater due to these additional requirements.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that tool use requests in the Claude API are priced based on the total number of tokens, which includes additional tokens for the tools parameter, tool use content blocks, and tool result content blocks. It also mentions the specific number of additional tokens required for different models, which adds detail. However, it incorrectly implies that using tools will generally result in higher costs compared to regular API requests, which contradicts the correct answer's assertion that tool use requests are priced the same as regular API requests based on total tokens. This critical piece of information about the pricing being the same is missing from the generated answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 7/10 [00:33<00:13,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 311, Query:\n",
      "When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\n",
      "__Correct answer__:\n",
      "The new Usage, Cost, and Rate Limits tabs in the Anthropic Developer Console that show API usage, billing details, and current rate limits will be available on June 27th, 2024.\n",
      "__Generated answer__:\n",
      "The new Anthropic Developer Console features that show API usage, billing details, and rate limits will be available on June 27th, 2024.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the new Anthropic Developer Console features showing API usage, billing details, and rate limits will be available on June 27th, 2024, which is exactly the same information provided in the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 8/10 [00:39<00:09,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 428, Query:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\n",
      "__Correct answer__:\n",
      "When deciding whether to use CoT, consider if the task requires in-depth thinking that a human would need to work through, and be aware that the increased output length from CoT may impact latency.\n",
      "__Generated answer__:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, two key factors to consider are:\n",
      "\n",
      "1. **Task Complexity**: CoT should be used for tasks that require in-depth thinking, such as complex math, multi-step analysis, or decisions with many factors. If the task does not necessitate extensive processing, it may be better to avoid CoT to maintain efficiency.\n",
      "\n",
      "2. **Latency Impact**: The use of CoT can increase output length, which may negatively impact latency. It's important to weigh the benefits of improved performance against the potential delay in response time, ensuring that the chosen approach aligns with the requirements of the task at hand.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two key factors to consider when deciding whether to use chain-of-thought (CoT): task complexity and latency impact. It emphasizes the need for in-depth thinking for complex tasks and acknowledges the potential increase in output length affecting latency. These points align well with the correct answer, which also mentions the need for in-depth thinking and the impact on latency. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 9/10 [00:46<00:05,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 631, Query:\n",
      "How can I use Claude to more easily digest the content of long PDF documents?\n",
      "__Correct answer__:\n",
      "You can upload PDFs and have Claude summarize their content, making it easier to understand the key points of long documents without having to read through everything.\n",
      "__Generated answer__:\n",
      "To use Claude for digesting long PDF documents more effectively, follow these strategies:\n",
      "\n",
      "1. **Place Longform Data at the Top**: Start your prompt with the content of the PDF document. This ensures that Claude processes the most relevant information first, which can significantly enhance its performance, especially with lengthy documents (around 20K+ tokens).\n",
      "\n",
      "2. **Structure Content with XML Tags**: If you're working with multiple documents, organize the content using XML tags. For example, wrap each document in `<document>` tags and include `<document_content>` and `<source>` tags for clarity. This structured approach helps Claude understand the context better.\n",
      "\n",
      "   Example structure:\n",
      "   ```xml\n",
      "   <documents>\n",
      "     <document index=\"1\">\n",
      "       <source>document1.pdf</source>\n",
      "       <document_content>{{CONTENT_OF_DOCUMENT1}}</document_content>\n",
      "     </document>\n",
      "     <document index=\"2\">\n",
      "       <source>document2.pdf</source>\n",
      "       <document_content>{{CONTENT_OF_DOCUMENT2}}</document_content>\n",
      "     </document>\n",
      "   </documents>\n",
      "   ```\n",
      "\n",
      "3. **Ground Responses in Quotes**: Ask Claude to extract and quote relevant sections from the document before proceeding with any analysis or summarization. This helps filter out unnecessary information and focuses on the most pertinent content.\n",
      "\n",
      "   Example prompt:\n",
      "   ```\n",
      "   Find quotes from the document that summarize the key findings. Then, based on these quotes, provide a summary of the document's main points.\n",
      "   ```\n",
      "\n",
      "4. **Place Queries at the End**: After providing the longform data and context, place your specific questions or queries at the end of the prompt. This can improve the quality of Claude's responses by allowing it to process the context fully before addressing your questions.\n",
      "\n",
      "By implementing these strategies, you can enhance Claude's ability to digest and summarize long PDF documents effectively.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides detailed strategies for using Claude to digest long PDF documents, including uploading PDFs for summarization, which aligns with the correct answer's main point. While it offers additional methods for enhancing Claude's performance, it does not contradict the correct answer and essentially conveys the same core idea of summarizing content for easier understanding. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 10/10 [00:48<00:00,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 276, Query:\n",
      "According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\n",
      "__Correct answer__:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab of the Developer Console.\n",
      "__Generated answer__:\n",
      "You can view your organization's current API rate limits in the new Rate Limits tab in the Developer Console.\n",
      "__Response text from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that you can view your organization's current API rate limits in the \"new Rate Limits tab\" in the Developer Console, which is essentially the same information as the correct answer that mentions the \"Rate Limits tab of the Developer Console.\" The addition of \"new\" does not change the substance of the answer, so it is still correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/10 questions. Current Accuracy: 0.8000\n",
      "Detailed results saved to evaluation/csvs/evaluation_results_summary_enhanced.csv\n",
      "Average Precision: 0.4667\n",
      "Average Recall: 0.7500\n",
      "Average MRR: 0.8000\n",
      "Average F1: 0.5753\n",
      "End-to-End Accuracy: 0.8000\n",
      "Evaluation complete. Results saved to evaluation/json_results/evaluation_results_summary_enhanced.json, evaluation/csvs/evaluation_results_summary_enhanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "# level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "# level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the evaluations\n",
    "eval_data_range = eval_data[:10]\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_similar_level_two, eval_data_range, level_two_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context_level_two, level_two_db, eval_data_range)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data_range],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_summary_enhanced.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a json file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_summary_enhanced.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Enhanced\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575a0a91-3a1e-4b67-90e5-b38bbda11d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Summary Enhanced\",\n",
      "  \"average_precision\": 0.4666666666666666,\n",
      "  \"average_recall\": 0.75,\n",
      "  \"average_f1\": 0.5753424657534246,\n",
      "  \"average_mrr\": 0.8,\n",
      "  \"end_to_end_accuracy\": 0.7\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/json_results/evaluation_results_summary_enhanced.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea28c0-4e1c-4398-ac76-0739a0a07553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
