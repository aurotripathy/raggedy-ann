{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4834a618-cb3c-4f71-b4f1-a18063afcc11",
   "metadata": {},
   "source": [
    "# RAG Retrieval Enhanced with Document Summaries\n",
    "In this section, we'll implement an improved approach to our retrieval system by incorporating document summaries. Instead of embedding chunks directly from the documents, we'll create a concise summary for each chunk and use this summary along with the original content in our embedding process.\n",
    "\n",
    "This approach aims to capture the essence of each document chunk more effectively, potentially leading to improved retrieval performance.\n",
    "\n",
    "Key steps in this process:\n",
    "\n",
    "1. We load the original document chunks.\n",
    "2. For each chunk, we generate a 2-3 sentence summary using OpenAI (or an OpenAI compatible API).\n",
    "3. We store both the original content and the summary for each chunk in a new json file: data/anthropic_summary_indexed_docs.json\n",
    "\n",
    "This summary-enhanced approach is designed to provide more context during the embedding and retrieval phases, potentially improving the system's ability to understand and match the most relevant documents to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a2b80e4-3558-445c-a17c-5a4b8db4cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## silent setup (-q), may take a while\n",
    "!pip install openai -q\n",
    "!pip install --upgrade tiktoken -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af37b9a7-0878-4b8d-ae76-ad694cb512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "embeddings_model_name = \"intfloat/multilingual-e5-large-instruct\"; generation_model = \"gpt-4o-mini\"; judge_model = \"gpt-4o-mini\"\n",
    "model_temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1d1e786-d81b-411c-b2e0-8d618a5f5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "262fa9ab-559b-41be-9de3-4ae757c2fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length of model, intfloat/multilingual-e5-large-instruct:, 512, about 384.0 words\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddings_model = SentenceTransformer(embeddings_model_name)\n",
    "max_len = embeddings_model.max_seq_length\n",
    "max_word_len = max_len * 0.75\n",
    "print(f\"Max Sequence Length of model, {embeddings_model_name}:, {max_len}, about {max_word_len} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54311b-bba4-40d4-b4fe-f17e54991e10",
   "metadata": {},
   "source": [
    "### Generating the Summaries and Storing Them\n",
    "You can invoke this depending on whether its already available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b741b86c-f53e-4220-9c5b-bbbe6b7db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, this is for Claud-3-haiku, need to be changed to OpenAI or Llama\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summaries(input_file, output_file):\n",
    " \n",
    "    # Load the original documents\n",
    "    with open(input_file, 'r') as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    # Prepare the context about the overall knowledge base\n",
    "    knowledge_base_context = \"This is documentation for Anthropic's, a frontier AI lab building Claude, an LLM that excels at a variety of general purpose tasks. These docs contain model details and documentation on Anthropic's APIs.\"\n",
    "\n",
    "    summarized_docs = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Generating summaries\"):\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with creating a short summary of the following content from Anthropic's documentation. \n",
    "\n",
    "        Context about the knowledge base:\n",
    "        {knowledge_base_context}\n",
    "\n",
    "        Content to summarize:\n",
    "        Heading: {doc['chunk_heading']}\n",
    "        {doc['text']}\n",
    "\n",
    "        Please provide a brief summary of the above content in 2-3 sentences. The summary should capture the key points and be concise. We will be using it as a key part of our search pipeline when answering user queries about this content. \n",
    "\n",
    "        Avoid using any preamble whatsoever in your response. Statements such as 'here is the summary' or 'the summary is as follows' are prohibited. You should get straight into the summary itself and be concise. Every word matters.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=150,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        summary = response.content[0].text.strip()\n",
    "\n",
    "        summarized_doc = {\n",
    "            \"chunk_link\": doc[\"chunk_link\"],\n",
    "            \"chunk_heading\": doc[\"chunk_heading\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"summary\": summary\n",
    "        }\n",
    "        summarized_docs.append(summarized_doc)\n",
    "\n",
    "    # Save the summarized documents to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(summarized_docs, f, indent=2)\n",
    "\n",
    "    print(f\"Summaries generated and saved to {output_file}\")\n",
    "    \n",
    "# this is already available, so the call is commented out\n",
    "# generate_summaries('data/anthropic_docs.json', 'data/anthropic_summary_indexed_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cfea-b81d-4d06-9c8f-337f9ecc95c9",
   "metadata": {},
   "source": [
    "### Summary-Enhanced Vector Database Creation (heading + summary + chunk)\n",
    "Here, we're creating a new vector database that incorporates our summary-enhanced document chunks. This approach combines the original text, the chunk heading, and the newly generated summary into a single text for embedding.\n",
    "\n",
    "Key features of this process:\n",
    "\n",
    "1. We create embeddings for the combined text (heading + summary + original content) using the Voyage AI API.\n",
    "2. The embeddings and full metadata (including summaries) are stored in our vector database.\n",
    "3. We implement caching mechanisms to improve efficiency in repeated queries.\n",
    "4. The database is saved to disk for persistence and quick loading in future sessions.\n",
    "\n",
    "This summary-enhanced approach aims to create more informative embeddings, potentially leading to more accurate and contextually relevant document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "351e638a-d09a-4295-8de6-6c54ac6e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class SummaryEnhancedVectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/summary_indexed_vector_db.pkl\"\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        \"\"\"not called for now\"\"\"\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size], normalize_embeddings=True)\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "        \n",
    "    def load_data(self, data_file):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Loading vector database from file: {self.db_path}.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "            \n",
    "        # well, if not...\n",
    "        print(f'file {self.db_path} does not exist')\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Embed Chunk Heading + Text + Summary Together\n",
    "        texts = [f\"{item['chunk_heading']}\\n\\n{item['text']}\\n\\n{item['summary']}\" for item in data]\n",
    "        print(f'****Total Chunks: {len(texts)}')\n",
    "        texts_exceeding_max_len = [s for s in texts if len(s) > 384]\n",
    "        print(f'****Chunks greater that {max_word_len}: {len(texts_exceeding_max_len)}')\n",
    "        \n",
    "        # Embed more than 128 documents with a for loop\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size], normalize_embeddings=True)\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data  # Store the entire item as metadata\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=3, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            # print(f'found in cache!')\n",
    "            query_embedding = np.array(self.query_cache[query])  #\n",
    "            # print(f'type:{type(query_embedding)}')\n",
    "        else:\n",
    "            query_embedding = embeddings_model.encode(query, normalize_embeddings=True)\n",
    "            # print(f'query embedding:\\n {query_embedding}')\n",
    "            self.query_cache[query] = query_embedding.tolist()\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        print(f'Saving DB in: {self.db_path}')\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1c306c5-1d54-4ca0-9a14-4947caae1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 3 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=3):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4e912-1419-4078-a063-c83d30b9a9de",
   "metadata": {},
   "source": [
    "### Enhanced Retrieval Using Summary-Enhanced Embeddings\n",
    "In this section, we implement the retrieval process using our new summary-enhanced vector database. This approach leverages the enhanced embeddings we created, which incorporate document summaries along with the original content.\n",
    "\n",
    "Key aspects of this updated retrieval process:\n",
    "\n",
    "1. We search the vector database using the query embedding, retrieving the top k most similar documents.\n",
    "2. For each retrieved document, we include the chunk heading, summary, and full text in the context provided to the LLM.\n",
    "3. This enriched context is then used to generate an answer to the user's query.\n",
    "\n",
    "By including summaries in both the embedding and retrieval phases, we aim to provide the LLM with a more comprehensive and focused context. This could potentially lead to more accurate and relevant answers, as the LLM has access to both a concise overview (the summary) and the detailed information (the full text) for each relevant document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a875906-ca83-4bb2-bdce-d8508e45025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl does not exist\n",
      "****Total Chunks: 232\n",
      "****Chunks greater that 384.0: 208\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 78.44 MiB is free. Process 2080273 has 15.69 GiB memory in use. Of the allocated memory 15.08 GiB is allocated by PyTorch, and 248.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Initialize the SummaryEnhancedVectorDB\u001b[39;00m\n\u001b[1;32m     60\u001b[0m level_two_db \u001b[38;5;241m=\u001b[39m SummaryEnhancedVectorDB(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_docs_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mlevel_two_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/anthropic_summary_indexed_docs.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m level_two_db\u001b[38;5;241m.\u001b[39msave_db()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# # Load the Anthropic documentation\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# with open('data/anthropic_docs.json', 'r') as f:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     anthropic_docs = json.load(f)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 48\u001b[0m, in \u001b[0;36mSummaryEnhancedVectorDB.load_data\u001b[0;34m(self, data_file)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Embed more than 128 documents with a for loop\u001b[39;00m\n\u001b[1;32m     47\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 48\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m     embeddings_model\u001b[38;5;241m.\u001b[39mencode(texts[i : i \u001b[38;5;241m+\u001b[39m batch_size], normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size)\n\u001b[1;32m     51\u001b[0m ]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Flatten the embeddings\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m [embedding \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "Cell \u001b[0;32mIn[48], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Embed more than 128 documents with a for loop\u001b[39;00m\n\u001b[1;32m     47\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     48\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 49\u001b[0m     \u001b[43membeddings_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size)\n\u001b[1;32m     51\u001b[0m ]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Flatten the embeddings\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m [embedding \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:977\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    975\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 977\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:632\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    621\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    622\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    623\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m         output_attentions,\n\u001b[1;32m    630\u001b[0m     )\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:563\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    560\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    561\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 563\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:575\u001b[0m, in \u001b[0;36mXLMRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 575\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:474\u001b[0m, in \u001b[0;36mXLMRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 474\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/vast-jupyter/lib/python3.10/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 78.44 MiB is free. Process 2080273 has 15.69 GiB memory in use. Of the allocated memory 15.08 GiB is allocated by PyTorch, and 248.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "def retrieve_similar_level_two(query, db):\n",
    "    # print(f'_______Query used for retrieval________:\\n {query}')\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        chunk = result['metadata']\n",
    "        # show model all 3 items; heading, text, summary\n",
    "        context += f\"\\n <document> \\n Heading:\\n{chunk['chunk_heading']}\\n\\nText:\\n {chunk['text']} \\n\\nSummary: \\n {chunk['summary']} \\n </document> \\n\" \n",
    "    \n",
    "        # print(f'-----------start retrieval {i} --------------')\n",
    "        # print(f\"__Retrieved results heading__:\\n{result['metadata']['chunk_heading']}\")\n",
    "        # print(f\"__Retrieved results text__:\\n{result['metadata']['text']}\")\n",
    "        # print(f\"__Retrieved results summary__:\\n{result['metadata']['summary']}\")\n",
    "        # print(f'-----------end retrieval {i} ----------------')\n",
    "        \n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context_level_two(query, db):\n",
    "    documents, context = retrieve_similar_level_two(query, db)\n",
    "    # print(f'query + context:\\n{construct_prompt(query, context)}')\n",
    "    completion = client.chat.completions.create(\n",
    "    model=generation_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=model_temperature\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Initialize the SummaryEnhancedVectorDB\n",
    "level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "level_two_db.save_db()\n",
    "\n",
    "# # Load the Anthropic documentation\n",
    "# with open('data/anthropic_docs.json', 'r') as f:\n",
    "#     anthropic_docs = json.load(f)\n",
    "\n",
    "# test\n",
    "#query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\n",
    "query = \"What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\"\n",
    "test_results, test_contexts = retrieve_similar_level_two(query, level_two_db)\n",
    "# for i, test_result in enumerate(test_results):\n",
    "#     print(f'ith:{i}\\n {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daa157-bd0c-4462-be43-f2a7d1f06bc4",
   "metadata": {},
   "source": [
    "### Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b51398e7-2da9-47ca-90f8-e4e565f6108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"For OpenAI models, returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db) # ??\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_tokens = num_tokens_from_string(comparision_prompt, \"o200k_base\")  # note, this encoding name is for gpt-4o, gpt-4o-mini\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful judge.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=model_temperature,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'Number of query tokens: {nb_tokens}, Query:\\n{query}')\n",
    "            print(f'__Correct answer__:\\n{correct_answer}')\n",
    "            print(f'__Generated answer__:\\n{generated_answer}')\n",
    "            print(f'__Response from judge LLM__:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd8852c4-15df-422c-bdb7-cccf25f8da08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  15%|█▌        | 15/100 [00:00<00:02, 32.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.4667, Avg Recall: 0.7500, Avg MRR: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  27%|██▋       | 27/100 [00:00<00:02, 36.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/100 items. Current Avg Precision: 0.3833, Avg Recall: 0.6250, Avg MRR: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  35%|███▌      | 35/100 [00:01<00:01, 37.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.3889, Avg Recall: 0.6222, Avg MRR: 0.7278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  47%|████▋     | 47/100 [00:01<00:01, 37.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 items. Current Avg Precision: 0.4250, Avg Recall: 0.6542, Avg MRR: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  55%|█████▌    | 55/100 [00:01<00:01, 38.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4267, Avg Recall: 0.6733, Avg MRR: 0.7467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  67%|██████▋   | 67/100 [00:01<00:00, 38.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/100 items. Current Avg Precision: 0.4222, Avg Recall: 0.6806, Avg MRR: 0.7722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  75%|███████▌  | 75/100 [00:02<00:00, 38.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.4095, Avg Recall: 0.6512, Avg MRR: 0.7452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  87%|████████▋ | 87/100 [00:02<00:00, 38.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 items. Current Avg Precision: 0.4167, Avg Recall: 0.6635, Avg MRR: 0.7583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  95%|█████████▌| 95/100 [00:02<00:00, 37.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4111, Avg Recall: 0.6546, Avg MRR: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:02<00:00, 36.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/100 items. Current Avg Precision: 0.3967, Avg Recall: 0.6325, Avg MRR: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:08<13:23,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 506, Query:\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\n",
      "__Correct answer__:\n",
      "To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\n",
      "__Generated answer__:\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. **Access the Evaluation Screen**: Open the Anthropic Console and navigate to the prompt editor. After composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\n",
      "\n",
      "2. **Ensure Dynamic Variables**: Make sure your prompt includes at least 1-2 dynamic variables using the double brace syntax (e.g., {{variable}}). This is necessary for creating evaluation test sets.\n",
      "\n",
      "3. **Add Test Cases**: On the Evaluation screen, you will see a single row for your test case. To add more test cases, click the ‘Add Test Case’ button.\n",
      "\n",
      "4. **Fill in Values**: For each new test case, fill in the values for each variable in your prompt. Repeat this process to create multiple scenarios.\n",
      "\n",
      "5. **Re-run Evaluations**: If you update your original prompt text, you can re-run the entire evaluation suite against the new prompt to see how changes affect performance across all test cases.\n",
      "\n",
      "By following these steps, you can effectively create and manage multiple test cases for your evaluation.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides a comprehensive and detailed explanation of how to create multiple test cases in the Anthropic Evaluation tool, including steps that align with the Correct Answer. Both answers mention clicking the 'Add Test Case' button and filling in values for each variable in the prompt. The additional details in the Generated Answer, such as ensuring dynamic variables and the ability to re-run evaluations, do not contradict the Correct Answer and enhance the understanding of the process. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:10<07:51,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 336, Query:\n",
      "What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\n",
      "__Correct answer__:\n",
      "Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\n",
      "__Generated answer__:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models and provides options for customized models tailored to specific industry domains, such as finance and healthcare, as well as bespoke fine-tuned models for individual customers.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. It mentions that Anthropic recommends Voyage AI for embedding models and includes the capabilities of Voyage AI, such as customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:16<08:47,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 627, Query:\n",
      "What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\n",
      "__Correct answer__:\n",
      "When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\n",
      "__Generated answer__:\n",
      "Key success metrics to consider when evaluating Claude's performance on a classification task include:\n",
      "\n",
      "1. **Accuracy**: Measures the proportion of correct predictions made by the model. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive instances and its ability to avoid false positives.\n",
      "\n",
      "3. **Consistency**: Evaluates whether the model produces similar outputs for similar inputs, ensuring logical and predictable behavior.\n",
      "\n",
      "4. **Structure**: Assesses whether the model's output adheres to the expected format, which is crucial for ease of parsing and interpretation.\n",
      "\n",
      "5. **Speed**: Refers to the model's response time, which is critical for applications requiring low latency. This metric directly impacts user experience and system performance.\n",
      "\n",
      "6. **Bias and Fairness**: Important for ensuring that the model does not exhibit biases that could lead to unfair or inaccurate classifications, particularly in sensitive applications involving demographic data.\n",
      "\n",
      "When choosing the right model to reduce latency, the smaller Claude 3 Haiku model is often recommended due to its speed and efficiency, making it suitable for many classification tasks. However, if the task requires specialized knowledge or complex reasoning, a larger model like Sonnet may be necessary, albeit at the cost of increased latency and expense. The choice of model should therefore balance the trade-offs between cost, accuracy, and response time, ensuring that the selected model meets the performance requirements while minimizing latency.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately lists the key success metrics for evaluating Claude's performance on a classification task, including accuracy, F1 score, consistency, structure, speed, and bias and fairness. It also discusses the importance of choosing the right model to reduce latency, mentioning the trade-offs between model size, speed, and performance. However, it introduces specific model names (Claude 3 Haiku and Sonnet) that were not mentioned in the correct answer, which could be seen as an unnecessary detail. Overall, the generated answer captures the essence of the correct answer and does not omit any critical information, so it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:20<07:54,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 426, Query:\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\n",
      "__Correct answer__:\n",
      "Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\n",
      "__Generated answer__:\n",
      "Claude for Sheets can improve prompt engineering workflows compared to using chained prompts in two significant ways:\n",
      "\n",
      "1. **Parallel Testing of Prompts**: Claude for Sheets allows users to test multiple prompts across evaluation suites simultaneously. This capability enables more efficient experimentation and iteration, as users can quickly assess the effectiveness of various prompts without the need to execute them sequentially, which is often the case with chained prompts.\n",
      "\n",
      "2. **Centralized Resource for Examples and Structures**: The Claude for Sheets prompting examples workbench serves as a centralized repository for example prompts and prompt engineering structures. This resource helps users to explore and learn about prompt engineering more effectively, providing a structured approach that can enhance creativity and efficiency compared to the more fragmented approach of managing chained prompts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts: parallel testing of prompts and a centralized resource for examples and structures. While the correct answer emphasizes the speed of parallel testing and mentions specific tasks like survey analysis, the generated answer provides a broader perspective on the benefits of centralized resources. Overall, both answers convey the same essential ideas, so the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:24<06:45,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 348, Query:\n",
      "What happens if a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns?\n",
      "__Correct answer__:\n",
      "If a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error.\n",
      "__Generated answer__:\n",
      "If a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. The API requires these specific markers to properly identify the roles of the participants in the conversation. Without them, the prompt is considered invalid, and the API will not process it.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. Both state that the absence of the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns will result in an API error, and the generated answer adds that these markers are necessary for identifying the roles in the conversation, which is consistent with the original statement. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:31<08:07,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 484, Query:\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?\n",
      "__Correct answer__:\n",
      "Tool use requests in the Claude API are priced the same as regular API requests, based on the total input and output tokens. However, tool use requests have additional tokens beyond the regular input and output, including the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt that enables tool use, which add to the total tokens and cost.\n",
      "__Generated answer__:\n",
      "The additional tokens required for tool use in Claude API requests impact pricing by increasing the total number of input and output tokens counted for each request. Tool use requests are priced the same as regular API requests, based on the total number of tokens, which includes:\n",
      "\n",
      "1. Tokens from the tools parameter (tool names, descriptions, and schemas).\n",
      "2. Tokens from tool_use content blocks in API requests and responses.\n",
      "3. Tokens from tool_result content blocks in API requests.\n",
      "4. A special system prompt for tool use, which adds a specific number of tokens depending on the model being used (ranging from 159 to 530 tokens).\n",
      "\n",
      "These additional tokens are added to the normal input and output tokens to calculate the total cost of a request. Therefore, using tools will generally result in a higher cost compared to regular API requests due to the inclusion of these extra tokens.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key points from the Correct Answer regarding how additional tokens for tool use in Claude API requests impact pricing. It correctly states that tool use requests are priced the same as regular API requests based on total tokens, and it details the various types of additional tokens that contribute to the total cost. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:33<06:30,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 311, Query:\n",
      "When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\n",
      "__Correct answer__:\n",
      "The new Usage, Cost, and Rate Limits tabs in the Anthropic Developer Console that show API usage, billing details, and current rate limits will be available on June 27th, 2024.\n",
      "__Generated answer__:\n",
      "The new Anthropic Developer Console features that show API usage, billing details, and rate limits will be available on June 27th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the new features will be available on June 27th, 2024, which matches the information provided in the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:36<06:11,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 417, Query:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\n",
      "__Correct answer__:\n",
      "When deciding whether to use CoT, consider if the task requires in-depth thinking that a human would need to work through, and be aware that the increased output length from CoT may impact latency.\n",
      "__Generated answer__:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, two key factors to consider are:\n",
      "\n",
      "1. **Task Complexity**: CoT should be used for tasks that require in-depth thinking, such as complex math, multi-step analysis, or writing complex documents. If the task does not necessitate extensive processing, it may be better to avoid CoT to reduce latency.\n",
      "\n",
      "2. **Model Selection**: Choosing the right model is crucial for optimizing both performance and latency. Different models have varying capabilities and performance characteristics, so selecting one that aligns with your specific requirements can help strike the right balance.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the importance of task complexity in deciding whether to use CoT, which aligns with the correct answer's emphasis on in-depth thinking. However, the correct answer specifically mentions the impact of increased output length on latency, which is not addressed in the generated answer. Instead, the generated answer introduces a new factor, model selection, which is not part of the correct answer. Since the generated answer lacks a critical piece of information regarding output length affecting latency, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:42<07:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 586, Query:\n",
      "How can I use Claude to more easily digest the content of long PDF documents?\n",
      "__Correct answer__:\n",
      "You can upload PDFs and have Claude summarize their content, making it easier to understand the key points of long documents without having to read through everything.\n",
      "__Generated answer__:\n",
      "To use Claude for digesting long PDF documents more easily, follow these strategies:\n",
      "\n",
      "1. **Place Longform Data at the Top**: Start your prompt with the content of the PDF document. This should be the first thing Claude sees, as it helps improve performance, especially for long documents (around 20K+ tokens). \n",
      "\n",
      "2. **Structure Content with XML Tags**: If you're working with multiple documents or sections, wrap each document in `<document>` tags and include `<document_content>` and `<source>` tags for clarity. This structured approach helps Claude understand the context better.\n",
      "\n",
      "   Example structure:\n",
      "   ```xml\n",
      "   <documents>\n",
      "     <document index=\"1\">\n",
      "       <source>your_document.pdf</source>\n",
      "       <document_content>\n",
      "         {{YOUR_DOCUMENT_CONTENT}}\n",
      "       </document_content>\n",
      "     </document>\n",
      "   </documents>\n",
      "   ```\n",
      "\n",
      "3. **Ground Responses in Quotes**: Ask Claude to extract and quote relevant parts of the document before performing any tasks. This helps focus on the most pertinent information and reduces noise from irrelevant content.\n",
      "\n",
      "   Example prompt:\n",
      "   ```\n",
      "   Please find quotes from the document that are relevant to my query. Then, summarize the key points based on these quotes.\n",
      "   ```\n",
      "\n",
      "4. **Place Queries at the End**: After providing the long document content and any necessary instructions, place your specific questions or queries at the end of the prompt. This can enhance the quality of Claude's responses by allowing it to process the context first.\n",
      "\n",
      "By implementing these strategies, you can effectively leverage Claude to analyze and summarize long PDF documents, making the information more digestible.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides detailed strategies for using Claude to digest long PDF documents, including uploading content, structuring it with XML tags, grounding responses in quotes, and placing queries at the end. While it elaborates on the process, it essentially conveys the same core idea as the correct answer: that Claude can summarize the content of PDFs to make it easier to understand key points. Therefore, it is correct in its substance, even if it includes additional details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:44<05:39,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 276, Query:\n",
      "According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\n",
      "__Correct answer__:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab of the Developer Console.\n",
      "__Generated answer__:\n",
      "You can view your organization's current API rate limits in the new Rate Limits tab in the Developer Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer conveys the same essential information as the correct answer, stating that the API rate limits can be viewed in the Rate Limits tab of the Developer Console. The addition of \"new\" does not change the substance of the answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/100 questions. Current Accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [00:49<06:07,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 494, Query:\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?\n",
      "__Correct answer__:\n",
      "In addition to accuracy, we can measure the 95th percentile response time and average cost per classification to assess the ticket classification system's performance and production-readiness.\n",
      "__Generated answer__:\n",
      "To measure the performance of the ticket classification system implemented using Claude beyond just accuracy, consider the following evaluation metrics:\n",
      "\n",
      "1. **F1 Score**: This metric balances precision and recall, providing a better measure of a model's performance when dealing with imbalanced classes.\n",
      "\n",
      "2. **Consistency**: Evaluate whether the model's outputs are consistent for similar inputs, ensuring it follows a logical pattern in its predictions.\n",
      "\n",
      "3. **Structure**: Assess if the model's output adheres to the expected format, such as JSON, which makes it easier to parse and interpret.\n",
      "\n",
      "4. **Speed**: Measure the response time of the model to ensure it meets acceptable latency thresholds for the task.\n",
      "\n",
      "5. **Bias and Fairness**: If the classification involves sensitive data about individuals, it's important to evaluate whether the model demonstrates any biases based on characteristics like gender or ethnicity.\n",
      "\n",
      "Additionally, consider adapting the model to handle common scenarios that may lead to misclassification, such as implicit requests, emotional prioritization, intent vs. routing, and issue prioritization. Regularly reviewing and refining prompts based on performance feedback can also enhance the system's effectiveness.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides several relevant metrics for evaluating the performance of the ticket classification system, such as F1 Score, consistency, structure, speed, and bias and fairness. However, it does not mention the specific metrics of 95th percentile response time and average cost per classification, which are highlighted in the correct answer. Since these critical pieces of information are missing, the generated answer cannot be considered correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [00:54<06:20,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 486, Query:\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?\n",
      "__Correct answer__:\n",
      "With the Text Completions API, the system prompt is added as text before the first \"\\n\\nHuman:\" turn. With the Messages API, the system prompt is specified using the separate \"system\" parameter when making the API request.\n",
      "__Generated answer__:\n",
      "To specify a system prompt using the Text Completions API, you add the text before the first `\\n\\nHuman:` turn in your prompt. For example:\n",
      "\n",
      "```python\n",
      "prompt = \"Today is January 1, 2024.\\n\\nHuman: Hello, Claude\\n\\nAssistant:\"\n",
      "```\n",
      "\n",
      "In contrast, when using the Messages API, you specify the system prompt with the `system` parameter in the API call. For example:\n",
      "\n",
      "```python\n",
      "anthropic.Anthropic().messages.create(\n",
      "    model=\"claude-3-opus-20240229\",\n",
      "    max_tokens=1024,\n",
      "    system=\"Today is January 1, 2024.\",  # <-- system prompt\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "In summary, the Text Completions API requires the system prompt to be part of the prompt string, while the Messages API uses a dedicated `system` parameter to set the context.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to specify a system prompt using both the Text Completions API and the Messages API, aligning with the correct answer. It explains that for the Text Completions API, the system prompt is included before the first \"\\n\\nHuman:\" turn, and for the Messages API, it uses a separate \"system\" parameter. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 699\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [01:01<07:28,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 635, Query:\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?\n",
      "__Correct answer__:\n",
      "You can combine XML tags like <thinking> and <answer> with chain of thought reasoning, where Claude explains its step-by-step reasoning process, to create structured, high-performance prompts. For example, you can prompt Claude to show its reasoning by including \"Before answering, explain your reasoning step-by-step in <thinking> tags.\" in the user message or system prompt.\n",
      "__Generated answer__:\n",
      "To combine XML tags with chain of thought reasoning for creating high-performance prompts for Claude, follow these steps:\n",
      "\n",
      "1. **Identify Subtasks**: Break down your main task into distinct, sequential subtasks. Each subtask should have a single, clear objective.\n",
      "\n",
      "2. **Use XML Tags for Structure**: Implement XML tags to clearly separate different parts of your prompt. This enhances clarity and reduces the likelihood of misinterpretation by Claude. For example, you can use tags like `<task>`, `<subtask>`, and `<output>` to organize your prompts effectively.\n",
      "\n",
      "3. **Incorporate Chain of Thought**: Utilize chain of thought reasoning by embedding tags like `<thinking>` and `<answer>` within your prompts. This encourages Claude to articulate its reasoning process step-by-step, leading to more coherent and logical outputs.\n",
      "\n",
      "4. **Nest Tags for Hierarchical Content**: When necessary, nest your XML tags to represent hierarchical relationships. For instance, you can structure a prompt as `<outer><inner></inner></outer>` to indicate a relationship between broader and more specific concepts.\n",
      "\n",
      "5. **Iterate and Refine**: After receiving outputs from Claude, evaluate its performance and refine your prompts accordingly. Adjust the structure, tag usage, or the clarity of your subtasks based on the results.\n",
      "\n",
      "6. **Maintain Consistency**: Use consistent tag names throughout your prompts to avoid confusion. This consistency helps Claude understand the context better and respond more accurately.\n",
      "\n",
      "By combining these techniques, you create super-structured, high-performance prompts that leverage both the clarity of XML tagging and the logical flow of chain of thought reasoning.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive approach to combining XML tags with chain of thought reasoning for creating high-performance prompts for Claude. It includes the use of XML tags for structure, the incorporation of chain of thought reasoning, and emphasizes the importance of clarity and consistency. However, it does not explicitly mention the specific XML tags like <thinking> and <answer> as highlighted in the correct answer. This omission is critical as it fails to directly address the specific tags that should be used to prompt Claude effectively. Therefore, while the generated answer contains relevant information, it lacks a key element from the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [01:05<06:56,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 446, Query:\n",
      "When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\n",
      "__Correct answer__:\n",
      "When evaluating the claude-3-haiku-20240307 model's performance on the 91 test samples, the three key metrics calculated are accuracy (89.01%), 95th percentile response time (1.61 seconds), and average cost per request routing ($0.0004).\n",
      "__Generated answer__:\n",
      "The three key metrics calculated when evaluating the Claude model's performance for ticket routing are:\n",
      "\n",
      "1. **Accuracy**: This measures how often the model's predictions match the correct classifications.\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects the model's performance on both false positives and false negatives.\n",
      "3. **Consistency**: This assesses whether the model's outputs are logical and consistent for similar inputs.\n",
      "\n",
      "The results for the claude-3-haiku-20240307 model on the 91 test samples are not provided in the documents, so specific numerical results for these metrics cannot be stated.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it lists different key metrics (Accuracy, F1 Score, and Consistency) compared to the Correct Answer, which specifies Accuracy, 95th percentile response time, and average cost per request routing. Additionally, the Generated Answer states that specific numerical results for these metrics cannot be provided, which contradicts the Correct Answer that provides specific values for the metrics. Therefore, there are critical pieces of information missing and direct contradictions present.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:08<05:54,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 364, Query:\n",
      "Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\n",
      "__Correct answer__:\n",
      "Before prompt engineering, Anthropic highly recommends having a clear definition of success criteria for your use case, some ways to empirically test against those criteria, and a first draft prompt you want to improve.\n",
      "__Generated answer__:\n",
      "Before starting to engineer and improve a prompt in Claude, Anthropic recommends having the following key things in place:\n",
      "\n",
      "1. A clear definition of the success criteria for your use case.\n",
      "2. Some ways to empirically test against those criteria.\n",
      "3. A first draft prompt that you want to improve.\n",
      "\n",
      "If you do not have a first draft prompt, Anthropic suggests using the prompt generator available in the Anthropic Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key recommendations from Anthropic regarding prompt engineering. It includes all the essential elements mentioned in the correct answer: a clear definition of success criteria, ways to empirically test those criteria, and a first draft prompt to improve. Additionally, it provides extra information about using the prompt generator if a first draft prompt is not available, which does not contradict the original answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:12<05:54,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 465, Query:\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?\n",
      "__Correct answer__:\n",
      "The Messages API allows you to continue a response by making the last input message have the \"assistant\" role, whereas the Text Completions API lets you pre-fill part of Claude's response directly in the prompt string.\n",
      "__Generated answer__:\n",
      "The Messages API handles mid-response prompting by allowing the last input message to have the assistant role, which enables the response to continue from that point. This means you can pre-fill part of the assistant's response directly in the message structure. For example, you can specify an assistant message with a partial response, and the model will continue from there.\n",
      "\n",
      "In contrast, the Text Completions API allows you to pre-fill part of the response by setting the prompt to start with the assistant's response. This is done using raw string prompts that alternate between \"Human:\" and \"Assistant:\".\n",
      "\n",
      "In summary, while both APIs allow for mid-response prompting, the Messages API uses a structured list of messages with roles (user and assistant), whereas the Text Completions API uses a simpler string format. The output format also differs, with the Messages API returning a list of content blocks, while the Text Completions API returns generated text directly.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key points from the Correct Answer regarding how the Messages API and Text Completions API handle mid-response prompting. It correctly states that the Messages API allows for continuing a response by using the assistant role for the last input message, and it explains the structure of both APIs. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:17<06:11,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 538, Query:\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?\n",
      "__Correct answer__:\n",
      "When given the role of CFO through a system prompt, Claude provides a much more insightful, structured, and actionable financial analysis compared to not having a specific role. The role-based response breaks down key financial metrics, provides strategic commentary, and makes specific recommendations.\n",
      "__Generated answer__:\n",
      "Claude's response differs significantly when given a specific role through a system prompt compared to not having a defined role in a financial analysis context. \n",
      "\n",
      "When a role is assigned, such as \"You are a seasoned data scientist at a Fortune 500 company,\" Claude's performance is enhanced in several ways:\n",
      "\n",
      "1. **Enhanced Accuracy**: The role prompting improves Claude's ability to handle complex scenarios, leading to more precise and relevant analyses.\n",
      "\n",
      "2. **Tailored Tone**: The communication style is adjusted to fit the expectations of the assigned role, which can be crucial in financial contexts where brevity and professionalism are often required.\n",
      "\n",
      "3. **Improved Focus**: With a clear role context, Claude is more likely to stay aligned with the specific requirements of the task, ensuring that the analysis is relevant and on point.\n",
      "\n",
      "In contrast, without a specific role, Claude may provide responses that are less accurate, lack the necessary professional tone, and may not focus adequately on the task at hand. This can lead to a more generalized or vague analysis that may not meet the specific needs of the user. \n",
      "\n",
      "Overall, role prompting allows Claude to leverage its capabilities more effectively, resulting in better insights and analyses tailored to the user's requirements.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the essence of the Correct Answer by highlighting the differences in Claude's responses when given a specific role versus not having one. It discusses enhanced accuracy, tailored tone, and improved focus, which align with the insights, structure, and actionable recommendations mentioned in the Correct Answer. However, it does not explicitly mention the breakdown of key financial metrics or the specific recommendations that the Correct Answer emphasizes. While the Generated Answer is largely correct in its assessment, it lacks some critical details about the nature of the analysis provided when a role is assigned. Therefore, it is not fully aligned with the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:25<07:20,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 721, Query:\n",
      "What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\n",
      "__Correct answer__:\n",
      "Quantitative metrics for evaluating a sentiment analysis model include task-specific metrics like F1 score, as well as generic metrics like accuracy, precision, and recall. Specific targets should be based on industry benchmarks, prior experiments, AI research, or expert knowledge, and should represent an improvement over the current baseline.\n",
      "__Generated answer__:\n",
      "Quantitative metrics that can be used to measure the success of a sentiment analysis model include:\n",
      "\n",
      "1. **Accuracy**: The proportion of correct predictions made by the model, calculated as (Number of correct predictions) / (Total number of predictions).\n",
      "\n",
      "2. **F1 Score**: A measure that balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive instances and its ability to avoid false positives.\n",
      "\n",
      "3. **Precision**: The ratio of true positive predictions to the total predicted positives, indicating how many of the predicted positive sentiments were actually correct.\n",
      "\n",
      "4. **Recall**: The ratio of true positive predictions to the total actual positives, showing how many of the actual positive sentiments were correctly identified by the model.\n",
      "\n",
      "5. **Consistency**: Evaluates whether the model's predictions are stable across similar inputs.\n",
      "\n",
      "6. **Speed**: Measures the response time of the model, ensuring it operates within acceptable latency thresholds for the task.\n",
      "\n",
      "7. **Bias and Fairness**: Assesses whether the model's predictions are free from biases related to gender, ethnicity, or other characteristics.\n",
      "\n",
      "To determine specific targets for these metrics, one should consider:\n",
      "\n",
      "- **Industry Benchmarks**: Research existing models and their performance metrics to set realistic targets.\n",
      "- **Prior Experiments**: Analyze historical data from previous model evaluations to establish a baseline for improvement.\n",
      "- **User Needs**: Align targets with the specific requirements of the application, ensuring that the metrics reflect the expectations of end-users.\n",
      "- **Incremental Improvements**: Set achievable goals, such as a specific percentage improvement over the current baseline (e.g., a 5% increase in F1 score).\n",
      "\n",
      "For example, a well-defined target for a sentiment analysis model could be: \"Our sentiment analysis model should achieve an F1 score of at least 0.85 on a held-out test set of 10,000 diverse Twitter posts, which represents a 5% improvement over our current baseline.\" This target is specific, measurable, achievable, and relevant to the task.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several quantitative metrics for evaluating a sentiment analysis model, including accuracy, F1 score, precision, and recall, which align with the correct answer. Additionally, it provides a comprehensive approach to determining specific targets for these metrics, mentioning industry benchmarks, prior experiments, user needs, and incremental improvements. The example target provided in the generated answer is also specific and measurable, which is consistent with the correct answer's guidance. Therefore, the generated answer captures the essential elements of the correct answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  19%|█▉        | 19/100 [01:29<06:42,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 314, Query:\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?\n",
      "__Correct answer__:\n",
      "Combining XML tags with other prompt engineering techniques like multishot prompting (using <examples> tags) or chain of thought (using <thinking> and <answer> tags) to create super-structured, high-performance prompts.\n",
      "__Generated answer__:\n",
      "Combine XML tags with other techniques like multishot prompting (<examples>) or chain of thought (<thinking>, <answer>). This creates super-structured, high-performance prompts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by mentioning the combination of XML tags with multishot prompting and chain of thought techniques. It conveys the same idea of creating super-structured, high-performance prompts without omitting any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [01:38<08:25,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 894, Query:\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?\n",
      "__Correct answer__:\n",
      "You can use an LLM like Claude to grade the outputs of other LLMs by providing it with the output to grade along with a detailed rubric. Instruct the LLM to think through its reasoning and then output a simple 'correct' or 'incorrect' result based on how well the output matches the criteria in the rubric.\n",
      "__Generated answer__:\n",
      "To use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric, follow these steps:\n",
      "\n",
      "1. **Develop a Clear Rubric**: Create a detailed and clear rubric that outlines the criteria for grading. For example, specify that certain key phrases must be included in the response, or define a scoring scale (e.g., 1-5) for qualitative assessments.\n",
      "\n",
      "2. **Construct Grading Prompts**: Use a function to build a prompt for Claude that includes the answer to be graded and the rubric. The prompt should instruct Claude to think through its reasoning before providing a final grade. For example:\n",
      "   ```python\n",
      "   def build_grader_prompt(answer, rubric):\n",
      "       return f\"\"\"Grade this answer based on the rubric:\n",
      "       <rubric>{rubric}</rubric>\n",
      "       <answer>{answer}</answer>\n",
      "       Think through your reasoning in <thinking> tags, then output 'correct' or 'incorrect' in <result> tags.\"\"\"\n",
      "   ```\n",
      "\n",
      "3. **Implement Grading Logic**: Create a function that sends the constructed prompt to Claude and retrieves the grading response. This function should check if the response indicates 'correct' or 'incorrect':\n",
      "   ```python\n",
      "   def grade_completion(output, golden_answer):\n",
      "       grader_response = client.messages.create(\n",
      "           model=\"claude-3-opus-20240229\",\n",
      "           max_tokens=2048,\n",
      "           messages=[{\"role\": \"user\", \"content\": build_grader_prompt(output, golden_answer)}]\n",
      "       ).content[0].text\n",
      "       return \"correct\" if \"correct\" in grader_response.lower() else \"incorrect\"\n",
      "   ```\n",
      "\n",
      "4. **Evaluate Outputs**: Gather the outputs from the LLMs you want to grade and compare them against the expected answers (golden answers). Use the grading function to evaluate each output:\n",
      "   ```python\n",
      "   eval_data = [\n",
      "       {\"question\": \"Is 42 the answer to life, the universe, and everything?\", \"golden_answer\": \"Yes, according to 'The Hitchhiker's Guide to the Galaxy'.\"},\n",
      "       {\"question\": \"What is the capital of France?\", \"golden_answer\": \"The capital of France is Paris.\"}\n",
      "   ]\n",
      "   outputs = [get_completion(q[\"question\"]) for q in eval_data]\n",
      "   grades = [grade_completion(output, a[\"golden_answer\"]) for output, a in zip(outputs, eval_data)]\n",
      "   ```\n",
      "\n",
      "5. **Calculate Scores**: Finally, calculate the overall score based on the grading results to assess the performance of the LLMs being evaluated:\n",
      "   ```python\n",
      "   print(f\"Score: {grades.count('correct') / len(grades) * 100}%\")\n",
      "   ```\n",
      "\n",
      "By following these steps, you can effectively use Claude to automate the grading of outputs from other LLMs based on a predefined rubric.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed and structured approach to using an LLM like Claude for grading outputs from other LLMs based on a rubric. It includes steps for developing a rubric, constructing prompts, implementing grading logic, evaluating outputs, and calculating scores. This aligns well with the correct answer, which emphasizes using Claude to grade outputs based on a rubric and instructing it to think through its reasoning. Both answers convey the same essential information about the process, and the generated answer does not omit any critical pieces of information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 20/100 questions. Current Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [01:42<07:25,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 563, Query:\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?\n",
      "__Correct answer__:\n",
      "To access Voyage embeddings on AWS, subscribe to the model package on AWS Marketplace, select the model to deploy, agree to the terms, and copy the Product ARN for your selected region. Then create a JupyterLab space in SageMaker Studio, upload Voyage's notebook, and follow the instructions to deploy the model package using the ARN.\n",
      "__Generated answer__:\n",
      "To access and deploy Voyage embeddings on AWS Marketplace, follow these steps:\n",
      "\n",
      "1. **Subscribe to the Model Package**:\n",
      "   - Navigate to the model package listing page on AWS Marketplace and select the Voyage model you wish to deploy.\n",
      "   - Click on the \"Continue to subscribe\" button.\n",
      "   - Review the details on the \"Subscribe to this software\" page. If you agree with the End-User License Agreement (EULA), pricing, and support terms, click on \"Accept Offer\".\n",
      "   - After selecting \"Continue to configuration\" and choosing a region, you will receive a Product ARN (Amazon Resource Name). This ARN is required for creating a deployable model using Boto3.\n",
      "\n",
      "2. **Deploy the Model Package**:\n",
      "   - Again, navigate to the model package listing page and select the model to deploy.\n",
      "   - Click on \"Continue to subscribe\" and accept the terms as before.\n",
      "   - Copy the Product ARN corresponding to your selected region for use in your deployment.\n",
      "\n",
      "3. **Create a JupyterLab Space**:\n",
      "   - Create a JupyterLab space in SageMaker Studio.\n",
      "   - Upload Voyage’s notebook to your JupyterLab space.\n",
      "   - Follow the instructions provided within the notebook to utilize the Voyage embeddings.\n",
      "\n",
      "By following these steps, you can successfully access and deploy Voyage embeddings on AWS Marketplace.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides a detailed and accurate description of the steps required to access and deploy Voyage embeddings on AWS Marketplace. It includes subscribing to the model package, deploying the model, and creating a JupyterLab space, which aligns with the Correct Answer. Although there are minor differences in wording and structure, the essential information and steps are consistent between both answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [01:48<07:21,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 619, Query:\n",
      "When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\n",
      "__Correct answer__:\n",
      "When using tools to get JSON output, you should provide a single tool, set the tool_choice to explicitly instruct the model to use that tool, and ensure the tool name and description are from the model's perspective since it will pass the input to the tool.\n",
      "__Generated answer__:\n",
      "To effectively set up tools for Claude to produce JSON output following a specific schema, you should focus on the following key aspects:\n",
      "\n",
      "1. **Define Tools Clearly**: When integrating tools, ensure that you provide clear definitions for each tool, including names, descriptions, and input schemas. This helps Claude understand how to use the tools effectively.\n",
      "\n",
      "2. **Craft an Appropriate User Prompt**: Include a user prompt that is relevant to the tools you have defined. The prompt should indicate the need for JSON output, guiding Claude to utilize the tools you’ve set up.\n",
      "\n",
      "3. **Tool Use Decision**: Claude will assess whether the provided tools can assist with the query. If applicable, it will construct a properly formatted tool use request, which will be indicated by a stop_reason of tool_use in the API response.\n",
      "\n",
      "4. **Extract and Execute Tool Input**: After Claude constructs the tool use request, you need to extract the tool name and input from Claude’s request. Execute the tool code client-side to generate the necessary output.\n",
      "\n",
      "5. **Return Results to Claude**: Continue the conversation by sending a new user message that includes the tool result. This allows Claude to analyze the results and formulate a final response based on the JSON output.\n",
      "\n",
      "6. **Ensure Flexibility**: Remember that all tools must be explicitly provided by you, as Claude does not have access to built-in server-side tools. This gives you control over the tools available for generating the desired JSON output.\n",
      "\n",
      "By following these steps, you can effectively set up tools and prompt Claude to produce JSON output that adheres to your specified schema.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of the steps needed to set up tools for Claude to produce JSON output. It emphasizes defining tools clearly, crafting an appropriate user prompt, and ensuring that all tools are explicitly provided. However, it does not explicitly mention the importance of providing a single tool and setting the tool_choice to instruct the model to use that tool, which is a critical piece of information from the correct answer. Therefore, while the generated answer contains relevant information, it is missing a key aspect that is essential for the task. Thus, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [01:54<07:36,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 605, Query:\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?\n",
      "__Correct answer__:\n",
      "The Claude 3 Haiku model has vision capabilities, is faster, more performant, and more intelligent than the legacy Claude Instant 1.2 model. Claude 3 Haiku also has more up-to-date training data.\n",
      "__Generated answer__:\n",
      "The key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance are as follows:\n",
      "\n",
      "1. **Performance**: Claude Instant 1.2 is described as a fast and efficient model but is considered less performant compared to the Claude 3 models. The Claude 3 family, including Claude 3 Haiku, offers improved capabilities and overall intelligence.\n",
      "\n",
      "2. **Multilingual Capabilities**: While Claude Instant 1.2 has some multilingual capabilities, it has less coverage, understanding, and skill compared to the Claude 3 models. The Claude 3 models are expected to have enhanced multilingual support.\n",
      "\n",
      "3. **Vision Capabilities**: Claude Instant 1.2 lacks vision capabilities, which are present in the Claude 3 family. This means that Claude 3 models can process and understand visual information, a feature not available in the legacy model.\n",
      "\n",
      "4. **Latency**: Claude Instant 1.2 is slower than the Claude 3 models of similar intelligence. The newer models are designed to be faster and more efficient.\n",
      "\n",
      "5. **Context Window and Output**: Both Claude Instant 1.2 and Claude 3 models have a context window of 100K tokens and a maximum output of 4096 tokens, but the Claude 3 models are likely to handle these features more effectively due to their advanced architecture.\n",
      "\n",
      "Overall, the Claude 3 Haiku model represents a significant upgrade over the legacy Claude Instant 1.2 in terms of performance, capabilities, and features.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several key differences between the Claude Instant 1.2 model and the Claude 3 Haiku model, including performance, multilingual capabilities, vision capabilities, and latency. It also mentions that both models have a context window of 100K tokens and a maximum output of 4096 tokens, which aligns with the correct answer's assertion of Claude 3 being more performant and intelligent. However, the generated answer does not explicitly mention that Claude 3 Haiku has more up-to-date training data, which is a critical piece of information from the correct answer. Therefore, while the generated answer is largely accurate, it is missing this important detail, leading to a conclusion that it is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [01:56<06:01,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 277, Query:\n",
      "What is one key benefit of using examples when prompt engineering with Claude?\n",
      "__Correct answer__:\n",
      "One key benefit of using examples in prompts is that they reduce misinterpretation of instructions, leading to more accurate outputs from Claude.\n",
      "__Generated answer__:\n",
      "One key benefit of using examples when prompt engineering with Claude is that examples reduce misinterpretation of instructions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the correct answer by stating that examples reduce misinterpretation of instructions. However, it omits the part about leading to more accurate outputs from Claude, which is a critical piece of information. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [01:59<05:13,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 368, Query:\n",
      "According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\n",
      "__Correct answer__:\n",
      "Prompt engineering allows you to easily adapt AI models to new domains by providing domain-specific context directly in the prompts, without needing to retrain the model through fine-tuning.\n",
      "__Generated answer__:\n",
      "One key advantage of using prompt engineering instead of fine-tuning when adapting an AI model to new domains or tasks is that prompt engineering is far faster and more resource-efficient. It allows for nearly instantaneous results and requires only text input, while fine-tuning necessitates high-end GPUs, large memory, and substantial task-specific labeled data, making prompt engineering a more cost-effective and flexible option for rapid experimentation and domain adaptation.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by highlighting the speed and resource efficiency of prompt engineering compared to fine-tuning. It emphasizes that prompt engineering allows for rapid experimentation and domain adaptation without the need for extensive computational resources or labeled data, which aligns with the key advantage mentioned in the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [02:02<04:32,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 335, Query:\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?\n",
      "__Correct answer__:\n",
      "You can make a copy of Anthropic's provided Claude for Sheets workbook template to quickly get started using the extension with your own work.\n",
      "__Generated answer__:\n",
      "To quickly get started using the Claude for Sheets extension with a pre-made template, you should make a copy of the Claude for Sheets workbook template provided by Anthropic. This template serves as a starting point for integrating the Claude AI model into your spreadsheet applications and workflows. Once you have copied the template, you can begin customizing it for your specific needs and exploring its features.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the correct answer by stating that you should make a copy of the Claude for Sheets workbook template provided by Anthropic to get started. It also adds context about the purpose of the template and customization, which does not detract from the correctness of the main point. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [02:06<04:37,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 447, Query:\n",
      "How does the \"index\" field in the \"content_block_delta\" event relate to the text being streamed in a response?\n",
      "__Correct answer__:\n",
      "The \"index\" field in each \"content_block_delta\" event indicates which content block the text delta applies to. Multiple deltas with the same index consecutively stream the text for a single content block in the response.\n",
      "__Generated answer__:\n",
      "The \"index\" field in the \"content_block_delta\" event indicates the specific position of the content block that is being updated. Each content_block_delta event contains a delta that modifies the content at this given index. The delta can represent various types of changes, such as inserting, deleting, or replacing text within the content block.\n",
      "\n",
      "In the context of streaming text in a response, the index helps to identify which part of the overall content is being modified with each delta event. For example, if multiple content_block_delta events are received, each with the same index, it suggests that the same content block is being updated incrementally, allowing for a more granular and dynamic update of the text being streamed. This enables the client to reconstruct the final text by applying all the deltas in the order they are received.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the role of the \"index\" field in the \"content_block_delta\" event, stating that it indicates which content block is being updated and that multiple deltas with the same index suggest incremental updates to the same content block. This aligns with the correct answer's explanation that the index relates to the content block being streamed in the response. Both answers convey the essential information about how the index functions in the context of streaming text. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [02:09<04:16,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 402, Query:\n",
      "How can you include an image as part of a Claude API request, and what image formats are currently supported?\n",
      "__Correct answer__:\n",
      "To include an image in a Claude API request, provide it as a base64-encoded image in an \"image\" content block within the \"messages\" array. The currently supported image formats are JPEG, PNG, GIF, and WebP.\n",
      "__Generated answer__:\n",
      "To include an image as part of a Claude API request, you need to upload the image directly in the request. The API supports the following image formats: JPEG, PNG, GIF, and WebP. Specifically, the supported MIME types are:\n",
      "\n",
      "- image/jpeg\n",
      "- image/png\n",
      "- image/gif\n",
      "- image/webp\n",
      "\n",
      "Keep in mind that Claude cannot read image URLs, so you must upload the image file itself rather than providing a link. Additionally, there are limits on image size: the maximum file size for images uploaded via the API is 5MB.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a different method for including an image in a Claude API request, stating that the image must be uploaded directly rather than being base64-encoded in a content block. This contradicts the correct answer, which specifies that the image should be base64-encoded within an \"image\" content block in the \"messages\" array. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [02:13<04:24,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 470, Query:\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?\n",
      "__Correct answer__:\n",
      "TTFT is a specific measure of latency that captures the time it takes for a language model to generate the first token of its response after receiving a prompt. It is an important component of a model's overall latency and responsiveness, especially for interactive applications.\n",
      "__Generated answer__:\n",
      "The relationship between Time to First Token (TTFT) and latency when evaluating a language model's performance is that both metrics measure aspects of the model's responsiveness. TTFT specifically measures the time it takes for the model to generate the first token of its output after receiving a prompt, while latency refers to the overall time taken for the model to respond to the prompt.\n",
      "\n",
      "A lower TTFT indicates that the model can start generating a response more quickly, which contributes to lower latency. Therefore, TTFT can be seen as a component of latency, particularly important in interactive applications where quick initial feedback is essential. Both metrics are influenced by similar factors, such as model size, hardware capabilities, network conditions, and the complexity of the prompt. In summary, while TTFT focuses on the initial output generation, latency encompasses the entire response time, and both are critical for assessing a model's performance in real-time scenarios.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the relationship between Time to First Token (TTFT) and latency, emphasizing that TTFT is a specific measure of latency that focuses on the time taken to generate the first token. It also correctly notes that TTFT contributes to overall latency and is particularly important in interactive applications. Additionally, it mentions factors that influence both metrics, which aligns with the correct answer's emphasis on TTFT being an important component of latency. Therefore, the generated answer captures the essential points of the correct answer without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [02:18<04:56,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 646, Query:\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?\n",
      "__Correct answer__:\n",
      "Providing edge case examples to Claude in the prompt can meaningfully improve its performance in correctly routing support tickets in scenarios where it may otherwise misclassify them, such as implicit requests, emotional prioritization, ambiguous intent vs. routing, or issue prioritization.\n",
      "__Generated answer__:\n",
      "Providing Claude with examples of handling edge cases like implicit requests and emotional prioritization can significantly enhance its performance in routing support tickets. \n",
      "\n",
      "1. **Implicit Requests**: Customers often communicate their needs indirectly. For instance, a statement like “I’ve been waiting for my package for over two weeks now” implies a request for order status. By training Claude with examples of such implicit requests, it can learn to recognize and appropriately classify these indirect inquiries, ensuring that tickets are routed to the right team without requiring explicit requests.\n",
      "\n",
      "2. **Emotional Prioritization**: Customers may express dissatisfaction or frustration, which can be crucial for prioritizing their tickets. If Claude is equipped with guidelines on when to prioritize emotional sentiment over the technical aspects of a problem, it can better address urgent customer concerns. This means that tickets from dissatisfied customers could be escalated or routed more quickly to ensure timely resolution.\n",
      "\n",
      "3. **Intent vs. Routing**: There are instances where Claude might correctly identify a customer's intent but misroute the ticket. By providing examples that clarify the appropriate routing for various intents, Claude can improve its accuracy in directing tickets to the correct departments, reducing misclassifications.\n",
      "\n",
      "4. **Issue Prioritization**: When customers present multiple issues in a single interaction, it can be challenging for Claude to determine which issue to prioritize. By clarifying how to identify and prioritize the primary concern among multiple issues, Claude can enhance its ability to route tickets effectively, ensuring that the most pressing matters are addressed first.\n",
      "\n",
      "In summary, incorporating these edge case examples into Claude's training can lead to more accurate classifications and improved routing of support tickets, ultimately enhancing customer satisfaction and operational efficiency. Regularly reviewing and refining these examples will further optimize Claude's performance as the system evolves.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how providing examples of edge cases like implicit requests and emotional prioritization can enhance Claude's performance in routing support tickets. It elaborates on several relevant aspects, such as intent vs. routing and issue prioritization, which align with the points made in the correct answer. Therefore, it conveys the same fundamental information without omitting any critical details. </explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 30/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [02:23<05:01,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 590, Query:\n",
      "How does the stop_reason of \"tool_use\" relate to the overall workflow of integrating external tools with Claude?\n",
      "__Correct answer__:\n",
      "When Claude determines that one of the user-provided tools can help answer the user's query, it constructs a tool use request. This causes the API response to have a stop_reason of \"tool_use\", signaling Claude's intent to use the tool. The user must then extract the tool input from Claude's request, run the actual tool code client-side, and continue the conversation by sending the tool results back to Claude.\n",
      "__Generated answer__:\n",
      "The stop_reason of \"tool_use\" is a critical part of the workflow for integrating external tools with Claude. It indicates that Claude has determined that a specific tool is necessary to address the user's query effectively. When Claude assesses a user prompt and identifies that a tool can assist, it constructs a tool use request, which is reflected in the API response with the stop_reason set to \"tool_use.\" This signals the intent to utilize the specified tool.\n",
      "\n",
      "The workflow involves several steps:\n",
      "\n",
      "1. **Providing Tools and User Prompt**: The user defines the tools available for Claude and provides a prompt that may require those tools.\n",
      "2. **Decision to Use a Tool**: Claude evaluates the prompt and decides if a tool is needed. If so, it generates a tool use request, which includes the stop_reason of \"tool_use.\"\n",
      "3. **Extracting Input and Running Code**: The user extracts the tool name and input from the tool use request and executes the tool's code on their end.\n",
      "4. **Formulating a Response**: After obtaining the results from the tool, Claude uses this information to formulate a final response to the original user prompt.\n",
      "\n",
      "The \"tool_use\" stop_reason thus serves as a key indicator of Claude's intent to engage with external tools, facilitating a structured interaction that enhances the overall functionality and responsiveness of the system.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the workflow involving the stop_reason of \"tool_use\" and aligns closely with the correct answer. It includes the essential steps of providing tools, the decision-making process by Claude, extracting input, running the tool, and formulating a response. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [02:25<04:14,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 324, Query:\n",
      "According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\n",
      "__Correct answer__:\n",
      "During periods of high usage, an overloaded_error event may be sent in the event stream, which would normally correspond to an HTTP 529 error code in a non-streaming context.\n",
      "__Generated answer__:\n",
      "During periods of high usage for the Anthropic API when using streaming responses, you may receive an \"overloaded_error.\" This error typically corresponds to an HTTP 529 error in a non-streaming context.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. Both answers mention the \"overloaded_error\" event and its correspondence to the HTTP 529 error code during periods of high usage for the Anthropic API when using streaming responses. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [02:27<03:37,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 302, Query:\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?\n",
      "__Correct answer__:\n",
      "The two types of deltas that can be contained in a content_block_delta event are text_delta and input_json_delta.\n",
      "__Generated answer__:\n",
      "The two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API are \"input_json_delta\" and \"text_delta\".\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two types of deltas as \"input_json_delta\" and \"text_delta\", which matches the correct answer. The wording is slightly different, but the substance is the same, so the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [02:30<03:24,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 326, Query:\n",
      "On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\n",
      "__Correct answer__:\n",
      "Claude 3.5 Sonnet became generally available across those platforms on June 20th, 2024, while tool use became generally available on May 30th, 2024.\n",
      "__Generated answer__:\n",
      "Claude 3.5 Sonnet and tool use both became generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI on June 20th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that both Claude 3.5 Sonnet and tool use became generally available on June 20th, 2024. However, the correct answer specifies that Claude 3.5 Sonnet became available on June 20th, 2024, while tool use became available earlier on May 30th, 2024. This critical piece of information is missing in the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [02:32<03:05,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 308, Query:\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?\n",
      "__Correct answer__:\n",
      "Anthropic launched Claude.ai and the Claude iOS app in Europe in May 2024, and then launched them in Canada the following month in June 2024.\n",
      "__Generated answer__:\n",
      "Claude.ai and the Claude iOS app were launched in Europe on May 13th, 2024, followed by their launch in Canada on June 5th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states the order of the launches, indicating that Claude.ai and the Claude iOS app were launched in Europe in May 2024, followed by their launch in Canada in June 2024. Although it provides specific dates, the essential information about the sequence of launches is accurate and aligns with the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [02:37<03:43,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 536, Query:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", what does this indicate and what should be done next to continue the conversation?\n",
      "__Correct answer__:\n",
      "A stop_reason of \"tool_use\" signals that Claude has decided to use a tool and has constructed a formatted tool use request. To continue the conversation, the tool name and input should be extracted from Claude's request, the actual tool code should be executed client-side, and then a new user message containing a tool_result content block should be sent to Claude.\n",
      "__Generated answer__:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", it indicates that Claude has determined that it needs to use an external tool to assist with the user's query. This response will include one or more tool_use content blocks that specify the tool being used, along with the input parameters for that tool.\n",
      "\n",
      "To continue the conversation after receiving this response, you should:\n",
      "\n",
      "1. Extract the name, id, and input from the tool_use block.\n",
      "2. Execute the corresponding tool in your codebase using the extracted input.\n",
      "3. After obtaining the results from the tool, send a new message back to Claude with the role of user, including a content block of type tool_result. This block should contain:\n",
      "   - tool_use_id: The id of the tool use request.\n",
      "   - content: The result of the tool, formatted as a string or a list of nested content blocks.\n",
      "   - Optionally, include is_error set to true if the tool execution resulted in an error.\n",
      "\n",
      "This will allow Claude to use the tool results to formulate a response to the original user prompt.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by explaining that a stop_reason of \"tool_use\" indicates that Claude needs to use an external tool and outlines the steps to continue the conversation. It includes the extraction of the tool name, id, and input, the execution of the tool, and the sending of a new message back to Claude with the appropriate content block. Although there are minor differences in wording and structure, the critical information is present and correctly conveyed. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [02:39<03:13,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 303, Query:\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?\n",
      "__Correct answer__:\n",
      "The example code snippet for evaluating tone and style in a customer service chatbot uses the anthropic Python library to interact with the Claude AI model.\n",
      "__Generated answer__:\n",
      "The Python library used in the example code snippet for evaluating tone and style in a customer service chatbot is `anthropic`. This library allows developers to interact with the Claude AI model programmatically.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the `anthropic` library as the one used in the example code snippet for evaluating tone and style in a customer service chatbot, which aligns with the correct answer. Both answers convey the same essential information about the library's purpose in interacting with the Claude AI model. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [02:42<03:02,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 341, Query:\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?\n",
      "__Correct answer__:\n",
      "The two main ways to authenticate are: 1) Directly providing the aws_access_key, aws_secret_key, and optionally aws_session_token, or 2) Using the default AWS credential providers, such as the ~/.aws/credentials file or the AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID environment variables.\n",
      "__Generated answer__:\n",
      "The two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock are:\n",
      "\n",
      "1. Using an API key.\n",
      "2. Creating an Anthropic Console account.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the two main ways to authenticate as specified in the correct answer. Instead, it refers to using an API key and creating an Anthropic Console account, which are not mentioned in the correct answer. The correct answer specifically details the use of AWS access keys and the default AWS credential providers, which are critical pieces of information that are missing in the generated answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [02:46<03:23,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 423, Query:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\n",
      "__Correct answer__:\n",
      "When deciding to use leak-resistant prompt engineering, the potential reduction in prompt leaks should be balanced against the risk of degraded model performance due to the added complexity of the prompt.\n",
      "__Generated answer__:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, two key factors to consider and balance are:\n",
      "\n",
      "1. **Complexity of the Prompt**: Implementing leak-resistant strategies can add complexity to the prompt, which may degrade the model's overall performance and the quality of its outputs. It's important to assess how much complexity is being introduced and whether it could negatively impact the task at hand.\n",
      "\n",
      "2. **Risk of Prompt Leak**: The necessity to prevent prompt leaks must be weighed against the potential performance degradation. While it's crucial to safeguard sensitive information, the strategies employed should not compromise the effectiveness of the model in delivering accurate and relevant outputs.\n",
      "\n",
      "Balancing these factors ensures that while you are protecting against leaks, you are also maintaining the model's performance and output quality.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by identifying the two key factors: the complexity of the prompt and the risk of prompt leak. It discusses how the added complexity can degrade model performance and emphasizes the need to balance this with the necessity to prevent prompt leaks. Therefore, the generated answer is correct as it conveys the same fundamental ideas without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [02:51<03:52,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 474, Query:\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?\n",
      "__Correct answer__:\n",
      "Choosing the right Claude model that best fits your needs in terms of speed and output quality is one of the most straightforward ways to reduce latency in your application. Anthropic offers a range of Claude models with different capabilities and performance characteristics to allow you to choose the optimal balance of intelligence, speed, and cost for your use case.\n",
      "__Generated answer__:\n",
      "Selecting the appropriate Claude model based on your specific requirements can significantly reduce latency in your application by ensuring that you choose a model that aligns with your needs for speed and output quality. Anthropic offers a variety of models with different capabilities and performance characteristics. By selecting a model that is optimized for your use case, you can achieve faster response times and better overall performance.\n",
      "\n",
      "For instance, if your application requires quick responses and can compromise slightly on output quality, choosing a faster model may be beneficial. Conversely, if your application demands high-quality outputs and can tolerate some latency, a more advanced model might be appropriate. Understanding the trade-offs between intelligence, speed, and cost allows you to make an informed decision that minimizes latency while still meeting your application's requirements. \n",
      "\n",
      "Additionally, leveraging the models overview page can provide detailed information on model metrics, further guiding your selection process to optimize latency effectively.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how selecting the appropriate Claude model can reduce latency based on the trade-offs between speed and output quality. It discusses the importance of choosing a model that aligns with specific application needs and mentions the variety of models available from Anthropic. Additionally, it includes the suggestion to leverage the models overview page for detailed information, which enhances the guidance for selection. Since the generated answer aligns well with the substance of the correct answer and does not omit any critical information, it should be marked as correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 40/100 questions. Current Accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [02:55<03:46,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 475, Query:\n",
      "How can you stream responses from the Anthropic API using the Python SDK?\n",
      "__Correct answer__:\n",
      "You can stream responses from the Anthropic API using the Python SDK by using the client.messages.stream() method and iterating over the stream.text_stream attribute in a for loop.\n",
      "__Generated answer__:\n",
      "To stream responses from the Anthropic API using the Python SDK, you can utilize the `messages.stream` method provided by the SDK. Here’s an example of how to implement this:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "client = anthropic.Anthropic()\n",
      "\n",
      "with client.messages.stream(\n",
      "    max_tokens=1024,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      ") as stream:\n",
      "    for text in stream.text_stream:\n",
      "        print(text, end=\"\", flush=True)\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "- You first import the `anthropic` library and create an instance of the `Anthropic` client.\n",
      "- The `messages.stream` method is called with parameters such as `max_tokens`, the `messages` list containing the user input, and the model name.\n",
      "- The response is streamed, and you can iterate over `stream.text_stream` to print each piece of text as it is received, ensuring real-time feedback.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to stream responses from the Anthropic API using the Python SDK by mentioning the `messages.stream` method and iterating over `stream.text_stream`. It also provides a relevant code example that aligns with the correct answer's substance. Therefore, it captures the essential information without missing any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [02:58<03:28,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 360, Query:\n",
      "How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\n",
      "__Correct answer__:\n",
      "You can shape Claude's response by pre-filling part of it in the last position of the input messages list. To get a short response like a single multiple choice answer, you can set the \"max_tokens\" parameter to a small value like 1.\n",
      "__Generated answer__:\n",
      "To guide Claude's response by pre-filling part of the response, you can include the desired initial text in the Assistant message. Claude will then continue from that point. The API parameter used to generate a short response in this case is `max_tokens`, which controls the maximum length of the response generated.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can guide Claude's response by pre-filling part of the response and mentions the use of the `max_tokens` parameter to control the response length. However, it does not explicitly mention that the pre-filled text should be placed in the last position of the input messages list, which is a critical detail from the correct answer. Therefore, the generated answer is missing a key piece of information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [03:02<03:30,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 400, Query:\n",
      "What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\n",
      "__Correct answer__:\n",
      "When building an eval set, it is better to prioritize having a larger volume of test cases with slightly lower signal automated grading over having fewer questions with high-quality human hand-grading.\n",
      "__Generated answer__:\n",
      "Having a larger number of test cases with automated grading is more important when building an eval set for an AI system. The eval design principles emphasize the importance of automating grading when possible and prioritizing volume over quality. This approach allows for a broader assessment of the AI's performance across various scenarios, including edge cases, which can be critical for understanding how the system behaves in real-world applications. While high-quality human-graded test cases can provide valuable insights, the principle suggests that a greater quantity of automated evaluations can yield more comprehensive data on the system's capabilities.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the correct answer's emphasis on the importance of having a larger number of test cases with automated grading over fewer high-quality human-graded cases. Both answers highlight the value of volume in assessing the AI's performance and the potential benefits of automated grading. Therefore, the generated answer is correct as it conveys the same essential message without critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [03:05<03:11,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 304, Query:\n",
      "What are the two required fields in a content_block_delta event for a text delta type?\n",
      "__Correct answer__:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\", where the \"delta\" field contains a \"type\" of \"text_delta\" and the \"text\" being added.\n",
      "__Generated answer__:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"type\" and \"text\".\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it states that the required fields are \"type\" and \"text\", while the Correct Answer specifies that the required fields are \"index\" and \"delta\". The \"delta\" field must contain a \"type\" of \"text_delta\" and the \"text\" being added, which is not mentioned in the Generated Answer. Therefore, there is a critical piece of information missing.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [03:09<03:10,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 432, Query:\n",
      "What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\n",
      "__Correct answer__:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks demonstrating how to upload PDFs, generate embeddings, and more. The Developer Console offers a prompt generator tool for easier, more powerful prompting.\n",
      "__Generated answer__:\n",
      "Two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings, are:\n",
      "\n",
      "1. **Prompt Engineering Interactive Tutorial**: This tutorial utilizes Claude for Sheets and provides an interactive way to learn prompt engineering techniques. Users will need an API key to access the tutorial, which allows them to experiment with Claude's capabilities in a hands-on manner.\n",
      "\n",
      "2. **Using the Console Workbench**: For users working with Claude 3 models, the Console Workbench allows for interactive experimentation. Users can select a model that accepts images and utilize features like uploading images directly or dragging and dropping files into the chat window to see how Claude processes visual inputs and generates text or code from them. \n",
      "\n",
      "These methods provide practical, interactive experiences to understand and utilize Claude's functionalities effectively.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two interactive ways to learn about Claude's capabilities, but it does not mention the Anthropic Cookbook, which is a critical piece of information from the correct answer. Instead, it focuses on a Prompt Engineering Interactive Tutorial and the Console Workbench, which are not part of the correct answer. Therefore, the generated answer is missing essential information and is not aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [03:15<03:47,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 573, Query:\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?\n",
      "__Correct answer__:\n",
      "Breaking a task into distinct subtasks for chained prompts improves Claude's accuracy because each subtask gets Claude's full attention, reducing errors compared to tackling the entire complex task at once.\n",
      "__Generated answer__:\n",
      "Breaking a task into distinct subtasks for chained prompts helps improve Claude's accuracy on the overall task for several reasons:\n",
      "\n",
      "1. **Focused Attention**: Each subtask allows Claude to concentrate fully on a specific aspect of the task, which reduces the likelihood of errors. By isolating subtasks, the model can process information more effectively without being overwhelmed by the complexity of the entire task.\n",
      "\n",
      "2. **Clarity of Instructions**: Simpler subtasks lead to clearer instructions and outputs. When tasks are broken down, the expectations for each step become more straightforward, which helps Claude generate more accurate responses.\n",
      "\n",
      "3. **Traceability and Debugging**: Chaining prompts enhances traceability, making it easier to identify and fix issues within the prompt chain. If a mistake occurs, it is simpler to pinpoint which subtask caused the error, allowing for targeted adjustments.\n",
      "\n",
      "4. **Structured Thinking**: Allowing Claude to think through problems in a structured manner improves coherence and organization in its responses. This structured approach is particularly beneficial for complex tasks, as it helps maintain logical flow and reduces inconsistencies.\n",
      "\n",
      "5. **Mitigation of Errors**: By breaking down complex tasks into smaller, manageable subtasks, the chances of inconsistency errors and hallucinations are reduced. This structured method ensures that each part of the task is addressed thoroughly, leading to more reliable outputs.\n",
      "\n",
      "Overall, chaining prompts into distinct subtasks enhances the model's ability to deliver accurate and coherent results while facilitating easier debugging and error correction.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how breaking tasks into subtasks allows Claude to focus better, clarifies instructions, aids in debugging, promotes structured thinking, and mitigates errors. It emphasizes the benefits of focused attention and reduced complexity, which aligns with the correct answer's main point about improving accuracy. Therefore, the generated answer is correct as it conveys the same fundamental ideas without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [03:20<03:56,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 530, Query:\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?\n",
      "__Correct answer__:\n",
      "Messages streaming responses can contain multiple content blocks of varying types, making the streaming format more complex compared to Text Completions which only include completion, ping, and error server-sent-events.\n",
      "__Generated answer__:\n",
      "The streaming format for Messages responses differs from Text Completions streaming responses in several key ways:\n",
      "\n",
      "1. **Complexity of Content**: The Messages streaming format is more complex because it can contain multiple content blocks of varying types, while the Text Completions streaming format primarily includes completion, ping, and error server-sent-events.\n",
      "\n",
      "2. **Event Structure**: In the raw HTTP stream response for Messages, the structure includes a series of events such as `message_start`, `content_block_start`, `content_block_delta`, `content_block_stop`, `message_delta`, and `message_stop`. There may also be ping events. In contrast, the Text Completions streaming format focuses on simpler events related to the completion process.\n",
      "\n",
      "3. **Input Specification**: For Text Completions, inputs are provided as raw strings, while for Messages, inputs are specified as a list of messages, each with a role (user or assistant) and content.\n",
      "\n",
      "4. **Output Format**: The output for Text Completions is returned in the completion values of the response, whereas for Messages, the output is a list of content blocks, which may include various types of content.\n",
      "\n",
      "These differences highlight the more intricate nature of the Messages streaming format compared to the simpler Text Completions format.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by highlighting the complexity of the Messages streaming format compared to Text Completions. It discusses the multiple content blocks, the event structure, input specification, and output format, which are all relevant points. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [03:22<03:24,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 330, Query:\n",
      "What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\n",
      "__Correct answer__:\n",
      "According to the documentation, users can start experimenting with Claude by visiting claude.ai or using Anthropic's web Console.\n",
      "__Generated answer__:\n",
      "Two ways to start experimenting with Claude as a user, according to Anthropic's documentation, are:\n",
      "\n",
      "1. Check out the quickstart guide for step-by-step instructions on how to create an account, obtain API keys, and start interacting with Claude's models.\n",
      "2. Use the web Console at claude.ai to start experimenting with Claude right away.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes a method (using the web Console at claude.ai) that aligns with the correct answer. However, it introduces an additional method (checking out the quickstart guide) that is not mentioned in the correct answer. The correct answer specifically states two ways: visiting claude.ai and using the web Console, but the generated answer does not explicitly mention visiting claude.ai as a standalone method. Therefore, it lacks a critical piece of information and is not fully aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [03:26<03:17,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 443, Query:\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?\n",
      "__Correct answer__:\n",
      "Chain prompts break complex tasks into smaller subtasks, allowing Claude to give its full attention to each one. This reduces errors and inconsistencies that may occur when trying to handle a complex workflow all at once.\n",
      "__Generated answer__:\n",
      "Using chain prompts can significantly reduce errors and inconsistency in complex tasks handled by Claude by breaking down these tasks into smaller, manageable subtasks. This approach allows Claude to focus on each subtask individually, which enhances accuracy and minimizes the likelihood of errors. \n",
      "\n",
      "Additionally, chain prompts improve clarity by simplifying instructions, making it easier for Claude to understand and execute the tasks. The transparency of the prompt chain also aids in traceability, allowing users to easily identify and resolve any issues that arise.\n",
      "\n",
      "Moreover, employing techniques such as specifying the desired output format, providing examples, and ensuring contextual consistency through retrieval further enhances the effectiveness of chain prompts. These methods help mitigate hallucinations and inconsistencies, leading to more reliable and coherent outputs from Claude. Overall, the structured approach of chain prompts fosters a more organized workflow, ultimately improving the quality of the results.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer regarding how chain prompts help reduce errors and inconsistencies in complex tasks handled by Claude. It emphasizes breaking down tasks into smaller subtasks, allowing for focused attention on each, which enhances accuracy and minimizes errors. Additionally, it introduces further benefits of chain prompts, such as improving clarity and traceability, which align with the overall theme of reducing errors. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [03:28<02:41,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 286, Query:\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?\n",
      "__Correct answer__:\n",
      "In a non-streaming context, an overloaded_error event would normally correspond to an HTTP 529 status code.\n",
      "__Generated answer__:\n",
      "An overloaded_error event corresponds to an HTTP status code of 529 in a non-streaming context for the Anthropic API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately states that an overloaded_error event corresponds to an HTTP status code of 529 in a non-streaming context for the Anthropic API, which is consistent with the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 50/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [03:31<02:36,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 347, Query:\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?\n",
      "__Correct answer__:\n",
      "When making a request to Voyage AI's embedding endpoint, you can either leave the encoding_format parameter unspecified to get the embeddings as lists of floating-point numbers, or set encoding_format to \"base64\" to get the embeddings compressed to Base64 encodings.\n",
      "__Generated answer__:\n",
      "The two ways to specify the format in which Voyage AI returns embeddings through its HTTP API are:\n",
      "\n",
      "1. By default, the embeddings are represented as lists of floating-point numbers.\n",
      "2. By setting the `encoding_format` parameter to \"base64\", the embeddings are compressed to Base64 encodings.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two ways to specify the format for embeddings: the default representation as lists of floating-point numbers and the option to set the `encoding_format` parameter to \"base64\". It captures the essence of the correct answer without omitting any critical information or introducing contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [03:36<02:59,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 553, Query:\n",
      "When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\n",
      "__Correct answer__:\n",
      "When streaming requests with tool use, the input JSON deltas for tool_use content blocks are sent as partial JSON strings in multiple content_block_delta events. The client can accumulate these partial JSON strings and parse the complete JSON object once a content_block_stop event is received, using a library like Pydantic for partial JSON parsing or helpers provided in Anthropic's SDKs.\n",
      "__Generated answer__:\n",
      "When streaming API requests that utilize tools, the input JSON deltas for tool_use content blocks are sent as partial JSON strings. Each delta corresponds to updates for the input field of the tool_use block, allowing for maximum granularity. The final tool_use.input is always an object.\n",
      "\n",
      "Clients can accumulate these string deltas and parse the JSON once they receive a `content_block_stop` event. This can be achieved using libraries like Pydantic for partial JSON parsing or by utilizing the provided SDKs, which offer helpers to access parsed incremental values.\n",
      "\n",
      "For example, a tool_use content block delta might look like this:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"event\": \"content_block_delta\",\n",
      "  \"data\": {\n",
      "    \"type\": \"content_block_delta\",\n",
      "    \"index\": 1,\n",
      "    \"delta\": {\n",
      "      \"type\": \"input_json_delta\",\n",
      "      \"partial_json\": \"{\\\"location\\\": \\\"San Fra\\\"}\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "As the streaming continues, multiple such events may be emitted, each providing a portion of the input JSON. Once the complete input is accumulated, it can be parsed into a valid JSON object.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how the input JSON deltas for tool_use content blocks are sent as partial JSON strings in multiple content_block_delta events, and it mentions the accumulation and parsing of these strings upon receiving a content_block_stop event. It also correctly suggests using libraries like Pydantic or SDK helpers for parsing. The example provided further illustrates the process, aligning well with the correct answer. Therefore, the generated answer is correct as it conveys the same essential information without omitting critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [03:40<02:56,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 411, Query:\n",
      "What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\n",
      "__Correct answer__:\n",
      "Anthropic offers a GitHub prompting tutorial that covers prompt engineering concepts in-depth with examples, and a lighter-weight Google Sheets prompting tutorial that utilizes Claude for Sheets.\n",
      "__Generated answer__:\n",
      "The two interactive prompt engineering tutorials offered by Anthropic are:\n",
      "\n",
      "1. **GitHub Prompting Tutorial**: This tutorial is example-filled and covers the prompt engineering concepts found in Anthropic's documentation. It is designed for users who prefer a more comprehensive and structured approach to learning about prompt engineering.\n",
      "\n",
      "2. **Google Sheets Prompting Tutorial**: This is a lighter-weight version of the prompt engineering tutorial that utilizes an interactive spreadsheet format. It allows users to engage with the material in a more hands-on manner, making it accessible for those who prefer a simpler interface.\n",
      "\n",
      "The main difference between the two is that the GitHub tutorial is more detailed and structured, while the Google Sheets tutorial offers a more interactive and simplified experience.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the two interactive prompt engineering tutorials offered by Anthropic, including the GitHub Prompting Tutorial and the Google Sheets Prompting Tutorial. It captures the essence of both tutorials, noting the GitHub tutorial's comprehensive nature and the Google Sheets tutorial's interactive and simplified approach. There are no critical pieces of information missing, and there are no contradictions between the correct answer and the generated answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [03:46<03:29,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 691, Query:\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?\n",
      "__Correct answer__:\n",
      "Claude offers a 200K token context window, tool use for integration into specialized applications, multimodal input capabilities for richer context, and is uniquely positioned to serve high-trust industries processing large volumes of sensitive data with enterprise-grade security and data handling.\n",
      "__Generated answer__:\n",
      "Claude is suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data due to several key capabilities:\n",
      "\n",
      "1. **Enterprise-Grade Security**: Claude offers SOC II Type 2 certification and HIPAA compliance options, ensuring robust security and data handling suitable for high-trust industries that manage sensitive user data.\n",
      "\n",
      "2. **Trustworthiness**: The model is designed to be resistant to jailbreaks and misuse, with continuous monitoring of prompts and outputs to prevent harmful or malicious use cases. It also provides copyright indemnity protections for paid commercial services.\n",
      "\n",
      "3. **Scalability**: Claude is built to handle extensive enterprise needs, with a 200K token context window that supports complex use cases, and future plans for a 1M token context window.\n",
      "\n",
      "4. **Integration Capabilities**: The tool use, or function calling, allows seamless integration of Claude into specialized applications and custom workflows, making it adaptable for various enterprise environments.\n",
      "\n",
      "5. **Multimodal Input**: Claude supports multimodal input capabilities, enabling users to upload images along with text prompts, which enhances the richness of context and allows for more complex use cases.\n",
      "\n",
      "6. **Developer Tools**: It includes a Developer Console with a Workbench and prompt generation tool, facilitating easier and more powerful experimentation and prompting for developers.\n",
      "\n",
      "7. **Low Hallucination Rates**: Claude has very low hallucination rates, ensuring high accuracy, especially over long documents, which is crucial for processing sensitive data reliably.\n",
      "\n",
      "8. **Global Utility**: The model is proficient in multiple languages, including English, Spanish, and Japanese, making it suitable for global enterprise applications.\n",
      "\n",
      "9. **Cost-Performance Balance**: Claude's family of models allows enterprises to choose the right balance of intelligence, speed, and cost, catering to diverse operational needs.\n",
      "\n",
      "These capabilities collectively make Claude a strong candidate for enterprises looking to integrate AI into their workflows while ensuring security, reliability, and adaptability to specialized applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several key capabilities of Claude that make it suitable for enterprise use cases, including enterprise-grade security, integration capabilities, a large token context window, and multimodal input. It also mentions low hallucination rates and developer tools, which are relevant to enterprise needs. While the generated answer includes additional details not present in the correct answer, it does not omit any critical information and aligns well with the substance of the correct answer. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [03:48<02:47,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 292, Query:\n",
      "As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\n",
      "__Correct answer__:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in the United States, Canada, and Europe.\n",
      "__Generated answer__:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in Canada and Europe.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is missing the critical information that the Claude.ai API and iOS app are also available in the United States, which is included in the correct answer. Therefore, the generated answer is not correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [03:52<02:52,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 501, Query:\n",
      "What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\n",
      "__Correct answer__:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are push-based using webhooks, and pull-based. The push-based approach is more web-scalable but requires exposing a public endpoint which has IT security implications. The pull-based approach is easier to implement but makes unnecessary calls to the support ticket system.\n",
      "__Generated answer__:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are the push-based approach and the pull-based approach.\n",
      "\n",
      "1. **Push-Based Approach**: In this method, the support ticket system (like Zendesk) triggers the classification process by sending a webhook event to a routing service when a new ticket is created. This approach is more scalable because it allows for real-time processing of tickets as they are created, minimizing delays. However, it requires exposing a public endpoint, which may raise IT security concerns.\n",
      "\n",
      "2. **Pull-Based Approach**: This method involves the code periodically checking for new tickets at scheduled intervals. While this approach is easier to implement since it does not require setting up a public endpoint, it can lead to unnecessary calls to the support ticket system, making it less efficient and potentially less scalable.\n",
      "\n",
      "In summary, the push-based approach is more scalable but has security implications, while the pull-based approach is easier to implement but may result in inefficiencies.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the two main approaches for integrating Claude into a support ticket workflow, including the push-based and pull-based methods. It correctly identifies the scalability and implementation differences between the two approaches, mentioning the security implications of the push-based approach and the inefficiencies of the pull-based approach. Therefore, the generated answer is correct as it conveys the same essential information as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [03:54<02:17,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 293, Query:\n",
      "When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\n",
      "__Correct answer__:\n",
      "On May 10th, 2024, Anthropic released a prompt generator tool that is available through the Developer Console.\n",
      "__Generated answer__:\n",
      "Anthropic released the prompt generator tool on May 10th, 2024. It is available through the Developer Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating both the release date of the prompt generator tool and its availability through the Developer Console. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [03:56<02:06,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 332, Query:\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?\n",
      "__Correct answer__:\n",
      "The Claude 3 Sonnet model balances intelligence and speed, making it well-suited for high-throughput tasks like sales forecasting and targeted marketing.\n",
      "__Generated answer__:\n",
      "The Claude 3 model that provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing is the claude-3-haiku-20240307 model. It is noted for delivering excellent results while being the fastest and most cost-effective option in the Claude 3 family.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly identifies the model as \"claude-3-haiku-20240307\" instead of the correct \"Claude 3 Sonnet\" model. This is a critical piece of information that is missing, as it does not provide the correct model name that balances intelligence and speed for high-throughput tasks. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [04:00<02:13,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 407, Query:\n",
      "How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\n",
      "__Correct answer__:\n",
      "You can calculate the similarity between two Voyage embedding vectors using the dot product, which is equivalent to cosine similarity since Voyage embeddings are normalized to length 1.\n",
      "__Generated answer__:\n",
      "To calculate the similarity between two Voyage embedding vectors, you can use cosine similarity, which is a common method for this purpose. Since Voyage embeddings are normalized to length 1, cosine similarity is equivalent to the dot product of the two vectors. You can compute this in Python using the following code snippet:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "similarity = np.dot(embd1, embd2)\n",
      "```\n",
      "\n",
      "This will give you the cosine similarity between the two embedding vectors, which ranges from -1 to 1, where 1 indicates that the vectors are identical in direction, 0 indicates orthogonality, and -1 indicates opposite directions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can calculate the similarity between two Voyage embedding vectors using cosine similarity, which is equivalent to the dot product since the embeddings are normalized to length 1. It also provides a Python code snippet for calculating the dot product, which is relevant and helpful. Therefore, the generated answer aligns well with the correct answer and includes all necessary information without any contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [04:05<02:34,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 592, Query:\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?\n",
      "__Correct answer__:\n",
      "Well-chosen examples in prompts can boost Claude's ability to handle complex tasks by reducing misinterpretation of instructions, enforcing consistent structure and style, and serving as a guide for the desired output.\n",
      "__Generated answer__:\n",
      "Using examples in prompts can significantly enhance Claude's performance on complex tasks in several ways:\n",
      "\n",
      "1. **Reduction of Misinterpretation**: Examples clarify the instructions, helping to minimize misunderstandings. This leads to more accurate outputs as Claude has a clearer understanding of what is expected.\n",
      "\n",
      "2. **Enforcement of Consistency**: By providing examples, you establish a uniform structure and style for Claude to follow. This consistency is crucial for maintaining coherence in responses, especially in complex scenarios.\n",
      "\n",
      "3. **Boosting Performance**: Well-chosen examples can enhance Claude's ability to tackle intricate tasks. They provide context and guidance that can lead to improved accuracy and effectiveness in the model's responses.\n",
      "\n",
      "4. **Structured Thinking**: Allowing Claude to think through problems, especially when examples are included, can lead to more organized and cohesive outputs. This structured approach helps in reducing errors, particularly in tasks involving math, logic, or analysis.\n",
      "\n",
      "5. **Debugging and Clarity**: When examples are included, it becomes easier to identify where prompts may be unclear. Observing Claude's thought process in relation to the examples can help pinpoint issues and refine prompts for better performance.\n",
      "\n",
      "6. **Targeted Examples for Improvement**: Iterating on prompts by incorporating targeted examples, possibly through methods like vector databases for similarity searches, can significantly enhance performance. This approach has been shown to improve accuracy dramatically, as evidenced by a case study that increased accuracy from 71% to 93%.\n",
      "\n",
      "In summary, using examples in prompts not only clarifies instructions and ensures consistency but also enhances Claude's overall ability to handle complex tasks effectively.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by discussing how examples in prompts can reduce misinterpretation, enforce consistency, and guide Claude's performance on complex tasks. It expands on these points with additional details about structured thinking, debugging, and targeted examples for improvement, which are relevant and enhance the understanding of the topic. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 60/100 questions. Current Accuracy: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [04:09<02:32,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 467, Query:\n",
      "What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\n",
      "__Correct answer__:\n",
      "When streaming responses with tool use, the two types of content block deltas are text deltas and input JSON deltas. Text deltas contain a \"text\" field with a string of the incrementally generated text. Input JSON deltas contain a \"partial_json\" field with a string containing part of the JSON object specifying the tool's input.\n",
      "__Generated answer__:\n",
      "The two types of content block deltas that can be emitted when streaming responses with tool use are:\n",
      "\n",
      "1. **Input JSON Delta**: This delta type corresponds to updates for the input field of a tool_use content block. It contains partial JSON strings that represent incremental updates to the input. The final input is always an object, and clients can accumulate these string deltas and parse the JSON once they receive a content_block_stop event.\n",
      "\n",
      "2. **Text Delta**: This delta type represents changes to a text content block. It includes the updated text and indicates the type of change (text delta). The structure contains information about the specific text that has been modified.\n",
      "\n",
      "Each delta type is designed to update the content block at a given index in a streaming manner.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies the two types of content block deltas: Input JSON Delta and Text Delta. It explains that the Input JSON Delta contains partial JSON strings representing incremental updates to the input, which aligns with the correct answer's description of the \"partial_json\" field. The Text Delta is also described as representing changes to a text content block, which corresponds to the \"text\" field in the correct answer. Although the wording and structure differ slightly, the essential information is present in both answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: not well-formed (invalid token): line 3, column 178\n",
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [04:13<02:25,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 357, Query:\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?\n",
      "__Correct answer__:\n",
      "Claude's question answering and text analysis capabilities enable it to build intelligent, interactive systems like chatbots and personalize user experiences by understanding sentiment and preferences.\n",
      "__Generated answer__:\n",
      "Two key capabilities of Claude that enable it to build interactive systems and personalized user experiences are:\n",
      "\n",
      "1. **Question Answering**: This capability allows Claude to create intelligent, interactive systems such as customer support chatbots and educational AI tutors, facilitating engaging and responsive user interactions.\n",
      "\n",
      "2. **Text Analysis & Recommendations**: This feature enables Claude to understand sentiment, preferences, and patterns, which helps in personalizing user experiences and offerings based on individual user needs and behaviors.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies \"Question Answering\" as a key capability of Claude, which aligns with the correct answer. It also mentions \"Text Analysis & Recommendations,\" which encompasses the idea of understanding sentiment and preferences, although it adds the aspect of recommendations. Overall, both answers convey the same essential capabilities of Claude in building interactive systems and personalizing user experiences, without any critical omissions or contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [04:17<02:22,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 483, Query:\n",
      "What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\n",
      "__Correct answer__:\n",
      "A raw HTTP stream response includes a message_start event, followed by one or more content blocks (each with a content_block_start, content_block_delta events, and content_block_stop), a message_delta event, and a final message_stop event. Ping events may also be dispersed throughout.\n",
      "__Generated answer__:\n",
      "The key event types included in a raw HTTP stream response when using message streaming, in typical order, are:\n",
      "\n",
      "1. **message_start**: This event indicates the beginning of a message and contains a Message object with empty content.\n",
      "2. **content_block_start**: This event marks the start of a content block.\n",
      "3. **content_block_delta**: There can be one or more of these events, which indicate changes or updates to the content block.\n",
      "4. **content_block_stop**: This event signifies the end of a content block.\n",
      "5. **message_delta**: This event indicates top-level changes to the final Message object.\n",
      "6. **message_stop**: This event marks the end of the message.\n",
      "\n",
      "Additionally, there may be **ping events** and **error events** (such as overloaded_error) that can occur throughout the response, but they are not part of the main event flow.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the key event types included in a raw HTTP stream response when using message streaming and presents them in the correct order. It includes all the essential events mentioned in the Correct Answer, such as message_start, content_block_start, content_block_delta, content_block_stop, message_delta, and message_stop. Additionally, it correctly notes the presence of ping events. However, it introduces the concept of error events, which are not mentioned in the Correct Answer. This addition does not contradict the original information but rather expands on it. Therefore, the Generated Answer is correct in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [04:20<02:05,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 314, Query:\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?\n",
      "__Correct answer__:\n",
      "The Messages API allows including up to 20 images per request, while the claude.ai interface has a lower limit of up to 5 images per turn.\n",
      "__Generated answer__:\n",
      "The maximum number of images that can be included in a single request using the Anthropic API is 20, while the claude.ai interface allows for up to 5 images per turn.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating that the Anthropic API allows for up to 20 images per request and the claude.ai interface allows for up to 5 images per turn. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [04:24<02:06,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 424, Query:\n",
      "When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\n",
      "__Correct answer__:\n",
      "If Claude's response hits the max_tokens limit and has an incomplete tool use block, you should retry the request with a higher max_tokens value to get Claude's full response including the complete tool use.\n",
      "__Generated answer__:\n",
      "When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, you should extract the name, id, and input from the tool_use block. Then, run the actual tool in your codebase corresponding to that tool name, passing in the tool input. After obtaining the results from the tool, continue the conversation by sending a new message with the role of user, including a content block that contains the tool_result type and the relevant information, such as the tool_use_id and the content of the tool result. This process ensures you retrieve the full tool use and allow Claude to continue generating a response based on the tool's output.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed process for handling an incomplete tool use block when Claude's response is cut off due to the max_tokens limit. It suggests extracting the necessary information from the tool_use block and running the tool in the codebase, which is a valid approach. However, it does not mention the option of simply retrying the request with a higher max_tokens value, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing an important aspect of the solution.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [04:26<01:48,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 320, Query:\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?\n",
      "__Correct answer__:\n",
      "Before running a classification evaluation on Claude, you need to 1) develop your test cases, and 2) take a look at Anthropic's guide to developing test cases.\n",
      "__Generated answer__:\n",
      "Before running a classification evaluation on Claude, you need to:\n",
      "\n",
      "1. Develop your test cases.\n",
      "2. Ensure you have a clear understanding of when to use Claude for classification tasks, considering factors like the nature of your classes and the type of input data.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the first step of developing test cases but fails to mention the second step, which is to take a look at Anthropic's guide to developing test cases. This omission is critical, as both steps are necessary before running a classification evaluation on Claude. Therefore, the generated answer is incomplete and should be marked incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [04:29<01:50,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 456, Query:\n",
      "How can you use the content parameter in the messages list to influence Claude's response?\n",
      "__Correct answer__:\n",
      "You can provide content in the last position of the messages list, with the \"assistant\" role, to pre-fill part of Claude's response. This allows you to shape the assistant's output.\n",
      "__Generated answer__:\n",
      "You can use the content parameter in the messages list to influence Claude's response by pre-filling part of the assistant's reply. By including a message with the role set to \"assistant,\" you can provide a starting point for Claude's response. For example, if you want Claude to continue from a specific phrase or sentence, you can structure the messages like this:\n",
      "\n",
      "```python\n",
      "messages = [\n",
      "    {\"role\": \"human\", \"content\": \"What is your favorite color?\"},\n",
      "    {\"role\": \"assistant\", \"content\": \"As an AI assistant, I don't have a favorite color, but if I had to pick, it would be green because\"}\n",
      "]\n",
      "```\n",
      "\n",
      "In this case, Claude will continue the response from the end of the assistant's message, allowing you to guide the conversation in a specific direction. This technique is useful for setting the tone or context of the response you want Claude to generate.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to use the content parameter in the messages list to influence Claude's response. It mentions pre-filling part of the assistant's reply and provides an example that illustrates the concept effectively. Both the correct answer and the generated answer convey the same essential information about shaping Claude's output. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [04:33<01:46,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 417, Query:\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?\n",
      "__Correct answer__:\n",
      "Compared to fine-tuning, prompt engineering is far more effective at helping models understand and utilize external content like retrieved documents. Prompt engineering also preserves the model's broad general knowledge, while fine-tuning risks catastrophic forgetting where the model loses its general capabilities.\n",
      "__Generated answer__:\n",
      "Two key advantages of prompt engineering over fine-tuning regarding model comprehension and general knowledge preservation are:\n",
      "\n",
      "1. **Comprehension Improvements**: Prompt engineering is more effective at helping models understand and utilize external content, such as retrieved documents. This allows for better performance in tasks that require comprehension of specific information without altering the underlying model.\n",
      "\n",
      "2. **Preservation of General Knowledge**: Fine-tuning poses a risk of catastrophic forgetting, where the model may lose its broad general knowledge due to the focus on specific tasks. In contrast, prompt engineering maintains the model's broad capabilities, ensuring that general knowledge is preserved while still adapting to specific needs.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key advantages of prompt engineering over fine-tuning as stated in the correct answer. It discusses both the effectiveness of prompt engineering in helping models understand external content and the preservation of general knowledge, while also mentioning the risk of catastrophic forgetting associated with fine-tuning. Since the substance of the generated answer aligns with the correct answer, it is deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [04:36<01:41,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 422, Query:\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?\n",
      "__Correct answer__:\n",
      "To get started making requests to Claude models on Anthropic's Bedrock API, you need to: 1) Install and configure the AWS CLI, and 2) Install an SDK for accessing Bedrock, such as the Python SDK shown in the example code.\n",
      "__Generated answer__:\n",
      "The two main steps to get started with making requests to Claude models on Anthropic's Bedrock API are:\n",
      "\n",
      "1. **Set Up Your Environment**: You need to create an Anthropic Console account and obtain an API key. Additionally, ensure you have Python 3.7+ or TypeScript 4.5+ installed, as Anthropic provides SDKs for these languages.\n",
      "\n",
      "2. **Call the API**: Use the API by passing the proper parameters to the `/messages/create` endpoint. You can set up the API client and specify the model, temperature, max tokens, and provide the system prompt and user input in your code.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the installation and configuration of the AWS CLI, which is a critical step outlined in the correct answer. While it provides additional information about setting up an Anthropic Console account and using the API, it fails to include the essential step of installing the AWS CLI, making it incomplete.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 70/100 [04:39<01:33,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 394, Query:\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?\n",
      "__Correct answer__:\n",
      "You can list the available Claude models in a specific AWS region by running the command `aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`, replacing `<region>` with the desired AWS region such as `us-west-2`.\n",
      "__Generated answer__:\n",
      "To check which Claude models are available in a specific AWS region using the AWS CLI, you can use the following command:\n",
      "\n",
      "```\n",
      "aws bedrock list-foundation-models --region=<your-region> --by-provider=anthropic --query \"modelSummaries[*].modelId\"\n",
      "```\n",
      "\n",
      "Replace `<your-region>` with the desired AWS region, such as `us-west-2`. This command will list all the available Claude models provided by Anthropic in that region.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers provide the same AWS CLI command to list available Claude models in a specific AWS region, with only minor differences in wording. The instruction to replace the placeholder with the desired AWS region is also present in both answers. Therefore, there are no critical pieces of information missing, and there are no contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 70/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [04:41<01:24,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 338, Query:\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?\n",
      "__Correct answer__:\n",
      "The input_type argument can be passed with a value of \"query\" or \"document\" to specify the type of input text being embedded.\n",
      "__Generated answer__:\n",
      "The argument that can be passed to the `voyageai.Client.embed()` method or the Voyage HTTP API to specify whether the input text is a query or a document is `input_type`. You can set `input_type` to either `\"query\"` or `\"document\"` to indicate the type of the input text.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the argument `input_type` and its possible values of `\"query\"` and `\"document\"`, which matches the information provided in the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [04:47<01:43,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 571, Query:\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?\n",
      "__Correct answer__:\n",
      "Tool_use content block deltas contain partial JSON strings for the input field, whereas text content block deltas directly contain the text delta. Tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time.\n",
      "__Generated answer__:\n",
      "The streaming API delta formats for tool_use content blocks and text content blocks differ primarily in the structure and type of data they handle.\n",
      "\n",
      "1. **Text Content Blocks**:\n",
      "   - The delta format for text content blocks is represented as a `text_delta`. \n",
      "   - The JSON structure includes a `type` field indicating \"text_delta\" and a `text` field containing the updated text. \n",
      "   - Example:\n",
      "     ```json\n",
      "     {\n",
      "       \"type\": \"content_block_delta\",\n",
      "       \"index\": 0,\n",
      "       \"delta\": {\n",
      "         \"type\": \"text_delta\",\n",
      "         \"text\": \"ello frien\"\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "\n",
      "2. **Tool_Use Content Blocks**:\n",
      "   - The delta format for tool_use content blocks involves `input_json_delta`, which represents updates to the input field of the block.\n",
      "   - The deltas are partial JSON strings, allowing for more granular updates. The final output is always an object.\n",
      "   - Example:\n",
      "     ```json\n",
      "     {\n",
      "       \"type\": \"content_block_delta\",\n",
      "       \"index\": 1,\n",
      "       \"delta\": {\n",
      "         \"type\": \"input_json_delta\",\n",
      "         \"partial_json\": \"{\\\"location\\\": \\\"San Fra\\\"}\"\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "\n",
      "In summary, text content blocks use a straightforward text delta format, while tool_use content blocks utilize partial JSON strings to allow for incremental updates to structured data.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences between the delta formats for tool_use content blocks and text content blocks, including the structure of the data and examples. It mentions that text content blocks use a straightforward text delta format and that tool_use content blocks involve partial JSON strings for updates. However, it does not mention the potential delays between streaming events for tool_use deltas, which is a critical piece of information from the correct answer. Therefore, while the generated answer is mostly correct, it is missing a key detail, leading to an overall evaluation of incorrectness.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [04:50<01:36,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 330, Query:\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?\n",
      "__Correct answer__:\n",
      "When uploading images to Claude, the API has a maximum file size limit of 5MB per image, while on claude.ai the limit is 10MB per image.\n",
      "__Generated answer__:\n",
      "The image file size limits for uploading images to Claude are as follows:\n",
      "\n",
      "- **API**: Maximum 5MB per image\n",
      "- **claude.ai**: Maximum 10MB per image\n",
      "\n",
      "Images larger than these limits will be rejected and return an error when using the respective interface.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the image file size limits for both the API and claude.ai, stating that the API has a maximum of 5MB per image and claude.ai has a maximum of 10MB per image. It also includes the information that images larger than these limits will be rejected and return an error, which is relevant and adds clarity. Therefore, the generated answer is correct and aligns with the correct answer provided.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [04:53<01:26,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 335, Query:\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?\n",
      "__Correct answer__:\n",
      "When selecting a Claude model for an enterprise use case that requires low latency, it's important to choose the model that best balances speed and output quality based on the specific requirements of the use case.\n",
      "__Generated answer__:\n",
      "One key consideration when selecting a Claude model for an enterprise use case that needs low latency is to choose a model that optimally balances speed with the required intelligence for the specific tasks. Additionally, implementing strategies such as optimizing network connections, caching responses, and using asynchronous API calls can further help in reducing latency.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the need to balance speed with the required intelligence for the specific tasks, which aligns with the correct answer's emphasis on balancing speed and output quality. Additionally, the mention of strategies to reduce latency, such as optimizing network connections and using asynchronous API calls, adds valuable context without contradicting the original answer. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [04:56<01:19,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 365, Query:\n",
      "What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\n",
      "__Correct answer__:\n",
      "For code retrieval, Voyage AI recommends using the voyage-code-2 embedding model, which they claim performs 17% better than alternatives and achieves state-of-the-art results on general-purpose corpora as well.\n",
      "__Generated answer__:\n",
      "Anthropic does not offer its own embedding model but recommends using Voyage AI for embedding needs. Voyage AI provides several models, with the \"voyage-code-2\" model specifically optimized for code retrieval. This model is reported to be 17% better than alternatives in terms of performance for code retrieval tasks. Additionally, it is state-of-the-art on general-purpose corpora, making it a strong choice for various applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the recommended embedding model (\"voyage-code-2\") for code retrieval and states that it performs 17% better than alternatives, as well as achieving state-of-the-art results on general-purpose corpora. It accurately reflects the information provided in the correct answer without omitting any critical details or introducing contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [04:59<01:19,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 397, Query:\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?\n",
      "__Correct answer__:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks that demonstrate how to upload PDFs and work with embeddings to help developers learn to use Anthropic's APIs.\n",
      "__Generated answer__:\n",
      "The Anthropic Cookbook can help developers learn to use Anthropic's APIs in two ways:\n",
      "\n",
      "1. **Interactive Workbench**: Developers can use the Workbench available in the Anthropic web Console to try out the API directly in the browser. This allows them to experiment with different API calls and see immediate results, which aids in understanding how the API functions.\n",
      "\n",
      "2. **Code Examples**: The Cookbook provides code examples that demonstrate how to call the API, including setting up the API client, specifying parameters like model, temperature, and max tokens, and handling user input. These examples serve as practical guides for developers to implement their own applications using the API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs: an interactive workbench for experimenting with API calls and code examples for implementing the API. However, it does not mention the specific feature of interactive Jupyter notebooks or the aspect of working with embeddings and uploading PDFs, which are highlighted in the Correct Answer. Therefore, it is missing critical information and does not fully align with the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [05:03<01:21,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 473, Query:\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?\n",
      "__Correct answer__:\n",
      "The size of the context window determines how much retrieved information can be passed to the language model to augment its knowledge when generating a response using RAG. A larger context window allows more relevant retrieved information to be utilized by the model, improving the accuracy and groundedness of the generated text.\n",
      "__Generated answer__:\n",
      "The size of the context window significantly impacts a language model's ability to utilize retrieval augmented generation (RAG). A larger context window allows the model to incorporate more retrieved information from the external knowledge base along with the original query, enabling it to generate more accurate and contextually relevant responses. This is particularly important in RAG, where the model needs to reference both the input prompt and the retrieved data to produce coherent and informed outputs.\n",
      "\n",
      "Conversely, a smaller context window may limit the amount of information the model can effectively use, potentially leading to less relevant or coherent responses. It restricts the model's ability to maintain context over longer interactions or complex queries, which can diminish the effectiveness of RAG, especially in tasks that require detailed or nuanced understanding. Thus, the context window size plays a crucial role in how well a language model can leverage retrieval mechanisms to enhance its generative capabilities.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer regarding the impact of context window size on retrieval augmented generation (RAG). It discusses how a larger context window allows for more retrieved information to be incorporated, leading to improved accuracy and relevance in responses. Additionally, it correctly notes the limitations of a smaller context window. Overall, the generated answer conveys the same essential information as the correct answer without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [05:09<01:35,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 536, Query:\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?\n",
      "__Correct answer__:\n",
      "The Evaluation tool helps identify edge cases where prompts might falter, allows rating individual results to determine prompt performance, ensures consistent performance across inputs, and enables prompt refinement for better reliability. Reviewing results across test cases helps spot patterns to make informed adjustments that lead to more robust AI applications.\n",
      "__Generated answer__:\n",
      "The Evaluation tool in Anthropic's Claude platform can significantly enhance prompt development and the robustness of AI applications in several ways:\n",
      "\n",
      "1. **Identifying Edge Cases**: The tool helps users pinpoint scenarios where prompts may not perform as expected, allowing for targeted improvements.\n",
      "\n",
      "2. **Rating Results**: Users can evaluate individual outputs to understand where prompts excel or fall short, facilitating a more nuanced approach to prompt refinement.\n",
      "\n",
      "3. **Ensuring Consistency**: By testing prompts across a variety of inputs, the Evaluation tool ensures that performance remains stable, which is crucial for reliable AI applications.\n",
      "\n",
      "4. **Refining Prompts**: The tool enables users to make informed adjustments based on the analysis of test case results, leading to more reliable and effective prompts.\n",
      "\n",
      "5. **Pattern Recognition**: Reviewing results allows users to identify patterns in performance, which can inform further adjustments and optimizations.\n",
      "\n",
      "Additionally, the integration of the Prompt Generator tool in the Developer Console aids in crafting high-quality prompts tailored to specific tasks, further enhancing the overall effectiveness of the AI applications built on the Claude platform. By combining the insights gained from the Evaluation tool with the capabilities of the Prompt Generator, developers can create more robust and reliable AI solutions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the main points from the Correct Answer regarding the Evaluation tool's functions, such as identifying edge cases, rating results, ensuring consistency, refining prompts, and recognizing patterns. Additionally, it introduces the Prompt Generator tool, which complements the Evaluation tool, enhancing the overall effectiveness of AI applications. Since it does not omit any critical information and aligns well with the substance of the Correct Answer, it should be marked as correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [05:12<01:19,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 290, Query:\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?\n",
      "__Correct answer__:\n",
      "The Claude 3 Haiku model has the fastest comparative latency\n",
      "__Generated answer__:\n",
      "According to the comparison tables, the Claude model with the fastest comparative latency is Claude Instant 1.2. It is described as a \"small and fast model\" and is noted for being the cheapest and fastest among the legacy models compared.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly identifies the Claude Instant 1.2 model as having the fastest comparative latency, while the correct answer states that the Claude 3 Haiku model has this distinction. This is a direct contradiction, making the generated answer incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [05:19<01:34,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 732, Query:\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?\n",
      "__Correct answer__:\n",
      "To have a multi-turn conversation using the Anthropic Messages API in Python, send the full conversation history in the messages parameter each time, including any prior user and assistant messages. The API is stateless, so the entire context must be provided with each request.\n",
      "__Generated answer__:\n",
      "To build up a conversation with multiple turns using the Anthropic Messages API in Python, you need to maintain the full conversational history and send it with each request. The API is stateless, meaning you must include all previous messages in the conversation context.\n",
      "\n",
      "Here’s a step-by-step example of how to do this:\n",
      "\n",
      "1. **Initialize the Anthropic client** with your API key.\n",
      "2. **Create a list to hold the conversation history**.\n",
      "3. **Add user and assistant messages to the list** as the conversation progresses.\n",
      "4. **Send the full conversation history** to the API with each request.\n",
      "\n",
      "Here’s a sample code snippet demonstrating this:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "# Initialize the client with your API key\n",
      "client = anthropic.Anthropic(api_key=\"your_api_key\")\n",
      "\n",
      "# Initialize the conversation history\n",
      "conversation_history = [\n",
      "    {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n",
      "    {\"role\": \"assistant\", \"content\": \"Hello!\"},\n",
      "]\n",
      "\n",
      "# Function to send a message and get a response\n",
      "def send_message(user_message):\n",
      "    # Add the user's message to the conversation history\n",
      "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
      "    \n",
      "    # Call the API with the full conversation history\n",
      "    response = client.messages.create(\n",
      "        model=\"claude-3-5-sonnet-20240620\",\n",
      "        max_tokens=1024,\n",
      "        messages=conversation_history\n",
      "    )\n",
      "    \n",
      "    # Get the assistant's response\n",
      "    assistant_message = response.content\n",
      "    # Add the assistant's response to the conversation history\n",
      "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
      "    \n",
      "    return assistant_message\n",
      "\n",
      "# Example of sending messages in a conversation\n",
      "print(send_message(\"Can you describe LLMs to me?\"))\n",
      "print(send_message(\"What are their applications?\"))\n",
      "```\n",
      "\n",
      "In this example:\n",
      "- The `conversation_history` list is updated with each user and assistant message.\n",
      "- The `send_message` function handles sending the user's message and receiving the assistant's response, updating the conversation history accordingly.\n",
      "- You can continue to call `send_message` with new user inputs to maintain the conversation.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly captures the essence of the correct answer by stating that the full conversation history must be maintained and sent with each request, and it emphasizes the stateless nature of the API. Additionally, it provides a clear step-by-step example that aligns with the requirements outlined in the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 80/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [05:24<01:30,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 518, Query:\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?\n",
      "__Correct answer__:\n",
      "Providing Claude with a specific role, such as being the General Counsel of a company, using XML tags can help it catch critical legal issues and risks in a contract that it might miss without the role context, potentially saving the company millions of dollars.\n",
      "__Generated answer__:\n",
      "Using XML tags to provide a specific role or context can significantly enhance Claude's analysis of a legal contract compared to not using a role prompt. The structured nature of XML tags allows for clearer separation of different parts of the prompt, which improves clarity and reduces the likelihood of misinterpretation. This structured approach ensures that Claude can accurately identify and focus on the relevant sections of the contract, leading to a more precise analysis.\n",
      "\n",
      "When a specific role is defined, such as that of a General Counsel, Claude is able to tailor its responses to reflect the expertise and perspective associated with that role. This results in a deeper analysis that identifies critical issues, risks, and recommendations that a general user might overlook. For example, without a role prompt, Claude may provide a general summary of the contract, but with the role of General Counsel, it highlights significant risks related to indemnification, liability, and IP ownership, offering strong recommendations based on those insights.\n",
      "\n",
      "In summary, using XML tags to define a role enhances the accuracy, focus, and depth of Claude's analysis, allowing it to provide more relevant and actionable insights when reviewing legal contracts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how using XML tags to define a specific role enhances Claude's analysis of legal contracts. It emphasizes the importance of clarity and focus that comes from structured prompts, and it discusses the benefits of tailoring responses to a specific role, such as that of a General Counsel. Both answers highlight the potential for identifying critical issues and risks that could be overlooked without the role context. Therefore, the generated answer is correct as it conveys the same substantive information as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [05:28<01:21,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 450, Query:\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?\n",
      "__Correct answer__:\n",
      "When required parameters are missing, Claude 3 Opus is more likely to ask the user for the missing information, while Claude 3 Sonnet is more likely to try to infer reasonable values on its own to proceed with the tool call.\n",
      "__Generated answer__:\n",
      "Claude 3 Opus and Claude 3 Sonnet handle missing information differently when making tool calls. \n",
      "\n",
      "Claude 3 Opus is designed for complex tools and ambiguous queries, and it actively seeks clarification when it encounters missing information. This means that if there are parameters or details that are not provided, Opus will attempt to ask for those specifics to ensure it can execute the tool call effectively.\n",
      "\n",
      "On the other hand, Claude 3 Sonnet, while being the most intelligent model, is not specifically noted for seeking clarification in the same way. It may infer missing parameters instead of asking for them, which could lead to assumptions that may not align with the user's intent.\n",
      "\n",
      "In summary, Opus is proactive in seeking clarification for missing information, while Sonnet may infer what is missing without direct confirmation.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key differences between Claude 3 Opus and Claude 3 Sonnet in handling missing information when making tool calls. It states that Opus seeks clarification for missing parameters, while Sonnet infers reasonable values, which aligns with the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [05:34<01:23,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 595, Query:\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?\n",
      "__Correct answer__:\n",
      "To ensure a reliable production deployment of Claude for ticket routing, key steps include implementing retry logic to handle errors, conducting thorough staging and load testing, setting up error handling and logging, using a gradual rollout process, providing documentation and training, and establishing monitoring and alerting.\n",
      "__Generated answer__:\n",
      "To ensure a reliable deployment of an automated ticket routing system using Claude into a production environment, follow these steps:\n",
      "\n",
      "1. **Implement Retry Logic**: Add try/except logic to handle instances where Claude may not return the expected output or is temporarily unavailable. Incorporate back-off logic to retry after increasing intervals or adjust the temperature for output variations.\n",
      "\n",
      "2. **Thorough Staging Testing**: Conduct extensive testing in a staging environment that closely resembles your production setup to identify potential issues or incompatibilities before deployment.\n",
      "\n",
      "3. **Load Testing**: Perform load testing to ensure the system can handle the anticipated volume of tickets without performance degradation, maintaining responsiveness under real-world conditions.\n",
      "\n",
      "4. **Error Handling and Logging**: Implement comprehensive error handling and logging mechanisms to facilitate debugging and monitoring in production, allowing for quick identification and resolution of issues.\n",
      "\n",
      "5. **Gradual Rollout**: Establish a phased rollout plan, starting with a small percentage of traffic and gradually increasing it while closely monitoring the system’s behavior to minimize risk.\n",
      "\n",
      "6. **Documentation and Training**: Prepare detailed documentation and provide training to relevant stakeholders on using and maintaining the new system effectively to ensure a smooth transition and promote adoption.\n",
      "\n",
      "7. **Monitoring and Alerting**: Set up robust monitoring and alerting mechanisms to proactively detect and address any issues that may arise in production, enabling quick responses to minimize downtime.\n",
      "\n",
      "By following these steps, you can achieve a successful and reliable deployment of your automated ticket routing system, ensuring a seamless experience for users.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key steps outlined in the correct answer for ensuring a reliable deployment of an automated ticket routing system using Claude. It includes all the essential elements such as implementing retry logic, conducting thorough staging and load testing, setting up error handling and logging, using a gradual rollout process, providing documentation and training, and establishing monitoring and alerting. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [05:39<01:20,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 524, Query:\n",
      "How should you evaluate a model's performance on a ticket routing classifier?\n",
      "__Correct answer__:\n",
      "You should evaluate performance in terms of accuracy, cost, and speed.\n",
      "__Generated answer__:\n",
      "To evaluate a model's performance on a ticket routing classifier, you should focus on several key metrics:\n",
      "\n",
      "1. **Accuracy**: Calculate the percentage of correct predictions made by the model compared to the actual intents. This is a fundamental measure of how well the model performs.\n",
      "\n",
      "2. **Response Time**: Measure the time taken for the model to classify a ticket. The 95th percentile response time is particularly useful as it indicates the time within which 95% of the requests are processed, helping to understand the model's speed under load.\n",
      "\n",
      "3. **Cost**: Assess the average cost per classification request. This metric is important for understanding the financial implications of deploying the model in a production environment.\n",
      "\n",
      "4. **Consistency and Reliability**: Evaluate how consistently the model performs across different ticket types and scenarios, including edge cases and ambiguous tickets.\n",
      "\n",
      "5. **Interpretability**: Analyze the usefulness of the model's classifications for human agents, including the clarity of the reasoning provided by the model.\n",
      "\n",
      "6. **Stability and Maintainability**: Consider the overall stability of the system and how easy it is to maintain and update the model.\n",
      "\n",
      "7. **Incremental Rollout Testing**: Conduct further testing and consider an incremental rollout to build confidence in the model's performance before full deployment.\n",
      "\n",
      "By focusing on these metrics, you can comprehensively assess the readiness of your ticket routing classifier for production use.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies accuracy, cost, and speed as key metrics for evaluating a model's performance on a ticket routing classifier. It also provides additional relevant metrics such as response time, consistency, interpretability, stability, and incremental rollout testing, which enhance the evaluation process. However, the core elements of accuracy, cost, and speed are present and emphasized, making the generated answer substantially correct. Therefore, it aligns well with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [05:42<01:07,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 334, Query:\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?\n",
      "__Correct answer__:\n",
      "Anthropic recommends trying their interactive GitHub prompting tutorial and Google Sheets prompting tutorial to learn prompt engineering concepts before diving into the techniques in the documentation.\n",
      "__Generated answer__:\n",
      "Anthropic recommends two methods for learning how to prompt engineer with Claude: \n",
      "\n",
      "1. Utilizing the Claude for Sheets prompting examples workbench, which is a Claude-powered spreadsheet that houses example prompts and prompt engineering structures.\n",
      "2. Engaging with the in-depth prompt engineering interactive tutorial that utilizes Claude for Sheets, which requires an API key to access.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two methods for learning prompt engineering with Claude, but it does not accurately reflect the methods mentioned in the correct answer. The correct answer specifies a \"GitHub prompting tutorial\" and a \"Google Sheets prompting tutorial,\" while the generated answer refers to a \"Claude for Sheets prompting examples workbench\" and an \"in-depth prompt engineering interactive tutorial.\" This discrepancy indicates that the generated answer does not align with the correct answer's content, leading to a conclusion that it is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [05:47<01:05,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 548, Query:\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?\n",
      "__Correct answer__:\n",
      "Pretrained large language models are trained on unlabeled text data to predict the next word given the previous context, but are not inherently good at answering questions or following instructions without prompt engineering. In contrast, Claude is a large language model that has been further fine-tuned and trained using RLHF to be more helpful, honest, and capable of performing a wider range of useful tasks.\n",
      "__Generated answer__:\n",
      "The key differences between a pretrained large language model and Claude primarily lie in their training processes and capabilities. \n",
      "\n",
      "1. **Pretraining Process**: \n",
      "   - Pretrained large language models are initially trained on a large unlabeled corpus of text using autoregressive methods, where they predict the next word based on the previous context. This process does not inherently equip them to answer questions or follow instructions effectively.\n",
      "   - Claude, on the other hand, is also based on an autoregressive model but has undergone additional refinement through fine-tuning and Reinforcement Learning from Human Feedback (RLHF). This makes Claude more adept at understanding and responding to user queries.\n",
      "\n",
      "2. **Capabilities**:\n",
      "   - Pretrained models often require extensive prompt engineering to elicit desired behaviors, as they are not optimized for specific tasks out of the box.\n",
      "   - Claude has been specifically fine-tuned to be more helpful, honest, and harmless, enhancing its performance in conversational contexts and making it more suitable for a wide range of applications.\n",
      "\n",
      "In summary, while both types of models start with a similar pretraining phase, Claude benefits from additional training techniques that improve its usability and effectiveness in real-world interactions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key differences between pretrained large language models and Claude, including the training processes and capabilities. It mentions the autoregressive nature of both models, the need for prompt engineering for pretrained models, and the additional fine-tuning and RLHF that Claude undergoes to enhance its performance. Therefore, it aligns well with the correct answer and does not omit any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [05:55<01:13,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 659, Query:\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?\n",
      "__Correct answer__:\n",
      "Prompt engineering is typically faster, more cost-effective, requires less data and compute resources, and preserves the model's general knowledge compared to fine-tuning. It also allows for greater flexibility, rapid iteration, and transparency.\n",
      "__Generated answer__:\n",
      "Key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain include:\n",
      "\n",
      "1. **Resource Efficiency**: Prompt engineering requires only text input and does not need high-end GPUs or large memory, making it more resource-friendly compared to fine-tuning.\n",
      "\n",
      "2. **Cost-Effectiveness**: Fine-tuning can incur significant costs, especially in cloud-based AI services. Prompt engineering typically utilizes the base model, which is cheaper.\n",
      "\n",
      "3. **Maintaining Model Updates**: Fine-tuned models may require retraining when providers update their models, whereas prompts usually remain effective across different model versions without changes.\n",
      "\n",
      "4. **Time-Saving**: Fine-tuning can take hours or even days, while prompt engineering allows for nearly instantaneous results, enabling quick problem-solving.\n",
      "\n",
      "5. **Minimal Data Needs**: Fine-tuning requires substantial task-specific labeled data, which can be scarce or expensive. In contrast, prompt engineering can work with few-shot or even zero-shot learning.\n",
      "\n",
      "6. **Flexibility and Rapid Iteration**: Prompt engineering allows for quick experimentation and iteration, enabling users to try various approaches and tweak prompts to see immediate results, which is more challenging with fine-tuning.\n",
      "\n",
      "7. **Domain Adaptation**: It is easier to adapt models to new domains by providing domain-specific context in prompts without the need for retraining.\n",
      "\n",
      "8. **Comprehension Improvements**: Prompt engineering is more effective at helping models understand and utilize external content, such as retrieved documents.\n",
      "\n",
      "9. **Preservation of General Knowledge**: Fine-tuning risks catastrophic forgetting, where the model loses its general knowledge. Prompt engineering helps maintain the model's broad capabilities.\n",
      "\n",
      "10. **Transparency**: Prompts are human-readable, providing clarity on what information the model receives, which aids in understanding and debugging the model's behavior.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key advantages of prompt engineering over fine-tuning as outlined in the correct answer. It discusses resource efficiency, cost-effectiveness, the maintenance of model updates, time-saving aspects, minimal data needs, flexibility and rapid iteration, domain adaptation, comprehension improvements, preservation of general knowledge, and transparency. All these points align well with the correct answer's emphasis on speed, cost, data requirements, flexibility, and preservation of general knowledge. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [06:00<01:03,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 417, Query:\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?\n",
      "__Correct answer__:\n",
      "Before running requests to access Claude models on Vertex AI, you may need to run `gcloud auth application-default login` to authenticate with GCP.\n",
      "__Generated answer__:\n",
      "To authenticate with Google Cloud Platform (GCP) before running requests to access Claude models on Vertex AI, you need to execute the following command in your terminal:\n",
      "\n",
      "```bash\n",
      "gcloud auth application-default login\n",
      "```\n",
      "\n",
      "This command will allow you to authenticate your application with GCP. Additionally, every API call to the Claude AI model requires a valid API key. You can set this API key by exporting it as an environment variable:\n",
      "\n",
      "For macOS and Linux:\n",
      "```bash\n",
      "export ANTHROPIC_API_KEY='your-api-key-here'\n",
      "```\n",
      "\n",
      "For Windows:\n",
      "```bash\n",
      "set ANTHROPIC_API_KEY='your-api-key-here'\n",
      "```\n",
      "\n",
      "Alternatively, you can supply the API key directly when initializing the Anthropic client in your code.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states the command to authenticate with GCP (`gcloud auth application-default login`) and provides additional relevant information about setting the API key, which is also important for accessing Claude models. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [06:02<00:49,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 496, Query:\n",
      "What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\n",
      "__Correct answer__:\n",
      "According to the information provided, on May 10th, 2024, Anthropic introduced a new \"Prompt Generator\" tool in the Developer Console. This tool is designed to help users guide Claude to generate high-quality prompts tailored to their specific tasks. The text states that the Prompt Generator \"makes it easy to guide Claude to generate a high-quality prompts tailored to your specific tasks.\" This indicates that the Prompt Generator feature provides users with the ability to create customized prompts for Claude, going beyond the standard prompting capabilities. By combining this information with the details about the Claude iOS app and the Claude Team plan released around the same time, we can infer that Anthropic was expanding its platform and tools to provide users with more advanced capabilities for interacting with and leveraging the Claude AI assistant for their specific needs and use cases.\n",
      "__Generated answer__:\n",
      "On May 10th, 2024, Anthropic introduced a new Prompt Generator tool in the Developer Console. This tool enables users to create high-quality prompts tailored specifically to their tasks, making it easier to guide the Claude AI model in generating desired outputs. The Prompt Generator is designed to enhance user experience by allowing for more precise and effective prompt creation, thereby improving the overall interaction with the AI for various applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the main points from the Correct Answer regarding the introduction of the Prompt Generator tool and its purpose in helping users create tailored prompts for the Claude AI model. Both answers emphasize the tool's role in enhancing user experience and improving interaction with the AI. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [06:04<00:37,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 299, Query:\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?\n",
      "__Correct answer__:\n",
      "Both Claude 3.5 Sonnet and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "__Generated answer__:\n",
      "Both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is identical to the correct answer, stating that both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024. There are no missing pieces of information or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 90/100 questions. Current Accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [06:07<00:31,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 302, Query:\n",
      "When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\n",
      "__Correct answer__:\n",
      "You can use \"max_tokens\": 1 in the request to limit Claude's response to a single token when putting words in its mouth.\n",
      "__Generated answer__:\n",
      "To limit Claude's response to a single token, you can use the header `\"max_tokens\": 1` in the request.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can use the header \"max_tokens\": 1 to limit Claude's response to a single token, which aligns with the correct answer provided. There are no critical pieces of information missing, and the substance of the answers is the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [06:10<00:25,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 326, Query:\n",
      "What does the temperature parameter do when working with large language models?\n",
      "__Correct answer__:\n",
      "Temperature is a parameter that controls the randomness of the model during generation\n",
      "__Generated answer__:\n",
      "The temperature parameter controls the randomness of a model's predictions during text generation. A higher temperature results in more creative and diverse outputs, allowing for variations in phrasing and responses. Conversely, a lower temperature leads to more conservative and deterministic outputs that favor the most probable choices. By adjusting the temperature, users can encourage the language model to explore rare or surprising word choices and sequences, rather than just selecting the most likely predictions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the function of the temperature parameter in controlling the randomness of a model's predictions during text generation. It elaborates on how higher temperatures lead to more creative outputs and lower temperatures result in more deterministic outputs, which aligns with the essence of the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [06:13<00:22,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 450, Query:\n",
      "What are two ways to specify API parameters when calling the Claude API using Claude for Sheets?\n",
      "__Correct answer__:\n",
      "When calling the Claude API using Claude for Sheets, you can specify API parameters in two ways: 1) As additional arguments after the prompt and model in the CLAUDE() function, like =CLAUDE(prompt, model, \"max_tokens\", 3). 2) By passing in an API key to be used just for a specific cell, like \"api_key\", \"sk-ant-api03-j1W...\"\n",
      "__Generated answer__:\n",
      "Two ways to specify API parameters when calling the Claude API using Claude for Sheets are:\n",
      "\n",
      "1. **Simple Prompt**: You can call the API with just a prompt, for example: `=CLAUDE(\"Claude, in one sentence, what's good about the color blue?\")`.\n",
      "\n",
      "2. **Adding Parameters**: You can specify parameters by including them after the initial prompt, like this: `=CLAUDE(\"Hi, Claude!\", \"claude-3-haiku-20240307\", \"max_tokens\", 3)`. This allows you to set various API parameters, including the model and specific options like `max_tokens`.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways to specify API parameters when calling the Claude API using Claude for Sheets. It mentions the use of a simple prompt and the addition of parameters after the prompt, which aligns with the correct answer's description. However, it does not explicitly mention the ability to pass an API key for a specific cell, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing an important detail and should be marked incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [06:19<00:24,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 415, Query:\n",
      "How does prefilling the response with an opening curly brace ({ ) affect Claude's output when extracting structured data from text?\n",
      "__Correct answer__:\n",
      "Prefilling Claude's response with { causes it to skip the preamble explanation and directly output the extracted data as a JSON object, resulting in a more concise response that is easier for programs to parse without additional processing.\n",
      "__Generated answer__:\n",
      "Prefilling the response with an opening curly brace ({) can influence Claude's output when extracting structured data from text by signaling that the expected response format is a structured object, such as JSON. This can help guide Claude to produce output that adheres to a specific structure, making it easier to parse and utilize the data programmatically. By starting with a curly brace, it indicates that the response should be formatted as a key-value pair structure, which is common in JSON, thus encouraging Claude to continue in that format. This technique can be particularly useful when the user wants to ensure that the output is structured and easily interpretable as data rather than free-form text.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains that prefilling the response with an opening curly brace ({) signals that the expected response format is structured, specifically JSON. It also mentions that this helps guide Claude to produce output that adheres to a specific structure, making it easier to parse programmatically. However, it lacks the specific detail that this approach causes Claude to skip the preamble explanation and directly output the extracted data, which is a critical piece of information in the correct answer. Therefore, while the generated answer captures the essence of the correct answer, it misses an important aspect regarding the conciseness and directness of the output.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [06:25<00:23,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 522, Query:\n",
      "What are some helpful resources provided by Anthropic to dive deeper into building with images using Claude?\n",
      "__Correct answer__:\n",
      "Anthropic provides a multimodal cookbook with tips on getting started with images and best practices, as well as API reference documentation for the Messages API that includes example API calls involving images.\n",
      "__Generated answer__:\n",
      "To dive deeper into building with images using Claude, you can explore the following helpful resources provided by Anthropic:\n",
      "\n",
      "1. **Multimodal Cookbook**: This resource offers tips on getting started with images and best practices for ensuring high-quality performance. It includes guidance on effectively prompting Claude with images for tasks like interpreting charts or extracting content from forms.\n",
      "\n",
      "2. **API Reference**: The documentation for the Messages API includes example API calls that involve images, helping you understand how to interact with Claude programmatically.\n",
      "\n",
      "3. **Prompt Examples**: This section demonstrates best practice prompt structures for image-based prompts. It emphasizes that Claude performs best when images are placed before text, recommending an image-then-text structure when possible.\n",
      "\n",
      "4. **Prompt Engineering Guide**: This guide helps you master prompt crafting to optimize your interactions with Claude, particularly useful for fine-tuning with legacy models.\n",
      "\n",
      "5. **Prompt Library**: A collection of pre-crafted prompts for various tasks and industries, providing inspiration and quick starts for your projects.\n",
      "\n",
      "6. **API Documentation**: Comprehensive information on how to interact with Claude via the API, including request formats, response handling, and troubleshooting tips.\n",
      "\n",
      "These resources collectively cover the essentials for effectively using images with Claude and enhancing your development experience.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides a comprehensive overview of resources offered by Anthropic for building with images using Claude. It includes the Multimodal Cookbook, API Reference, Prompt Examples, Prompt Engineering Guide, Prompt Library, and API Documentation. All these resources align with the Correct Answer, which mentions the Multimodal Cookbook and API reference documentation. The additional details in the Generated Answer enhance the understanding of how to use these resources effectively, but do not contradict or omit any critical information from the Correct Answer. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [06:30<00:18,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 473, Query:\n",
      "How do you specify the API key when creating a new Anthropic client in the Python and TypeScript SDK examples?\n",
      "__Correct answer__:\n",
      "In both the Python and TypeScript examples, you can specify the API key as a string parameter when creating a new Anthropic client object. If no API key is provided, it defaults to using the ANTHROPIC_API_KEY environment variable.\n",
      "__Generated answer__:\n",
      "In the Python and TypeScript SDK examples for creating a new Anthropic client, you can specify the API key in the following ways:\n",
      "\n",
      "**Python:**\n",
      "You can set the API key by exporting the environment variable `ANTHROPIC_API_KEY` or by passing it directly when initializing the client (though the specific Python example is not provided in the documents).\n",
      "\n",
      "**TypeScript:**\n",
      "You specify the API key when creating a new Anthropic client like this:\n",
      "\n",
      "```typescript\n",
      "import Anthropic from '@anthropic-ai/sdk';\n",
      "\n",
      "const anthropic = new Anthropic({\n",
      "  apiKey: 'my_api_key', // defaults to process.env[\"ANTHROPIC_API_KEY\"]\n",
      "});\n",
      "```\n",
      "\n",
      "In this example, you can replace `'my_api_key'` with your actual API key. If you do not provide the API key directly, the SDK will default to using the `ANTHROPIC_API_KEY` environment variable.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to specify the API key in both the Python and TypeScript SDK examples. It mentions that the API key can be set via the environment variable `ANTHROPIC_API_KEY` or passed directly when initializing the client, which aligns with the correct answer. The TypeScript example provided is also accurate and illustrates the point well. Therefore, the generated answer captures the essential information and is consistent with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [06:33<00:12,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 372, Query:\n",
      "What are two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application?\n",
      "__Correct answer__:\n",
      "The Evaluation tool helps identify edge cases where the prompt might falter, and ensures consistent performance across a range of test case inputs. This allows you to refine the prompt for better reliability in the AI classification application.\n",
      "__Generated answer__:\n",
      "Two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application are:\n",
      "\n",
      "1. **Identifying Edge Cases**: The Evaluation tool helps users pinpoint edge cases where the prompt might not perform as expected, allowing for targeted improvements and refinements.\n",
      "\n",
      "2. **Ensuring Consistent Performance**: It enables users to rate individual results and assess the prompt's performance across a range of inputs, ensuring that the prompt delivers reliable and consistent results.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key benefits of using the Anthropic Evaluation tool as stated in the Correct Answer. Both answers emphasize the identification of edge cases and the importance of ensuring consistent performance across various inputs. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [06:38<00:09,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 602, Query:\n",
      "What are the key differences between a pretrained language model like Claude's underlying model, and the final version of Claude available through Anthropic's API?\n",
      "__Correct answer__:\n",
      "The pretrained language model that forms Claude's foundation is not inherently good at answering questions or following instructions. To create the helpful, honest and safe Claude assistant available through the API, the pretrained model underwent fine-tuning and reinforcement learning from human feedback (RLHF).\n",
      "__Generated answer__:\n",
      "The key differences between a pretrained language model like Claude's underlying model and the final version of Claude available through Anthropic's API include:\n",
      "\n",
      "1. **Training and Fine-Tuning**: The pretrained model serves as a foundational model that has been trained on a large corpus of text data. The final version available through the API, such as Claude 3.5 Sonnet, has undergone additional fine-tuning and optimization to enhance its performance, making it more intelligent and capable of handling specific tasks effectively.\n",
      "\n",
      "2. **Model Versioning**: The pretrained model is a generic version, while the final version (e.g., Claude 3.5 Sonnet) is a specific iteration that has been released for public use. This version is tailored for better performance and may include improvements based on user feedback and ongoing research.\n",
      "\n",
      "3. **Access and Usage**: The pretrained model may not be directly accessible for application development, whereas the final version is available through the Anthropic API, allowing developers to integrate it into their applications with specific functionalities and parameters.\n",
      "\n",
      "4. **Capabilities**: The final version typically includes enhancements in areas such as understanding context, generating more coherent responses, and handling complex queries, which may not be present in the initial pretrained model.\n",
      "\n",
      "5. **Deployment Environment**: The pretrained model may be used in various research settings, while the final version is optimized for deployment in production environments, ensuring reliability and efficiency in real-world applications. \n",
      "\n",
      "These differences contribute to the overall effectiveness and usability of the model in practical applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed comparison between the pretrained model and the final version of Claude, touching on aspects such as training, model versioning, access, capabilities, and deployment. However, it lacks the critical information that the pretrained model is not inherently good at answering questions or following instructions, which is a key point in the correct answer. Additionally, it does not mention the process of fine-tuning and reinforcement learning from human feedback (RLHF) that is essential for creating the final version. Therefore, the generated answer is missing important details that are necessary for a complete understanding of the differences.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [06:40<00:03,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 270, Query:\n",
      "What is the IPv6 address range used by Anthropic?\n",
      "__Correct answer__:\n",
      "The IPv6 address range used by Anthropic is 2607:6bc0::/48.\n",
      "__Generated answer__:\n",
      "The IPv6 address range used by Anthropic is 2607:6bc0::/48.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer matches the correct answer exactly, providing the same IPv6 address range used by Anthropic. There are no missing pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [06:42<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 348, Query:\n",
      "When using the Python SDK to create a message with Claude, what are two ways you can specify your API key?\n",
      "__Correct answer__:\n",
      "When using the Python SDK, you can specify your API key either by passing it as the api_key parameter when initializing the Anthropic client, or by setting it as an environment variable named ANTHROPIC_API_KEY which the client will use by default.\n",
      "__Generated answer__:\n",
      "You can specify your API key in two ways when using the Python SDK to create a message with Claude:\n",
      "\n",
      "1. By exporting the API key as an environment variable named `ANTHROPIC_API_KEY`.\n",
      "2. By supplying the API key directly to the Anthropic client when initializing it.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two ways to specify the API key when using the Python SDK: by exporting it as an environment variable named `ANTHROPIC_API_KEY` and by supplying it directly to the Anthropic client during initialization. It captures the essence of the correct answer without omitting any critical information or introducing contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 100/100 questions. Current Accuracy: 0.7200\n",
      "Detailed results saved to evaluation/csvs/evaluation_results_summary_enhanced.csv\n",
      "Average Precision: 0.3967\n",
      "Average Recall: 0.6325\n",
      "Average MRR: 0.7300\n",
      "Average F1: 0.4876\n",
      "End-to-End Accuracy: 0.7200\n",
      "Evaluation complete. Results saved to evaluation/json_results/evaluation_results_summary_enhanced.json, evaluation/csvs/evaluation_results_summary_enhanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "# level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "# level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the evaluations\n",
    "eval_data_range = eval_data[0:100]\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_similar_level_two, eval_data_range, level_two_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context_level_two, level_two_db, eval_data_range)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data_range],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_summary_enhanced.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a json file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_summary_enhanced.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Enhanced\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ea28c0-4e1c-4398-ac76-0739a0a07553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "from utils.plot_perf import plot_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec177621-9804-425e-a2c7-db0a00d6a0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's visualize our accuracy\n",
    "# plot_performance('evaluation/json_results', ['Basic RAG', 'Summary Enhanced'], colors=['skyblue', 'green'])\n",
    "plot_performance('evaluation/json_results', [ 'Summary Enhanced'], colors=['green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d19c2-a063-4bb9-ad29-beedbdf24e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
