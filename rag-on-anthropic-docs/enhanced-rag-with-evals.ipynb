{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4834a618-cb3c-4f71-b4f1-a18063afcc11",
   "metadata": {},
   "source": [
    "# Retrieval in a RAG Flow Enhanced with Document Summaries\n",
    "In this notebook, we'll show an improved approach to our retrieval system by add chunk summaries to the chunks. Instead of embedding chunks directly from the documents, we'll create a concise summary for each chunk and use this summary along with the original chunk content in our embedding process (both generation ad queries).\n",
    "\n",
    "This approach aims to capture the essence of each document chunk more effectively, potentially leading to improved retrieval performance.\n",
    "\n",
    "Key steps in this process:\n",
    "1. For each chunk, we generate a 2-3 sentence summary using OpenAI (or an OpenAI compatible API).\n",
    "2. We store both the original content and the summary for each chunk in a new json file: data/anthropic_summary_indexed_docs.json\n",
    "\n",
    "This summary-enhanced approach is designed to provide more context during the embedding and retrieval phases, potentially improving the system's ability to understand and match the most relevant documents to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b80e4-3558-445c-a17c-5a4b8db4cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## silent setup (-q), may take a while\n",
    "!pip install openai -q\n",
    "!pip install --upgrade tiktoken -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af37b9a7-0878-4b8d-ae76-ad694cb512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "# embeddings_model_name = \"intfloat/multilingual-e5-large-instruct\" # supported fix context length of 512\n",
    "embeddings_model_name = \"jinaai/jina-embeddings-v2-base-en\" # supports variable context-length upto 8K\n",
    "generation_model = \"gpt-4o-mini\" \n",
    "judge_model = \"gpt-4o-mini\"\n",
    "model_temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d1e786-d81b-411c-b2e0-8d618a5f5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262fa9ab-559b-41be-9de3-4ae757c2fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length of model, jinaai/jina-embeddings-v2-base-en:, 4096, about 3072.0 words\n",
      "tensor([[0.9341]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# embeddings_model = SentenceTransformer(embeddings_model_name)\n",
    "# max_len = embeddings_model.max_seq_length\n",
    "# max_word_count = max_len * 0.75\n",
    "# print(f\"Max Sequence Length of model, {embeddings_model_name}:, {max_len}, about {max_word_count} words\")\n",
    "\n",
    "# try a different model with longer context window\n",
    "embeddings_model = SentenceTransformer(\n",
    "    embeddings_model_name, # switch to en/zh for English or Chinese\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# control your input sequence length up to 8192\n",
    "embeddings_model.max_seq_length = 4096\n",
    "\n",
    "max_word_count = embeddings_model.max_seq_length * 0.75\n",
    "print(f\"Max Sequence Length of model, {embeddings_model_name}:, {embeddings_model.max_seq_length}, about {max_word_count} words\")\n",
    "\n",
    "# run a short test\n",
    "from sentence_transformers.util import cos_sim\n",
    "embeddings = embeddings_model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54311b-bba4-40d4-b4fe-f17e54991e10",
   "metadata": {},
   "source": [
    "### Generating the Summaries to Enhance the Chunks\n",
    "Any LLM will do. This is one-time. You can invoke the function is case thay are not already generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b741b86c-f53e-4220-9c5b-bbbe6b7db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gen_summaries_using_llm import generate_summaries\n",
    "# this is already available, so the call is commented out\n",
    "# generate_summaries('data/anthropic_docs.json', 'data/anthropic_summary_indexed_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cfea-b81d-4d06-9c8f-337f9ecc95c9",
   "metadata": {},
   "source": [
    "### Summary-Enhanced Vector Database Creation\n",
    "Here, we're creating a new vector database that incorporates our summary-enhanced document chunks. This approach combines the original text, the chunk heading, and the newly generated summary into a single text for embedding.\n",
    "\n",
    "Key features of this process:\n",
    "\n",
    "1. We create embeddings for the combined text (heading + summary + original content) using the Voyage AI API.\n",
    "2. The embeddings and full metadata (including summaries) are stored in our vector database.\n",
    "3. We implement caching mechanisms to improve efficiency in repeated queries.\n",
    "4. The database is saved to disk for persistence and quick loading in future sessions.\n",
    "\n",
    "This summary-enhanced approach aims to create more informative embeddings, potentially leading to more accurate and contextually relevant document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351e638a-d09a-4295-8de6-6c54ac6e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class SummaryEnhancedVectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/summary_indexed_vector_db.pkl\"\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        \"\"\"not called for now\"\"\"\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size], normalize_embeddings=True)\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "        \n",
    "    def load_data(self, data_file):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Loading vector database from file: {self.db_path}.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "            \n",
    "        # well, if not...\n",
    "        print(f'file {self.db_path} does not exist')\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Embed Chunk Heading + Text + Summary Together\n",
    "        texts = [f\"{item['chunk_heading']}\\n\\n{item['text']}\\n\\n{item['summary']}\" for item in data]\n",
    "        print(f'****Total Chunks: {len(texts)}')\n",
    "        texts_exceeding_max_len = [s for s in texts if len(s) > max_word_count]\n",
    "        print(f'****Chunks greater that {max_word_count} words: {len(texts_exceeding_max_len)}')\n",
    "        \n",
    "        # Embed more than 128 documents with a for loop\n",
    "        # batch_size = 128\n",
    "        batch_size = 16\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size], normalize_embeddings=True)\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data  # Store the entire item as metadata\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=3, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            # print(f'found in cache!')\n",
    "            query_embedding = np.array(self.query_cache[query])  #\n",
    "            # print(f'type:{type(query_embedding)}')\n",
    "        else:\n",
    "            query_embedding = embeddings_model.encode(query, normalize_embeddings=True)\n",
    "            # print(f'query embedding:\\n {query_embedding}')\n",
    "            self.query_cache[query] = query_embedding.tolist()\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        print(f'Saving DB in: {self.db_path}')\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c306c5-1d54-4ca0-9a14-4947caae1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 3 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "from utils.preview_eval_set import preview_json\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4e912-1419-4078-a063-c83d30b9a9de",
   "metadata": {},
   "source": [
    "### Enhanced Retrieval Using Summary-Enhanced Embeddings\n",
    "In this section, we implement the retrieval process using our new summary-enhanced vector database. This approach leverages the enhanced embeddings we created, which incorporate document summaries along with the original content.\n",
    "\n",
    "Key aspects of this updated retrieval process:\n",
    "\n",
    "1. We search the vector database using the query embedding, retrieving the top k most similar documents.\n",
    "2. For each retrieved document, we include the chunk heading, summary, and full text in the context provided to the LLM.\n",
    "3. This enriched context is then used to generate an answer to the user's query.\n",
    "\n",
    "By including summaries in both the embedding and retrieval phases, we aim to provide the LLM with a more comprehensive and focused context. This could potentially lead to more accurate and relevant answers, as the LLM has access to both a concise overview (the summary) and the detailed information (the full text) for each relevant document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a875906-ca83-4bb2-bdce-d8508e45025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from file: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl.\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "def retrieve_similar_level_two(query, db):\n",
    "    # print(f'_______Query used for retrieval________:\\n {query}')\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        chunk = result['metadata']\n",
    "        # show model all 3 items; heading, text, summary\n",
    "        context += f\"\\n <document> \\n Heading:\\n{chunk['chunk_heading']}\\n\\nText:\\n {chunk['text']} \\n\\nSummary: \\n {chunk['summary']} \\n </document> \\n\" \n",
    "    \n",
    "        # print(f'-----------start retrieval {i} --------------')\n",
    "        # print(f\"__Retrieved results heading__:\\n{result['metadata']['chunk_heading']}\")\n",
    "        # print(f\"__Retrieved results text__:\\n{result['metadata']['text']}\")\n",
    "        # print(f\"__Retrieved results summary__:\\n{result['metadata']['summary']}\")\n",
    "        # print(f'-----------end retrieval {i} ----------------')\n",
    "        \n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context_level_two(query, db):\n",
    "    documents, context = retrieve_similar_level_two(query, db)\n",
    "    # print(f'query + context:\\n{construct_prompt(query, context)}')\n",
    "    completion = client.chat.completions.create(\n",
    "    model=generation_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=model_temperature\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Initialize the SummaryEnhancedVectorDB\n",
    "level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "level_two_db.save_db()\n",
    "\n",
    "# # Load the Anthropic documentation\n",
    "# with open('data/anthropic_docs.json', 'r') as f:\n",
    "#     anthropic_docs = json.load(f)\n",
    "\n",
    "# test\n",
    "#query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\n",
    "query = \"What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\"\n",
    "test_results, test_contexts = retrieve_similar_level_two(query, level_two_db)\n",
    "# for i, test_result in enumerate(test_results):\n",
    "#     print(f'ith:{i}\\n {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daa157-bd0c-4462-be43-f2a7d1f06bc4",
   "metadata": {},
   "source": [
    "### Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51398e7-2da9-47ca-90f8-e4e565f6108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"For OpenAI models, returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db) # ??\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_tokens = num_tokens_from_string(comparision_prompt, \"o200k_base\")  # note, this encoding name is for gpt-4o, gpt-4o-mini\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful judge.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=model_temperature,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'Number of query tokens: {nb_tokens}, Query:\\n{query}')\n",
    "            print(f'__Correct answer__:\\n{correct_answer}')\n",
    "            print(f'__Generated answer__:\\n{generated_answer}')\n",
    "            print(f'__Response from judge LLM__:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8852c4-15df-422c-bdb7-cccf25f8da08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  18%|█▊        | 18/100 [00:00<00:01, 80.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.4667, Avg Recall: 0.7500, Avg MRR: 0.9500\n",
      "Processed 20/100 items. Current Avg Precision: 0.3667, Avg Recall: 0.6000, Avg MRR: 0.7417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  45%|████▌     | 45/100 [00:00<00:00, 81.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.4111, Avg Recall: 0.6500, Avg MRR: 0.7611\n",
      "Processed 40/100 items. Current Avg Precision: 0.4167, Avg Recall: 0.6542, Avg MRR: 0.7833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  63%|██████▎   | 63/100 [00:00<00:00, 81.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4200, Avg Recall: 0.6733, Avg MRR: 0.7733\n",
      "Processed 60/100 items. Current Avg Precision: 0.4278, Avg Recall: 0.6944, Avg MRR: 0.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  81%|████████  | 81/100 [00:00<00:00, 81.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.4048, Avg Recall: 0.6631, Avg MRR: 0.7786\n",
      "Processed 80/100 items. Current Avg Precision: 0.4208, Avg Recall: 0.6865, Avg MRR: 0.7938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:01<00:00, 82.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4222, Avg Recall: 0.6824, Avg MRR: 0.7833\n",
      "Processed 100/100 items. Current Avg Precision: 0.4033, Avg Recall: 0.6508, Avg MRR: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:06<09:54,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 436, Query:\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\n",
      "__Correct answer__:\n",
      "To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\n",
      "__Generated answer__:\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. Access the Evaluation screen after composing your prompt in the Anthropic Console.\n",
      "2. Click the ‘Add Test Case’ button to create a new test case.\n",
      "3. Fill in the required values for each variable in your prompt. Ensure your prompt includes at least 1-2 dynamic variables using the double brace syntax ({{variable}}) to facilitate the creation of eval test sets.\n",
      "4. Repeat the process to create additional test cases as needed.\n",
      "\n",
      "Once you have multiple test cases, you can update your original prompt text and re-run the entire evaluation suite to see how changes affect performance across all test cases. This allows you to identify edge cases, rate individual results, and refine your prompt for better reliability.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed and accurate description of how to create multiple test cases in the Anthropic Evaluation tool, including the steps to access the Evaluation screen, add test cases, fill in variables, and the importance of using dynamic variables. It also mentions the ability to update the original prompt and re-run evaluations, which enhances the understanding of the process. This aligns well with the correct answer, which emphasizes the 'Add Test Case' button and filling in values for each variable. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:09<07:51,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 360, Query:\n",
      "What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\n",
      "__Correct answer__:\n",
      "Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\n",
      "__Generated answer__:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models, including options tailored for specific industries such as finance and healthcare, as well as bespoke fine-tuned models for individual customers. They provide a variety of models, including generalist and specialized options, and also offer fine-tuning services to customize models based on unique requirements.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer, including the recommendation of Voyage AI as the embeddings provider and the mention of customized models for specific industries like finance and healthcare. It also includes the aspect of bespoke fine-tuned models for individual customers and highlights the variety of options available. Therefore, there are no critical pieces of information missing, and the substance of the answers is essentially the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:17<09:48,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 629, Query:\n",
      "What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\n",
      "__Correct answer__:\n",
      "When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\n",
      "__Generated answer__:\n",
      "Key success metrics to consider when evaluating Claude's performance on a classification task include:\n",
      "\n",
      "1. **Accuracy**: Measures the proportion of correct predictions made by the model. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: Balances precision and recall, providing a single metric that captures both false positives and false negatives, which is particularly useful in imbalanced datasets.\n",
      "\n",
      "3. **Consistency**: Evaluates whether the model produces similar outputs for similar inputs, ensuring reliability in predictions.\n",
      "\n",
      "4. **Structure**: Assesses if the model's output adheres to the expected format, making it easier to interpret and use.\n",
      "\n",
      "5. **Speed**: Refers to the model's response time, which is crucial for meeting latency requirements in real-time applications.\n",
      "\n",
      "6. **Bias and Fairness**: Important for ensuring that the model does not exhibit biases that could lead to unfair or incorrect classifications, especially in sensitive applications.\n",
      "\n",
      "These metrics relate to choosing the right model to reduce latency in that a smaller model, such as Claude 3 Haiku, is typically preferred for its speed and efficiency in classification tasks. While larger models like Sonnet or Opus may offer better performance for complex reasoning, they may also introduce higher latency. Therefore, evaluating these metrics helps in selecting a model that not only performs well but also meets the necessary speed requirements for the application. Additionally, strategies to reduce latency, such as optimizing network connections and using asynchronous API calls, can further enhance the model's performance in a production environment.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the key success metrics for evaluating Claude's performance on a classification task, including accuracy, F1 score, consistency, structure, speed, and bias and fairness. It also explains how these metrics relate to choosing the right model to reduce latency, mentioning the trade-off between smaller and larger models in terms of speed and performance. Additionally, it includes strategies for further reducing latency, which aligns with the correct answer's emphasis on model selection for speed. Therefore, the generated answer captures the essential points and is consistent with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:23<09:27,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 439, Query:\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\n",
      "__Correct answer__:\n",
      "Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\n",
      "__Generated answer__:\n",
      "Claude for Sheets can improve prompt engineering workflows compared to using chained prompts in two significant ways:\n",
      "\n",
      "1. **Parallel Testing of Prompts**: Claude for Sheets allows users to test prompts across evaluation suites in parallel, enabling a more efficient and scalable approach to prompt engineering. This contrasts with chained prompts, where each subtask must be executed sequentially, potentially leading to longer turnaround times and less flexibility in testing different approaches simultaneously.\n",
      "\n",
      "2. **Resource for Learning and Exploration**: The Claude for Sheets prompting examples workbench serves as a comprehensive resource that houses example prompts and prompt engineering structures. This facilitates exploration and learning about prompt engineering, which can enhance the overall workflow. In contrast, chained prompts may not provide the same level of structured guidance or examples, making it harder for users to refine their prompts effectively.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts: parallel testing of prompts and the resource for learning and exploration. While the correct answer emphasizes office tasks like survey analysis and online data processing, the generated answer focuses on the structured guidance provided by the prompting examples workbench. However, both answers highlight the efficiency of parallel testing over sequential execution. Since the generated answer captures the essence of the correct answer without missing critical information or introducing contradictions, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:26<07:51,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 352, Query:\n",
      "What happens if a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns?\n",
      "__Correct answer__:\n",
      "If a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error.\n",
      "__Generated answer__:\n",
      "If a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. The API requires these specific markers to properly identify the roles of the participants in the conversation. Without them, the prompt does not conform to the expected format, leading to failure in processing the request.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. Both state that missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns will result in an API error, and the generated answer adds that these markers are required to identify the roles in the conversation, which is consistent with the original statement. There are no critical pieces of information missing or contradictions present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:31<07:50,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 473, Query:\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?\n",
      "__Correct answer__:\n",
      "Tool use requests in the Claude API are priced the same as regular API requests, based on the total input and output tokens. However, tool use requests have additional tokens beyond the regular input and output, including the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt that enables tool use, which add to the total tokens and cost.\n",
      "__Generated answer__:\n",
      "The additional tokens required for tool use in Claude API requests impact pricing by increasing the total number of input and output tokens counted for the request. Tool use requests are priced based on the total number of tokens, which includes:\n",
      "\n",
      "1. Tokens from the tools parameter (tool names, descriptions, and schemas).\n",
      "2. Tokens from tool_use content blocks in API requests and responses.\n",
      "3. Tokens from tool_result content blocks in API requests.\n",
      "4. A special system prompt token count that is automatically included when using tools, which varies by model (ranging from 159 to 530 tokens).\n",
      "\n",
      "These additional tokens are added to the normal input and output tokens to calculate the total cost of a request. Therefore, using tools will generally result in a higher cost compared to regular API requests that do not utilize these additional tokens.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer regarding how additional tokens for tool use in Claude API requests affect pricing. It mentions that tool use requests are priced based on the total number of tokens, including the additional tokens from the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt. It also correctly concludes that using tools generally results in a higher cost compared to regular API requests. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:34<06:26,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 311, Query:\n",
      "When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\n",
      "__Correct answer__:\n",
      "The new Usage, Cost, and Rate Limits tabs in the Anthropic Developer Console that show API usage, billing details, and current rate limits will be available on June 27th, 2024.\n",
      "__Generated answer__:\n",
      "The new Anthropic Developer Console features that show API usage, billing details, and rate limits will be available on June 27th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer matches the correct answer in substance, stating the same date and details about the new features in the Anthropic Developer Console. There are no critical pieces of information missing or contradictions present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:40<07:25,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 427, Query:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\n",
      "__Correct answer__:\n",
      "When deciding whether to use CoT, consider if the task requires in-depth thinking that a human would need to work through, and be aware that the increased output length from CoT may impact latency.\n",
      "__Generated answer__:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, two key factors to consider are:\n",
      "\n",
      "1. **Task Complexity**: CoT should be used for tasks that require in-depth thinking, such as complex math, multi-step analysis, or writing complex documents. If the task does not necessitate extensive processing, it may be better to avoid CoT to maintain efficiency.\n",
      "\n",
      "2. **Latency Impact**: The use of CoT can increase output length, which may negatively affect latency. It is important to balance the need for thorough reasoning with the acceptable response time for the task at hand. Choosing the right model can also help optimize this balance.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two key factors to consider when deciding whether to use chain-of-thought (CoT): task complexity and latency impact. It emphasizes the need for in-depth thinking for complex tasks and acknowledges the potential increase in output length affecting latency. Both answers convey the same essential information, thus the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:48<08:44,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 546, Query:\n",
      "How can I use Claude to more easily digest the content of long PDF documents?\n",
      "__Correct answer__:\n",
      "You can upload PDFs and have Claude summarize their content, making it easier to understand the key points of long documents without having to read through everything.\n",
      "__Generated answer__:\n",
      "To use Claude for more easily digesting the content of long PDF documents, follow these steps:\n",
      "\n",
      "1. **Upload the PDF**: Use the PDF upload feature to input your document into Claude. This allows Claude to access the full content of the PDF.\n",
      "\n",
      "2. **Summarization**: Request Claude to summarize the content of the PDF. This can help distill the key points and main ideas, making it easier to understand the overall message without reading the entire document.\n",
      "\n",
      "3. **Quote Extraction**: If you need specific information, ask Claude to extract relevant quotes from the document. This can help you focus on the most pertinent sections without getting lost in the details.\n",
      "\n",
      "4. **Structured Input**: When dealing with multiple documents, structure your input using XML tags. For example, wrap each document in `<document>` tags with `<document_content>` and `<source>` subtags. This organization helps Claude process the information more effectively.\n",
      "\n",
      "5. **Place Long Content First**: Ensure that the longform data is placed at the top of your prompt, above any queries or instructions. This positioning can enhance Claude's performance and understanding of the context.\n",
      "\n",
      "6. **Ask Specific Questions**: After summarizing or extracting quotes, pose specific questions about the content. This can lead to more focused and relevant responses from Claude.\n",
      "\n",
      "By following these steps, you can leverage Claude's capabilities to efficiently digest and understand long PDF documents.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to use Claude to digest long PDF documents by mentioning the upload feature, summarization, quote extraction, structured input, and asking specific questions. It aligns with the correct answer's essence of summarizing content for easier understanding. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:56<09:53,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 275, Query:\n",
      "According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\n",
      "__Correct answer__:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab of the Developer Console.\n",
      "__Generated answer__:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab in the Developer Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer conveys the same essential information as the correct answer, stating that the API rate limits can be viewed in the Rate Limits tab of the Developer Console. The slight difference in wording does not affect the substance of the answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/100 questions. Current Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [01:11<13:28,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 685, Query:\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?\n",
      "__Correct answer__:\n",
      "In addition to accuracy, we can measure the 95th percentile response time and average cost per classification to assess the ticket classification system's performance and production-readiness.\n",
      "__Generated answer__:\n",
      "To measure the performance of the ticket classification system implemented using Claude beyond just accuracy, consider the following metrics and evaluation methods:\n",
      "\n",
      "1. **Precision, Recall, and F1 Score**: \n",
      "   - **Precision** measures the proportion of true positive classifications among all positive classifications made by the model. \n",
      "   - **Recall** assesses the proportion of true positives identified out of all actual positives. \n",
      "   - **F1 Score** is the harmonic mean of precision and recall, providing a balance between the two.\n",
      "\n",
      "2. **Confusion Matrix**: \n",
      "   - Analyze the confusion matrix to understand how well the model is performing across different classes. This will help identify specific classes where the model may be underperforming.\n",
      "\n",
      "3. **Cost and Speed**: \n",
      "   - Evaluate the operational cost of running the model, including computational resources and time taken to classify tickets. This is crucial for assessing the system's efficiency in a real-world setting.\n",
      "\n",
      "4. **Interpretable Reasoning**: \n",
      "   - Utilize Claude's capability to generate human-readable explanations for its classification decisions. This can help in understanding the rationale behind classifications and in building trust with stakeholders.\n",
      "\n",
      "5. **Load Testing**: \n",
      "   - Conduct load testing to ensure the system can handle the expected volume of tickets without performance degradation. This is essential for maintaining responsiveness under real-world conditions.\n",
      "\n",
      "6. **Error Handling and Logging**: \n",
      "   - Implement comprehensive error handling and logging to track issues that arise during classification. Analyzing these logs can provide insights into system performance and areas for improvement.\n",
      "\n",
      "7. **User Feedback**: \n",
      "   - Collect feedback from users regarding the relevance and accuracy of the classifications. This qualitative data can provide context that quantitative metrics may miss.\n",
      "\n",
      "8. **Monitoring and Alerting**: \n",
      "   - Set up monitoring mechanisms to proactively detect and address any issues in production. This ensures that the system remains reliable and effective over time.\n",
      "\n",
      "By employing these methods, you can gain a comprehensive understanding of the performance of your ticket classification system beyond just accuracy, ensuring it meets both technical and business requirements.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive set of metrics and evaluation methods for measuring the performance of the ticket classification system beyond just accuracy. It includes precision, recall, F1 score, confusion matrix analysis, cost and speed evaluation, interpretable reasoning, load testing, error handling, user feedback, and monitoring. However, it does not specifically mention the 95th percentile response time, which is a critical piece of information included in the correct answer. Therefore, while the generated answer is thorough, it is missing a key metric that is highlighted in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [01:17<11:50,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 483, Query:\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?\n",
      "__Correct answer__:\n",
      "With the Text Completions API, the system prompt is added as text before the first \"\\n\\nHuman:\" turn. With the Messages API, the system prompt is specified using the separate \"system\" parameter when making the API request.\n",
      "__Generated answer__:\n",
      "To specify a system prompt using the Text Completions API, you add the text before the first `\\n\\nHuman:` turn in your prompt. For example:\n",
      "\n",
      "```python\n",
      "prompt = \"Today is January 1, 2024.\\n\\nHuman: Hello, Claude\\n\\nAssistant:\"\n",
      "```\n",
      "\n",
      "In contrast, when using the Messages API, you specify the system prompt with the `system` parameter in the API call. For example:\n",
      "\n",
      "```python\n",
      "anthropic.Anthropic().messages.create(\n",
      "    model=\"claude-3-opus-20240229\",\n",
      "    max_tokens=1024,\n",
      "    system=\"Today is January 1, 2024.\",  # <-- system prompt\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "Thus, the main difference lies in the method of specifying the system prompt: as part of the raw string in Text Completions versus as a separate parameter in Messages.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to specify a system prompt using both the Text Completions API and the Messages API, aligning well with the correct answer. It correctly states that the system prompt for the Text Completions API is added before the first \"\\n\\nHuman:\" turn and that for the Messages API, it is specified using a separate \"system\" parameter. The examples provided further clarify the explanation without omitting any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 736\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [01:26<12:26,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 687, Query:\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?\n",
      "__Correct answer__:\n",
      "You can combine XML tags like <thinking> and <answer> with chain of thought reasoning, where Claude explains its step-by-step reasoning process, to create structured, high-performance prompts. For example, you can prompt Claude to show its reasoning by including \"Before answering, explain your reasoning step-by-step in <thinking> tags.\" in the user message or system prompt.\n",
      "__Generated answer__:\n",
      "To combine XML tags with chain of thought reasoning for creating high-performance prompts for Claude, follow these guidelines:\n",
      "\n",
      "1. **Use XML Tags for Structure**: Clearly separate different parts of your prompt using XML tags. This enhances clarity and reduces the chances of misinterpretation by Claude. For example, you can use tags like `<task>`, `<subtask>`, and `<context>` to delineate different sections of your prompt.\n",
      "\n",
      "2. **Identify Subtasks**: Break your overall task into distinct, sequential subtasks. Each subtask should have a single, clear objective, making it easier for Claude to focus on one aspect at a time.\n",
      "\n",
      "3. **Nest Tags for Hierarchical Content**: When dealing with complex information, nest your XML tags to reflect the hierarchy of the content. For instance, you might have `<project><task><subtask></subtask></task></project>` to represent a structured breakdown.\n",
      "\n",
      "4. **Incorporate Chain of Thought**: Use tags like `<thinking>` and `<answer>` to guide Claude through the reasoning process. This helps in structuring the thought process and allows for a more coherent response.\n",
      "\n",
      "5. **Iterate and Refine**: After receiving outputs from Claude, refine your prompts based on its performance. This iterative process helps in improving the clarity and effectiveness of your prompts over time.\n",
      "\n",
      "6. **Consistency in Tagging**: Maintain consistent tag names throughout your prompts. This consistency aids in clarity and helps Claude understand the context better.\n",
      "\n",
      "7. **Combine Techniques**: Leverage the power of combining XML tags with other techniques such as multishot prompting. For example, you can use `<examples>` to provide multiple instances that Claude can learn from, enhancing its ability to generate relevant responses.\n",
      "\n",
      "By following these practices, you can create structured, high-performance prompts that effectively utilize both XML tagging and chain of thought reasoning.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive approach to combining XML tags with chain of thought reasoning for creating high-performance prompts for Claude. It includes the use of XML tags for structure, identifying subtasks, nesting tags, incorporating chain of thought, iterating and refining prompts, maintaining consistency in tagging, and combining techniques. However, it does not explicitly mention the specific use of `<thinking>` and `<answer>` tags as highlighted in the correct answer. This omission is critical as it directly relates to the core concept of using chain of thought reasoning. Therefore, while the generated answer contains valuable information, it lacks a key element from the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [01:31<10:30,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 479, Query:\n",
      "When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\n",
      "__Correct answer__:\n",
      "When evaluating the claude-3-haiku-20240307 model's performance on the 91 test samples, the three key metrics calculated are accuracy (89.01%), 95th percentile response time (1.61 seconds), and average cost per request routing ($0.0004).\n",
      "__Generated answer__:\n",
      "The three key metrics calculated to evaluate the Claude model's performance for ticket routing are:\n",
      "\n",
      "1. **Accuracy**: This measures how often the model's predictions match the correct classifications.\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive cases and its ability to avoid false positives.\n",
      "3. **Consistency**: This assesses whether the model's predictions are stable and logical across similar inputs.\n",
      "\n",
      "As for the results of the claude-3-haiku-20240307 model on the 91 test samples, the specific performance metrics (accuracy, F1 score, and consistency values) are not provided in the documents. Therefore, the exact results for this model on the test samples cannot be determined from the available information.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it lists different key metrics (accuracy, F1 score, and consistency) compared to the Correct Answer, which specifies accuracy, 95th percentile response time, and average cost per request routing. Additionally, the Generated Answer does not provide any specific results for the claude-3-haiku-20240307 model, which are clearly stated in the Correct Answer. Therefore, there are critical pieces of information missing in the Generated Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:36<09:29,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 411, Query:\n",
      "Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\n",
      "__Correct answer__:\n",
      "Before prompt engineering, Anthropic highly recommends having a clear definition of success criteria for your use case, some ways to empirically test against those criteria, and a first draft prompt you want to improve.\n",
      "__Generated answer__:\n",
      "Before starting to engineer and improve a prompt in Claude, Anthropic recommends having the following key things in place:\n",
      "\n",
      "1. **Understanding of Claude's Capabilities**: Familiarize yourself with what Claude can do by exploring its capabilities and development flow.\n",
      "\n",
      "2. **API Setup**: Ensure you have installed the SDK, set your API key, and are able to make API calls.\n",
      "\n",
      "3. **Access to Resources**: Utilize the Prompt Engineering Guide to master prompt crafting, the Prompt Library for inspiration from pre-crafted prompts, and the API Documentation for details on request formats and response handling.\n",
      "\n",
      "These steps will help you optimize Claude's performance and effectively engineer prompts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes additional recommendations such as understanding Claude's capabilities, setting up the API, and accessing resources, which are not mentioned in the correct answer. However, it fails to include the critical elements from the correct answer, specifically the importance of having a clear definition of success criteria, ways to test against those criteria, and a first draft prompt to improve. Since these key components are missing, the generated answer does not fully align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:42<09:07,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 473, Query:\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?\n",
      "__Correct answer__:\n",
      "The Messages API allows you to continue a response by making the last input message have the \"assistant\" role, whereas the Text Completions API lets you pre-fill part of Claude's response directly in the prompt string.\n",
      "__Generated answer__:\n",
      "The Messages API handles mid-response prompting differently compared to the Text Completions API. \n",
      "\n",
      "In the Text Completions API, you can pre-fill part of the assistant's response by including it directly in the prompt. For example, you might set a prompt that ends with \"Assistant: Hello, my name is\", and the model will continue from there.\n",
      "\n",
      "In contrast, the Messages API allows you to achieve a similar effect by structuring the last input message with the assistant role. For instance, if you want the assistant to continue from a specific point, you would include a message like `{\"role\": \"assistant\", \"content\": \"Hello, my name is\"}` as the last entry in the message list. The model will then generate a continuation based on that input.\n",
      "\n",
      "Thus, while both APIs allow for mid-response prompting, the method of implementation differs: Text Completions uses a raw prompt, while Messages uses a structured list of messages with defined roles.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences between the Messages API and the Text Completions API regarding mid-response prompting. It correctly states that the Text Completions API allows pre-filling part of the assistant's response in the prompt, and it explains how the Messages API uses a structured message with the assistant role to continue a response. Both answers convey the same essential information without any critical omissions or contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:48<08:57,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 522, Query:\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?\n",
      "__Correct answer__:\n",
      "When given the role of CFO through a system prompt, Claude provides a much more insightful, structured, and actionable financial analysis compared to not having a specific role. The role-based response breaks down key financial metrics, provides strategic commentary, and makes specific recommendations.\n",
      "__Generated answer__:\n",
      "Claude's response differs significantly when given a specific role through a system prompt compared to not having a defined role. When a role is assigned, such as \"You are a seasoned data scientist at a Fortune 500 company,\" Claude's performance is enhanced in several ways:\n",
      "\n",
      "1. **Enhanced Accuracy**: With a defined role, Claude is better equipped to handle complex scenarios, such as financial analysis, leading to more accurate and relevant insights.\n",
      "\n",
      "2. **Tailored Tone**: The communication style can be adjusted to fit the expectations of the role. For instance, a data scientist's response may be more technical and concise, while a marketing strategist might provide a more narrative-driven analysis.\n",
      "\n",
      "3. **Improved Focus**: By establishing a role context, Claude remains more aligned with the specific requirements of the task, ensuring that the analysis is relevant and targeted.\n",
      "\n",
      "In contrast, without a specific role, Claude may lack the necessary context to provide focused and accurate insights, potentially leading to less relevant or generalized responses. The absence of a role can result in a broader, less tailored approach that may not meet the specific needs of the financial analysis task.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the essence of the Correct Answer by highlighting the differences in Claude's responses when given a specific role versus not having one. It discusses enhanced accuracy, tailored tone, and improved focus, which align with the structured and actionable insights mentioned in the Correct Answer. However, it does not explicitly mention the breakdown of key financial metrics or specific recommendations, which are critical aspects of the Correct Answer. Therefore, while the Generated Answer is largely correct in its overall message, it lacks some critical details that are present in the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:58<10:10,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 722, Query:\n",
      "What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\n",
      "__Correct answer__:\n",
      "Quantitative metrics for evaluating a sentiment analysis model include task-specific metrics like F1 score, as well as generic metrics like accuracy, precision, and recall. Specific targets should be based on industry benchmarks, prior experiments, AI research, or expert knowledge, and should represent an improvement over the current baseline.\n",
      "__Generated answer__:\n",
      "Quantitative metrics that can be used to measure the success of a sentiment analysis model include:\n",
      "\n",
      "1. **Accuracy**: The proportion of correct predictions made by the model. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive instances and its ability to avoid false positives.\n",
      "\n",
      "3. **Precision**: The ratio of true positive predictions to the total number of positive predictions made by the model. It indicates how many of the predicted positive sentiments were actually correct.\n",
      "\n",
      "4. **Recall**: The ratio of true positive predictions to the total number of actual positive instances. It measures the model's ability to identify all relevant instances.\n",
      "\n",
      "5. **Response Time**: The time taken by the model to produce a response, measured in milliseconds. This is important for applications requiring real-time feedback.\n",
      "\n",
      "6. **Consistency**: This measures how consistently the model produces similar outputs for similar inputs, which is crucial for user trust and experience.\n",
      "\n",
      "7. **Bias and Fairness Metrics**: These assess whether the model's predictions are free from biases based on sensitive attributes like gender or ethnicity.\n",
      "\n",
      "Targets for these metrics can be determined based on:\n",
      "\n",
      "- **Industry Benchmarks**: Research existing models in the same domain to establish a baseline for performance.\n",
      "  \n",
      "- **Prior Experiments**: Analyze results from previous iterations of the model to set realistic improvement goals.\n",
      "\n",
      "- **Expert Knowledge**: Consult with domain experts to understand what constitutes acceptable performance in specific applications.\n",
      "\n",
      "- **User Needs**: Align targets with the expectations and requirements of the end-users, ensuring that the model meets practical needs.\n",
      "\n",
      "For example, a specific target for a sentiment analysis model might be to achieve an F1 score of at least 0.85 on a held-out test set of diverse Twitter posts, which could represent a 5% improvement over the current baseline. This target is measurable, specific, and achievable based on prior performance and industry standards.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies several quantitative metrics for evaluating a sentiment analysis model, including accuracy, F1 score, precision, recall, and additional metrics like response time, consistency, and bias/fairness metrics. It also provides a comprehensive explanation of how to determine specific targets for these metrics, including industry benchmarks, prior experiments, expert knowledge, and user needs. The details provided align well with the Correct Answer, which also emphasizes the importance of setting targets based on improvement over the current baseline. Therefore, the Generated Answer is correct as it captures the essence of the Correct Answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  19%|█▉        | 19/100 [02:01<08:12,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 325, Query:\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?\n",
      "__Correct answer__:\n",
      "Combining XML tags with other prompt engineering techniques like multishot prompting (using <examples> tags) or chain of thought (using <thinking> and <answer> tags) to create super-structured, high-performance prompts.\n",
      "__Generated answer__:\n",
      "A power user tip mentioned in the documentation for creating high-performance prompts using XML tags is to combine XML tags with other techniques like multishot prompting and chain of thought. This approach creates super-structured, high-performance prompts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by mentioning the combination of XML tags with multishot prompting and chain of thought techniques to create high-performance prompts. It conveys the same idea without omitting any critical information, thus making it correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [02:12<09:53,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 638, Query:\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?\n",
      "__Correct answer__:\n",
      "You can use an LLM like Claude to grade the outputs of other LLMs by providing it with the output to grade along with a detailed rubric. Instruct the LLM to think through its reasoning and then output a simple 'correct' or 'incorrect' result based on how well the output matches the criteria in the rubric.\n",
      "__Generated answer__:\n",
      "To use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric, follow these steps:\n",
      "\n",
      "1. **Develop Detailed Rubrics**: Create clear and specific rubrics that outline the criteria for grading. For example, specify that certain key phrases or concepts must be included in the response to be considered correct.\n",
      "\n",
      "2. **Construct Grading Prompts**: Use a function to build a prompt that instructs Claude to evaluate the answer based on the rubric. The prompt should include the rubric and the answer to be graded, and it should ask Claude to think through its reasoning before providing a final judgment.\n",
      "\n",
      "   Example prompt structure:\n",
      "   ```\n",
      "   Grade this answer based on the rubric:\n",
      "   <rubric>{rubric}</rubric>\n",
      "   <answer>{answer}</answer>\n",
      "   Think through your reasoning in <thinking> tags, then output 'correct' or 'incorrect' in <result> tags.\n",
      "   ```\n",
      "\n",
      "3. **Implement Grading Logic**: Create a function that sends the constructed prompt to Claude and retrieves the grading response. This function should check if the response contains \"correct\" or \"incorrect\" to determine the grade.\n",
      "\n",
      "4. **Evaluate Outputs**: Collect the outputs from the LLMs you want to grade and use the grading function to assess each output against the corresponding golden answer (the expected correct answer).\n",
      "\n",
      "5. **Calculate Scores**: After grading all outputs, calculate the percentage of correct responses to evaluate overall performance.\n",
      "\n",
      "By following these steps, you can effectively leverage Claude to automate the grading of LLM outputs based on predefined rubrics, ensuring a systematic and scalable evaluation process.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides a comprehensive and detailed approach to using an LLM like Claude for grading outputs from other LLMs based on a rubric. It includes the essential steps of developing rubrics, constructing grading prompts, implementing grading logic, evaluating outputs, and calculating scores. These steps align well with the Correct Answer, which emphasizes providing the output and rubric to Claude and instructing it to think through its reasoning. Although the Generated Answer includes more detail, it does not omit any critical information and accurately reflects the process described in the Correct Answer. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 20/100 questions. Current Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [02:19<09:36,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 563, Query:\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?\n",
      "__Correct answer__:\n",
      "To access Voyage embeddings on AWS, subscribe to the model package on AWS Marketplace, select the model to deploy, agree to the terms, and copy the Product ARN for your selected region. Then create a JupyterLab space in SageMaker Studio, upload Voyage's notebook, and follow the instructions to deploy the model package using the ARN.\n",
      "__Generated answer__:\n",
      "To access and deploy Voyage embeddings on AWS Marketplace, follow these steps:\n",
      "\n",
      "1. **Subscribe to the Model Package**:\n",
      "   - Navigate to the model package listing page for Voyage embeddings on AWS Marketplace.\n",
      "   - Select the model you wish to deploy.\n",
      "   - Click on the \"Continue to subscribe\" button.\n",
      "   - Review the details on the \"Subscribe to this software\" page. If you agree with the End-User License Agreement (EULA), pricing, and support terms, click on \"Accept Offer\".\n",
      "   - After selecting \"Continue to configuration\" and choosing a region, you will receive a Product ARN (Amazon Resource Name). This ARN is necessary for creating a deployable model using Boto3.\n",
      "\n",
      "2. **Deploy the Model Package**:\n",
      "   - Again, navigate to the model package listing page and select the model to deploy.\n",
      "   - Follow the same steps as above to subscribe and accept the offer.\n",
      "   - Copy the Product ARN for your selected region.\n",
      "\n",
      "3. **Create a JupyterLab Space**:\n",
      "   - Create a JupyterLab space in SageMaker Studio.\n",
      "   - Upload Voyage’s notebook to the JupyterLab space.\n",
      "   - Follow the instructions provided within the notebook to implement and use the embeddings.\n",
      "\n",
      "By following these steps, you will be able to access and deploy Voyage embeddings on AWS Marketplace.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed and accurate step-by-step process for accessing and deploying Voyage embeddings on AWS Marketplace, which aligns closely with the correct answer. It includes subscribing to the model package, accepting the terms, obtaining the Product ARN, creating a JupyterLab space, and following the notebook instructions. While there are minor differences in wording and some repetition in the steps, the essential information is present and correctly conveyed. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [02:27<10:03,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 616, Query:\n",
      "When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\n",
      "__Correct answer__:\n",
      "When using tools to get JSON output, you should provide a single tool, set the tool_choice to explicitly instruct the model to use that tool, and ensure the tool name and description are from the model's perspective since it will pass the input to the tool.\n",
      "__Generated answer__:\n",
      "To get Claude to produce JSON output following a particular schema, you should focus on the following key aspects of tool setup and prompting:\n",
      "\n",
      "1. **Define Tools Clearly**: When setting up tools, ensure you provide clear names, descriptions, and input schemas in your API request. This helps Claude understand how to use the tools effectively.\n",
      "\n",
      "2. **Use Specific Prompts**: Include a user prompt that explicitly indicates the need for JSON output. For example, you might say, \"Please provide the data in the following JSON format: [insert schema].\"\n",
      "\n",
      "3. **Utilize JSON-Compatible Tools**: Choose tools that are designed to return JSON output. For instance, you can use a tool like `record_summary` that is intended for this purpose.\n",
      "\n",
      "4. **Monitor Tool Use Requests**: After providing the tools and prompts, monitor Claude's decision-making process to ensure it constructs a properly formatted tool use request. This is indicated by a `stop_reason` of `tool_use`.\n",
      "\n",
      "5. **Handle Tool Results**: If Claude uses a tool, extract the tool name and input from its request, execute the tool code client-side, and return the results. Ensure that the results are in the expected JSON format.\n",
      "\n",
      "6. **Refine Prompts Based on Feedback**: Continuously refine your prompts based on the output you receive. Use tools like Workbench to create evaluations and iteratively improve the prompts to ensure they yield the desired JSON structure.\n",
      "\n",
      "By following these steps, you can effectively set up tools and prompt Claude to produce JSON output that adheres to your specified schema.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the correct answer by emphasizing the importance of clear tool definitions, specific prompts for JSON output, and monitoring the tool use requests. However, it does not explicitly mention the need to provide a single tool and set the tool_choice to instruct the model to use that tool, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing a key aspect and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [02:36<10:21,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 677, Query:\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?\n",
      "__Correct answer__:\n",
      "The Claude 3 Haiku model has vision capabilities, is faster, more performant, and more intelligent than the legacy Claude Instant 1.2 model. Claude 3 Haiku also has more up-to-date training data.\n",
      "__Generated answer__:\n",
      "The key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance are as follows:\n",
      "\n",
      "1. **Intelligence and Performance**: Claude 3 Haiku is part of the Claude 3 family, which is designed to be more intelligent and capable than the legacy models, including Claude Instant 1.2. The legacy model is noted to perform less well compared to the Claude 3 models.\n",
      "\n",
      "2. **Speed and Latency**: Claude 3 Haiku is described as the fastest and most compact model for near-instant responsiveness, whereas Claude Instant 1.2 is generally slower and less performant.\n",
      "\n",
      "3. **Multilingual Capabilities**: Both models support multilingual functionality, but Claude 3 Haiku offers better coverage, understanding, and skill compared to Claude Instant 1.2.\n",
      "\n",
      "4. **Vision Capabilities**: Claude Instant 1.2 lacks vision capabilities, while Claude 3 Haiku includes vision support, enhancing its functionality.\n",
      "\n",
      "5. **Context Window and Output**: Both models have a context window of 100K tokens and a maximum output of 4096 tokens, but the overall efficiency and effectiveness of handling these features are superior in Claude 3 Haiku.\n",
      "\n",
      "6. **Cost**: The cost structure for using Claude 3 Haiku is more favorable, with lower input and output costs compared to Claude Instant 1.2, which is the cheapest among the legacy models.\n",
      "\n",
      "7. **Training Data Cut-off**: Claude 3 Haiku has a more recent training data cut-off (August 2023) compared to Claude Instant 1.2, which has a cut-off in early 2023, allowing it to leverage more up-to-date information.\n",
      "\n",
      "Overall, Claude 3 Haiku represents a significant advancement in capabilities, performance, and efficiency over the legacy Claude Instant 1.2 model.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model, including aspects such as intelligence, performance, speed, vision capabilities, multilingual support, training data cut-off, and cost. It aligns well with the Correct Answer, providing a comprehensive overview without omitting critical information. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [02:39<08:02,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 295, Query:\n",
      "What is one key benefit of using examples when prompt engineering with Claude?\n",
      "__Correct answer__:\n",
      "One key benefit of using examples in prompts is that they reduce misinterpretation of instructions, leading to more accurate outputs from Claude.\n",
      "__Generated answer__:\n",
      "One key benefit of using examples when prompt engineering with Claude is that they reduce misinterpretation of instructions. This helps ensure that the prompts are understood correctly, leading to more accurate and effective responses.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by stating that using examples reduces misinterpretation of instructions, which leads to more accurate outputs. Both answers convey the same key benefit of using examples in prompt engineering with Claude, thus they are essentially saying the same thing. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [02:44<07:27,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 362, Query:\n",
      "According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\n",
      "__Correct answer__:\n",
      "Prompt engineering allows you to easily adapt AI models to new domains by providing domain-specific context directly in the prompts, without needing to retrain the model through fine-tuning.\n",
      "__Generated answer__:\n",
      "One key advantage of using prompt engineering instead of fine-tuning is that prompt engineering is far faster and more resource-efficient. It allows for nearly instantaneous results and requires only text input, while fine-tuning necessitates high-end GPUs, large memory, and can take hours or even days to implement. This makes prompt engineering a more agile option for adapting AI models to new domains or tasks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that prompt engineering is faster and more resource-efficient compared to fine-tuning, which aligns with the essence of the correct answer. However, it does not explicitly mention the aspect of providing domain-specific context directly in the prompts, which is a key point in the correct answer. Therefore, while the generated answer captures some advantages of prompt engineering, it lacks a critical piece of information regarding how it allows for adaptation to new domains.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [02:48<06:45,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 351, Query:\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?\n",
      "__Correct answer__:\n",
      "You can make a copy of Anthropic's provided Claude for Sheets workbook template to quickly get started using the extension with your own work.\n",
      "__Generated answer__:\n",
      "To quickly get started using the Claude for Sheets extension with a pre-made template, you can make a copy of the Claude for Sheets workbook template provided by Anthropic. This template serves as a starting point for integrating the Claude AI model into your spreadsheet applications and workflows. You can find the template and instructions on how to copy it in the Claude for Sheets resources. For additional information, you may also visit the Claude for Sheets Google Workspace Marketplace overview page.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can make a copy of the Claude for Sheets workbook template provided by Anthropic to get started with the extension. It also adds context about the template serving as a starting point for integrating the Claude AI model into spreadsheet applications, which is relevant information. Additionally, it mentions where to find the template and instructions, which enhances the answer without contradicting the correct answer. Therefore, the generated answer is essentially saying the same thing as the correct answer and includes additional helpful details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [02:53<06:26,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 400, Query:\n",
      "How does the \"index\" field in the \"content_block_delta\" event relate to the text being streamed in a response?\n",
      "__Correct answer__:\n",
      "The \"index\" field in each \"content_block_delta\" event indicates which content block the text delta applies to. Multiple deltas with the same index consecutively stream the text for a single content block in the response.\n",
      "__Generated answer__:\n",
      "The \"index\" field in the \"content_block_delta\" event indicates the position of the content block being updated within the overall message structure. Each content block is associated with an index that corresponds to its place in the final Message content array. When a \"content_block_delta\" event occurs, it signifies that a change (or delta) is being applied to the text at that specific index. This allows the system to manage and update the text being streamed in a response incrementally, reflecting modifications such as insertions, deletions, or replacements of text at the designated index.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains that the \"index\" field indicates the position of the content block being updated and that it allows for incremental updates to the text being streamed. However, it does not explicitly mention that multiple deltas with the same index consecutively stream the text for a single content block, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing an important aspect of the relationship between the index and the streaming of text in the response.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [02:57<05:51,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 399, Query:\n",
      "How can you include an image as part of a Claude API request, and what image formats are currently supported?\n",
      "__Correct answer__:\n",
      "To include an image in a Claude API request, provide it as a base64-encoded image in an \"image\" content block within the \"messages\" array. The currently supported image formats are JPEG, PNG, GIF, and WebP.\n",
      "__Generated answer__:\n",
      "To include an image as part of a Claude API request, you need to upload the image directly in the request. The supported image formats are JPEG, PNG, GIF, and WebP. Specifically, the MIME types are: \n",
      "\n",
      "- image/jpeg\n",
      "- image/png\n",
      "- image/gif\n",
      "- image/webp\n",
      "\n",
      "Keep in mind that Claude cannot read image URLs, so you must upload the image file itself rather than providing a link. Additionally, there are limits on image size: the maximum file size for images uploaded via the API is 5MB.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that the image must be uploaded directly in the request, which contradicts the correct answer that specifies the image should be provided as a base64-encoded image in an \"image\" content block within the \"messages\" array. While both answers mention the same supported image formats, the critical detail about how to include the image is missing in the generated answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [03:03<06:20,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 500, Query:\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?\n",
      "__Correct answer__:\n",
      "TTFT is a specific measure of latency that captures the time it takes for a language model to generate the first token of its response after receiving a prompt. It is an important component of a model's overall latency and responsiveness, especially for interactive applications.\n",
      "__Generated answer__:\n",
      "The relationship between Time to First Token (TTFT) and latency is that both metrics measure the responsiveness of a language model, but they focus on slightly different aspects of performance. TTFT specifically measures the time it takes for the model to generate the first token of output after receiving a prompt, making it a critical metric for applications that require quick initial feedback, such as chatbots and interactive systems. \n",
      "\n",
      "Latency, on the other hand, refers to the overall time taken for the model to respond to a prompt, which includes the time to process the prompt and generate the entire response. Lower latency indicates faster response times, which is also essential for real-time applications.\n",
      "\n",
      "Both TTFT and latency are influenced by similar factors, including model size, hardware capabilities, network conditions, and the complexity of the prompt. A lower TTFT typically contributes to lower latency, as a quicker generation of the first token can lead to a faster overall response time. Thus, while TTFT is a specific measure of the initial responsiveness, latency encompasses the broader response time of the model.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the relationship between TTFT and latency, highlighting that TTFT is a specific measure of latency focused on the time to generate the first token, while latency encompasses the overall response time. It also mentions the importance of both metrics for applications requiring quick feedback and notes the factors influencing both. Therefore, the generated answer captures the essential points from the correct answer without omitting any critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [03:12<07:23,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 629, Query:\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?\n",
      "__Correct answer__:\n",
      "Providing edge case examples to Claude in the prompt can meaningfully improve its performance in correctly routing support tickets in scenarios where it may otherwise misclassify them, such as implicit requests, emotional prioritization, ambiguous intent vs. routing, or issue prioritization.\n",
      "__Generated answer__:\n",
      "Providing Claude with examples of handling edge cases like implicit requests and emotional prioritization can significantly enhance its performance in routing support tickets. \n",
      "\n",
      "1. **Implicit Requests**: Customers often communicate their needs indirectly. For instance, a statement like “I’ve been waiting for my package for over two weeks now” implies a request for order status. By training Claude with examples of such implicit requests, it can learn to recognize and appropriately categorize these indirect inquiries, ensuring that tickets are routed to the correct department without requiring explicit requests.\n",
      "\n",
      "2. **Emotional Prioritization**: Customers may express dissatisfaction or frustration, and Claude might need guidance on when to prioritize addressing these emotions versus solving the underlying issue. By providing examples of scenarios where emotional context is crucial, Claude can better assess when to prioritize customer sentiment, leading to improved customer satisfaction and more effective ticket resolution.\n",
      "\n",
      "3. **Intent vs. Routing**: There may be instances where Claude identifies a customer’s intent correctly but routes the ticket incorrectly. By clarifying the appropriate routes for certain intents through examples, Claude can improve its routing accuracy, ensuring that tickets reach the right teams more consistently.\n",
      "\n",
      "4. **Issue Prioritization**: When customers present multiple issues in a single interaction, Claude may struggle to identify the primary concern. By providing examples that clarify how to prioritize these issues, Claude can enhance its ability to discern the most pressing problem, leading to more efficient ticket handling.\n",
      "\n",
      "Overall, incorporating these edge case examples into Claude's training helps refine its understanding and classification capabilities, ultimately improving the accuracy and efficiency of the support ticket routing process. Regularly reviewing and updating these examples as the system evolves is also essential to maintain optimal performance.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by explaining how providing examples of edge cases like implicit requests and emotional prioritization can enhance Claude's performance in routing support tickets. It elaborates on several key aspects, including implicit requests, emotional prioritization, intent vs. routing, and issue prioritization, which are all mentioned in the Correct Answer. The Generated Answer does not miss any critical pieces of information and aligns well with the main points of the Correct Answer. Therefore, it can be deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 30/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [03:19<07:33,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 617, Query:\n",
      "How does the stop_reason of \"tool_use\" relate to the overall workflow of integrating external tools with Claude?\n",
      "__Correct answer__:\n",
      "When Claude determines that one of the user-provided tools can help answer the user's query, it constructs a tool use request. This causes the API response to have a stop_reason of \"tool_use\", signaling Claude's intent to use the tool. The user must then extract the tool input from Claude's request, run the actual tool code client-side, and continue the conversation by sending the tool results back to Claude.\n",
      "__Generated answer__:\n",
      "The stop_reason of \"tool_use\" indicates that Claude has determined that utilizing an external tool is necessary to address the user's query effectively. This is a crucial part of the workflow for integrating external tools with Claude, as it signals the transition from Claude's internal processing to the execution of a specific tool that has been provided by the user.\n",
      "\n",
      "In the overall workflow, the integration process involves several steps:\n",
      "\n",
      "1. **Providing Tools and Prompts**: Users must define the tools, including their names, descriptions, and input schemas, along with a user prompt that may require these tools.\n",
      "\n",
      "2. **Decision to Use a Tool**: Claude evaluates the user’s query and decides whether any of the provided tools can assist. If it identifies a suitable tool, it constructs a request to use that tool, which results in the API response containing the stop_reason of \"tool_use\".\n",
      "\n",
      "3. **Execution of Tool Code**: After receiving the tool use request, the user extracts the necessary tool name and input, executes the tool code on their end, and prepares to continue the conversation with the results.\n",
      "\n",
      "4. **Formulating a Response**: Finally, Claude uses the results from the tool to craft a comprehensive response to the original user prompt.\n",
      "\n",
      "This workflow emphasizes that all tools must be explicitly provided by the user, giving them control over the tools available to Claude. The \"tool_use\" stop_reason is a key indicator of Claude's intent to leverage these external tools to enhance its responses.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the role of the \"tool_use\" stop_reason in the workflow of integrating external tools with Claude. It explains that this stop_reason indicates Claude's decision to utilize a tool, which aligns with the Correct Answer. Both answers detail the steps involved in the process, including the user's role in providing tools, executing the tool code, and how Claude formulates a response based on the tool's results. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [03:22<06:19,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 322, Query:\n",
      "According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\n",
      "__Correct answer__:\n",
      "During periods of high usage, an overloaded_error event may be sent in the event stream, which would normally correspond to an HTTP 529 error code in a non-streaming context.\n",
      "__Generated answer__:\n",
      "During periods of high usage for the Anthropic API when using streaming responses, you may receive an \"overloaded_error,\" which corresponds to an HTTP 529 error in a non-streaming context.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately conveys the essential information from the Correct Answer regarding the \"overloaded_error\" event and its corresponding HTTP 529 error code during periods of high usage for the Anthropic API when using streaming responses. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [03:27<05:56,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 331, Query:\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?\n",
      "__Correct answer__:\n",
      "The two types of deltas that can be contained in a content_block_delta event are text_delta and input_json_delta.\n",
      "__Generated answer__:\n",
      "The two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API are likely related to different ways the content block can be modified, such as inserting, deleting, or replacing text. However, the specific types of deltas are not explicitly detailed in the provided documents.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer does not correctly identify the specific types of deltas (text_delta and input_json_delta) as stated in the Correct Answer. Instead, it provides a vague description of modifications without naming the actual deltas. This omission of critical information makes the Generated Answer incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [03:30<05:05,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 326, Query:\n",
      "On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\n",
      "__Correct answer__:\n",
      "Claude 3.5 Sonnet became generally available across those platforms on June 20th, 2024, while tool use became generally available on May 30th, 2024.\n",
      "__Generated answer__:\n",
      "Claude 3.5 Sonnet and tool use both became generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI on June 20th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that both Claude 3.5 Sonnet and tool use became generally available on June 20th, 2024. However, the correct answer specifies that Claude 3.5 Sonnet became available on June 20th, 2024, while tool use became available earlier on May 30th, 2024. This critical piece of information about the different dates is missing from the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [03:34<04:40,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 308, Query:\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?\n",
      "__Correct answer__:\n",
      "Anthropic launched Claude.ai and the Claude iOS app in Europe in May 2024, and then launched them in Canada the following month in June 2024.\n",
      "__Generated answer__:\n",
      "Claude.ai and the Claude iOS app were launched in Europe on May 13th, 2024, followed by their launch in Canada on June 5th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides the correct order of the launches, stating that Claude.ai and the Claude iOS app were launched in Europe in May 2024, followed by their launch in Canada in June 2024. Although it includes specific dates, the essential information regarding the sequence of the launches is accurate and aligns with the correct answer. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [03:40<05:11,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 430, Query:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", what does this indicate and what should be done next to continue the conversation?\n",
      "__Correct answer__:\n",
      "A stop_reason of \"tool_use\" signals that Claude has decided to use a tool and has constructed a formatted tool use request. To continue the conversation, the tool name and input should be extracted from Claude's request, the actual tool code should be executed client-side, and then a new user message containing a tool_result content block should be sent to Claude.\n",
      "__Generated answer__:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", it indicates that Claude has determined that a tool is necessary to assist with the user's query. The next step is to extract the tool name and input from Claude's request, execute the corresponding tool code on your end, and then return the results to Claude. This is done by continuing the conversation with a new user message that includes a tool_result content block containing the results from the tool execution. This allows Claude to analyze the tool results and formulate a final response to the original user prompt.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer. It explains that a stop_reason of \"tool_use\" indicates that Claude has determined a tool is necessary, and it outlines the steps to continue the conversation, including extracting the tool name and input, executing the tool code, and sending a new user message with the tool results. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [03:45<05:06,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 382, Query:\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?\n",
      "__Correct answer__:\n",
      "The example code snippet for evaluating tone and style in a customer service chatbot uses the anthropic Python library to interact with the Claude AI model.\n",
      "__Generated answer__:\n",
      "The Python libraries used in the example code snippet for evaluating tone and style in a customer service chatbot include:\n",
      "\n",
      "1. `anthropic` - This library is used to interact with the Anthropic API for generating responses and evaluating the tone of customer inquiries.\n",
      "2. `numpy` - This library is used for numerical operations, particularly in the cosine similarity evaluation.\n",
      "3. `sentence_transformers` - This library is used to create sentence embeddings for evaluating the consistency of responses.\n",
      "\n",
      "These libraries facilitate the evaluation of the chatbot's performance in terms of tone and style, as well as other metrics.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes additional libraries (`numpy` and `sentence_transformers`) that are not mentioned in the correct answer. While it correctly identifies the `anthropic` library, the inclusion of the other libraries suggests a broader context that may not be accurate based on the correct answer provided. Therefore, the generated answer is not fully aligned with the correct answer, which only specifies the `anthropic` library.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [03:48<04:43,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 333, Query:\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?\n",
      "__Correct answer__:\n",
      "The two main ways to authenticate are: 1) Directly providing the aws_access_key, aws_secret_key, and optionally aws_session_token, or 2) Using the default AWS credential providers, such as the ~/.aws/credentials file or the AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID environment variables.\n",
      "__Generated answer__:\n",
      "The two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock are by creating an account and obtaining API keys.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it does not mention the specific methods of authentication outlined in the correct answer, such as providing AWS access keys directly or using default AWS credential providers. Instead, it focuses on creating an account and obtaining API keys, which is not aligned with the correct authentication methods for the Anthropic Python SDK.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [04:02<07:15,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 405, Query:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\n",
      "__Correct answer__:\n",
      "When deciding to use leak-resistant prompt engineering, the potential reduction in prompt leaks should be balanced against the risk of degraded model performance due to the added complexity of the prompt.\n",
      "__Generated answer__:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, two key factors to consider and balance are:\n",
      "\n",
      "1. **Complexity vs. Performance**: Implementing leak-resistant strategies can add complexity to the prompts, which may degrade the overall performance of the model. It's essential to ensure that any added complexity does not negatively impact the quality of the outputs or the model's performance on the task.\n",
      "\n",
      "2. **Necessity vs. Alternatives**: Leak-resistant techniques should only be used when absolutely necessary. Before resorting to these strategies, it's advisable to explore simpler monitoring techniques, such as output screening and post-processing, to address prompt leaks without complicating the prompt engineering process.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the need to balance complexity against performance, which aligns with the correct answer. However, it introduces an additional factor, \"Necessity vs. Alternatives,\" which is not mentioned in the correct answer. This additional factor could lead to confusion as it diverges from the original focus on balancing the reduction in prompt leaks against the risk of degraded model performance. Therefore, the generated answer is not fully aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [04:07<06:42,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 474, Query:\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?\n",
      "__Correct answer__:\n",
      "Choosing the right Claude model that best fits your needs in terms of speed and output quality is one of the most straightforward ways to reduce latency in your application. Anthropic offers a range of Claude models with different capabilities and performance characteristics to allow you to choose the optimal balance of intelligence, speed, and cost for your use case.\n",
      "__Generated answer__:\n",
      "Selecting the appropriate Claude model based on your specific requirements can significantly reduce latency in your application. Each model offered by Anthropic has different capabilities and performance characteristics, which means that some models are optimized for speed while others may prioritize output quality or handle complex reasoning better.\n",
      "\n",
      "For instance, if your application requires quick responses and operates within a classification context, choosing a smaller model like Claude 3 Haiku can be ideal due to its speed and efficiency. On the other hand, if your tasks involve specialized knowledge or complex reasoning, you might opt for models like Sonnet or Opus, which may have higher latency but provide better performance for those specific tasks.\n",
      "\n",
      "By carefully evaluating your use case and selecting the model that best aligns with your needs, you can optimize both latency and output quality. This targeted selection helps ensure that your application runs efficiently, minimizing delays and improving overall user experience.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains that selecting the appropriate Claude model can reduce latency by aligning the model's capabilities with the application's specific requirements. It emphasizes the trade-off between speed and output quality, similar to the correct answer. Additionally, it provides examples of different models and their suitability for various tasks, which enhances the explanation. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 40/100 questions. Current Accuracy: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [04:18<07:41,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 453, Query:\n",
      "How can you stream responses from the Anthropic API using the Python SDK?\n",
      "__Correct answer__:\n",
      "You can stream responses from the Anthropic API using the Python SDK by using the client.messages.stream() method and iterating over the stream.text_stream attribute in a for loop.\n",
      "__Generated answer__:\n",
      "To stream responses from the Anthropic API using the Python SDK, you can use the following code snippet:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "# Initialize the Anthropic client\n",
      "client = anthropic.Anthropic()\n",
      "\n",
      "# Start streaming messages\n",
      "with client.messages.stream(\n",
      "    max_tokens=1024,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      ") as stream:\n",
      "    for text in stream.text_stream:\n",
      "        print(text, end=\"\", flush=True)\n",
      "```\n",
      "\n",
      "This code sets up a streaming session where you can receive model responses incrementally. Make sure to replace `\"Hello\"` with your desired input message and set the appropriate model name. Additionally, ensure that you have your API key set up either by exporting it as an environment variable `ANTHROPIC_API_KEY` or by supplying it directly when initializing the client.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to stream responses from the Anthropic API using the Python SDK by utilizing the client.messages.stream() method and iterating over the stream.text_stream attribute in a for loop. It also provides a code snippet that illustrates this process, which aligns with the correct answer's essence. Additionally, it includes important details about setting the API key and customizing the input message, which enhances the completeness of the response. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [04:22<06:27,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 397, Query:\n",
      "How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\n",
      "__Correct answer__:\n",
      "You can shape Claude's response by pre-filling part of it in the last position of the input messages list. To get a short response like a single multiple choice answer, you can set the \"max_tokens\" parameter to a small value like 1.\n",
      "__Generated answer__:\n",
      "You can guide Claude's response by pre-filling part of the response using either Text Completions or Messages. In Text Completions, you set the prompt to start with the assistant's response. In Messages, you achieve this by making the last input message have the assistant role, allowing the response to continue from that content.\n",
      "\n",
      "To generate a short response, you can use the API parameter `max_tokens`, which limits the number of tokens in the response, effectively controlling its length.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to guide Claude's response by pre-filling part of it using either Text Completions or Messages, which aligns with the correct answer's concept. It also mentions the use of the `max_tokens` parameter to generate a short response, which is consistent with the correct answer. Although the phrasing and details differ slightly, the essential information is present and accurate. Therefore, the generated answer is deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [04:26<05:35,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 412, Query:\n",
      "What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\n",
      "__Correct answer__:\n",
      "When building an eval set, it is better to prioritize having a larger volume of test cases with slightly lower signal automated grading over having fewer questions with high-quality human hand-grading.\n",
      "__Generated answer__:\n",
      "When building an eval set for an AI system, having a larger number of test cases with automated grading is generally considered more important than having fewer high-quality test cases graded by humans. The principles suggest prioritizing volume over quality, as more questions with slightly lower signal from automated grading can provide a better assessment of the system's performance across a wider range of scenarios, including edge cases. While human grading is high-quality and flexible, it is also slow and expensive, making it less desirable when scalability and speed are critical. Therefore, the focus should be on creating a larger set of automated evaluations that reflect real-world task distributions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the core idea of the correct answer, emphasizing the importance of having a larger number of test cases with automated grading over fewer high-quality human-graded cases. It discusses the benefits of volume and the challenges of human grading, which aligns with the correct answer's message. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [04:30<04:58,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 336, Query:\n",
      "What are the two required fields in a content_block_delta event for a text delta type?\n",
      "__Correct answer__:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\", where the \"delta\" field contains a \"type\" of \"text_delta\" and the \"text\" being added.\n",
      "__Generated answer__:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\". The \"delta\" field itself must contain a \"type\" (which should be \"text_delta\") and a \"text\" field that represents the updated text.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two required fields \"index\" and \"delta\" for a content_block_delta event of type text delta. It also accurately states that the \"delta\" field must contain a \"type\" of \"text_delta\" and a \"text\" field representing the updated text. Therefore, the substance of the answer aligns with the correct answer, and there are no critical pieces of information missing or contradictions present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [04:33<04:22,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 343, Query:\n",
      "What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\n",
      "__Correct answer__:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks demonstrating how to upload PDFs, generate embeddings, and more. The Developer Console offers a prompt generator tool for easier, more powerful prompting.\n",
      "__Generated answer__:\n",
      "1. **Anthropic Cookbook**: This resource includes interactive Jupyter notebooks that demonstrate how to upload PDFs and generate embeddings, providing a hands-on learning experience.\n",
      "\n",
      "2. **Developer Console**: Utilize the Workbench and prompt generator tool in your browser for easier and more powerful prompting, allowing you to experiment with Claude's capabilities interactively.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer regarding the Anthropic Cookbook and the Developer Console. Both answers mention the interactive Jupyter notebooks for the Cookbook and the prompt generator tool in the Developer Console, which are essential for learning how to use Claude's capabilities. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [04:42<05:29,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 533, Query:\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?\n",
      "__Correct answer__:\n",
      "Breaking a task into distinct subtasks for chained prompts improves Claude's accuracy because each subtask gets Claude's full attention, reducing errors compared to tackling the entire complex task at once.\n",
      "__Generated answer__:\n",
      "Breaking a task into distinct subtasks for chained prompts helps improve Claude's accuracy on the overall task for several reasons:\n",
      "\n",
      "1. **Focused Attention**: Each subtask allows Claude to concentrate fully on a specific aspect of the task, which reduces the likelihood of errors. By isolating each step, the model can process information more effectively.\n",
      "\n",
      "2. **Clarity of Instructions**: Simpler subtasks lead to clearer instructions and outputs. When tasks are broken down, it becomes easier to understand what is required at each stage, which enhances the quality of responses.\n",
      "\n",
      "3. **Traceability**: Chaining prompts makes it easier to identify and fix issues within the prompt chain. If an error occurs, it can be traced back to a specific subtask, allowing for targeted adjustments.\n",
      "\n",
      "4. **Reduction of Inconsistencies**: By breaking down complex tasks into smaller, consistent subtasks, the approach helps mitigate inconsistencies and errors that may arise in more complex workflows.\n",
      "\n",
      "5. **Structured Thinking**: Allowing Claude to think through problems step-by-step leads to more coherent and well-organized responses. This structured approach not only improves accuracy but also enhances the overall quality of the outputs.\n",
      "\n",
      "Overall, these factors contribute to a more effective and accurate completion of complex tasks when using chained prompts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how breaking a task into subtasks allows Claude to focus on specific aspects, leading to reduced errors and improved accuracy. It elaborates on several reasons, such as focused attention, clarity of instructions, traceability, reduction of inconsistencies, and structured thinking, all of which align with the idea presented in the correct answer. Therefore, the generated answer is correct as it conveys the same fundamental reasoning without omitting critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [04:48<05:19,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 445, Query:\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?\n",
      "__Correct answer__:\n",
      "Messages streaming responses can contain multiple content blocks of varying types, making the streaming format more complex compared to Text Completions which only include completion, ping, and error server-sent-events.\n",
      "__Generated answer__:\n",
      "The streaming format for Messages responses differs from Text Completions streaming responses in complexity and structure. \n",
      "\n",
      "For Text Completions, the streaming format includes events such as completion, ping, and error server-sent-events. The responses are more straightforward, focusing primarily on the completion of text.\n",
      "\n",
      "In contrast, the streaming format for Messages is more complex, as it can contain multiple content blocks of varying types. The raw HTTP stream response for Messages includes a series of events: a message_start event, multiple content blocks (each with a content_block_start, potentially multiple content_block_delta events, and a content_block_stop), a message_delta event, and a message_stop event. Additionally, ping events may be dispersed throughout the response.\n",
      "\n",
      "Overall, while Text Completions have a simpler structure, Messages involve a more intricate format due to the potential for multiple content types and blocks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences in complexity and structure between Messages streaming responses and Text Completions streaming responses. It correctly identifies that Text Completions have a simpler format focusing on completion, ping, and error events, while Messages can contain multiple content blocks and various event types. The additional details about the specific events in the Messages streaming format enhance the understanding without contradicting the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [04:51<04:26,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 329, Query:\n",
      "What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\n",
      "__Correct answer__:\n",
      "According to the documentation, users can start experimenting with Claude by visiting claude.ai or using Anthropic's web Console.\n",
      "__Generated answer__:\n",
      "Two ways to start experimenting with Claude as a user, according to Anthropic's documentation, are:\n",
      "\n",
      "1. Check out the quickstart guide for step-by-step instructions on how to create an account, obtain API keys, and start interacting with the models.\n",
      "2. Use the web Console at claude.ai to start experimenting with Claude right away.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes a method (using the web Console at claude.ai) that matches the correct answer. However, it also introduces a different method (checking out the quickstart guide) that is not mentioned in the correct answer. The correct answer specifically states that users can start experimenting by visiting claude.ai or using Anthropic's web Console, which does not include the quickstart guide. Therefore, the generated answer is incorrect as it adds information that is not aligned with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [04:56<04:14,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 463, Query:\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?\n",
      "__Correct answer__:\n",
      "Chain prompts break complex tasks into smaller subtasks, allowing Claude to give its full attention to each one. This reduces errors and inconsistencies that may occur when trying to handle a complex workflow all at once.\n",
      "__Generated answer__:\n",
      "Using chain prompts can significantly reduce errors and inconsistencies in complex tasks handled by Claude by breaking down these tasks into smaller, manageable subtasks. This approach allows Claude to focus on each individual step, which enhances accuracy and minimizes the likelihood of errors. \n",
      "\n",
      "Additionally, chain prompts improve clarity by simplifying instructions, making it easier for Claude to understand and execute the required tasks. This clarity also aids in traceability, as it becomes easier to identify and resolve issues within the prompt chain. If a specific step is performed poorly, it can be isolated and fine-tuned without the need to redo the entire task, further enhancing the efficiency of the process.\n",
      "\n",
      "Moreover, employing techniques such as specifying desired output formats, providing examples, and using retrieval for contextual consistency can help maintain consistency across the subtasks, thereby reducing the risk of hallucinations and other errors. Overall, chain prompts facilitate a structured approach to complex tasks, leading to more reliable and consistent outcomes.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how chain prompts break down complex tasks into smaller subtasks, allowing Claude to focus on each step, which reduces errors and inconsistencies. It also adds valuable information about clarity, traceability, and techniques to maintain consistency, which enhances the overall understanding of the benefits of using chain prompts. Since the generated answer aligns with the main points of the correct answer and expands upon them without contradicting any critical information, it is deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [04:59<03:34,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 285, Query:\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?\n",
      "__Correct answer__:\n",
      "In a non-streaming context, an overloaded_error event would normally correspond to an HTTP 529 status code.\n",
      "__Generated answer__:\n",
      "An overloaded_error event corresponds to an HTTP status code 529 in a non-streaming context for the Anthropic API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately states that an overloaded_error event corresponds to an HTTP status code 529 in a non-streaming context for the Anthropic API, which is consistent with the correct answer. There are no critical pieces of information missing or contradictions present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 50/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [05:03<03:26,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 348, Query:\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?\n",
      "__Correct answer__:\n",
      "When making a request to Voyage AI's embedding endpoint, you can either leave the encoding_format parameter unspecified to get the embeddings as lists of floating-point numbers, or set encoding_format to \"base64\" to get the embeddings compressed to Base64 encodings.\n",
      "__Generated answer__:\n",
      "The two ways to specify the format in which Voyage AI returns embeddings through its HTTP API are:\n",
      "\n",
      "1. By default, the embeddings are represented as lists of floating-point numbers.\n",
      "2. By setting the `encoding_format` parameter to `\"base64\"`, the embeddings are compressed to Base64 encodings.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two ways to specify the format for embeddings: by default as lists of floating-point numbers and by setting the `encoding_format` parameter to `\"base64\"`. It captures the essence of the correct answer without omitting any critical information or introducing contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [05:07<03:30,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 480, Query:\n",
      "When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\n",
      "__Correct answer__:\n",
      "When streaming requests with tool use, the input JSON deltas for tool_use content blocks are sent as partial JSON strings in multiple content_block_delta events. The client can accumulate these partial JSON strings and parse the complete JSON object once a content_block_stop event is received, using a library like Pydantic for partial JSON parsing or helpers provided in Anthropic's SDKs.\n",
      "__Generated answer__:\n",
      "The input JSON deltas for tool_use content blocks are sent as partial JSON strings through multiple `content_block_delta` events. Each delta corresponds to updates for the input field of the tool_use content block. The final `tool_use.input` is always an object, but during streaming, the deltas are emitted as partial JSON to allow for finer granularity.\n",
      "\n",
      "Clients can accumulate these string deltas until they receive a `content_block_stop` event. At that point, they can parse the accumulated JSON using libraries like Pydantic or through the provided SDKs, which offer helpers for accessing parsed incremental values. \n",
      "\n",
      "In summary, the process involves collecting the partial JSON strings from the deltas and parsing them into a complete JSON object once the streaming of the content block is complete.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the process of sending input JSON deltas for tool_use content blocks during streaming API requests. It mentions that these deltas are sent as partial JSON strings through multiple `content_block_delta` events and that clients can accumulate these until they receive a `content_block_stop` event, at which point they can parse the accumulated JSON. It also correctly references the use of libraries like Pydantic and helpers in the SDKs for parsing. Therefore, the generated answer captures all critical information present in the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [05:11<03:18,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 408, Query:\n",
      "What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\n",
      "__Correct answer__:\n",
      "Anthropic offers a GitHub prompting tutorial that covers prompt engineering concepts in-depth with examples, and a lighter-weight Google Sheets prompting tutorial that utilizes Claude for Sheets.\n",
      "__Generated answer__:\n",
      "The two interactive prompt engineering tutorials offered by Anthropic are:\n",
      "\n",
      "1. **GitHub Prompting Tutorial**: This is an example-filled tutorial that covers the prompt engineering concepts found in Anthropic's documentation. It is designed for users who prefer a more comprehensive and detailed approach to learning about prompt engineering.\n",
      "\n",
      "2. **Google Sheets Prompting Tutorial**: This is a lighter-weight version of the prompt engineering tutorial, presented in an interactive spreadsheet format. It allows users to engage with the material in a more simplified manner.\n",
      "\n",
      "The main difference between the two tutorials lies in their format and depth; the GitHub tutorial is more extensive and example-driven, while the Google Sheets tutorial offers a more accessible and streamlined experience.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. It identifies both tutorials, describes their formats, and highlights the differences in depth and accessibility. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [05:23<04:51,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 666, Query:\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?\n",
      "__Correct answer__:\n",
      "Claude offers a 200K token context window, tool use for integration into specialized applications, multimodal input capabilities for richer context, and is uniquely positioned to serve high-trust industries processing large volumes of sensitive data with enterprise-grade security and data handling.\n",
      "__Generated answer__:\n",
      "Claude is suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data due to several key capabilities:\n",
      "\n",
      "1. **Enterprise-Grade Security**: Claude offers SOC II Type 2 certification and HIPAA compliance options, ensuring robust security and data handling suitable for high-trust industries that manage sensitive user data.\n",
      "\n",
      "2. **Trustworthiness**: The model is designed to be resistant to jailbreaks and misuse, with continuous monitoring for harmful use cases, which is critical for enterprises concerned about data integrity and compliance.\n",
      "\n",
      "3. **Scalability**: Claude is built to handle large volumes of data, featuring a 200K token context window, with future support for up to 1 million tokens, allowing for the processing of extensive documents and complex data sets.\n",
      "\n",
      "4. **Multimodal Input Capabilities**: It can process and analyze visual inputs alongside text, enabling richer context and more complex use cases, such as extracting insights from images or generating code from diagrams.\n",
      "\n",
      "5. **Tool Use and Integration**: Claude supports function calling, allowing seamless integration into specialized applications and custom workflows, which is essential for enterprises looking to automate processes and enhance productivity.\n",
      "\n",
      "6. **Developer Tools**: The Developer Console, Workbench, and prompt generation tools facilitate easier and more powerful experimentation and integration, making it accessible for developers to build and deploy applications effectively.\n",
      "\n",
      "7. **Low Hallucination Rates**: Claude maintains very low rates of hallucination, ensuring high accuracy and reliability, particularly important for enterprises relying on precise outputs for decision-making.\n",
      "\n",
      "8. **Global Language Support**: It demonstrates fluency in multiple languages, enabling enterprises to operate in diverse markets and support global communication needs.\n",
      "\n",
      "These capabilities collectively position Claude as a powerful tool for enterprises that require secure, efficient, and scalable solutions for processing sensitive data and integrating with specialized applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the key capabilities of Claude that make it suitable for enterprise use cases, including enterprise-grade security, scalability with a 200K token context window, multimodal input capabilities, tool use for integration, and low hallucination rates. It also mentions trustworthiness and global language support, which are relevant but not explicitly stated in the Correct Answer. However, it does not mention the specific aspect of being uniquely positioned for high-trust industries, which is a critical point in the Correct Answer. Therefore, while the Generated Answer is comprehensive, it lacks this specific critical piece of information, leading to a conclusion of incorrectness.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [05:25<03:49,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 292, Query:\n",
      "As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\n",
      "__Correct answer__:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in the United States, Canada, and Europe.\n",
      "__Generated answer__:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in Canada and Europe.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is missing the information that the Claude.ai API and iOS app are also available in the United States, which is a critical piece of information compared to the correct answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [05:30<03:46,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 513, Query:\n",
      "What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\n",
      "__Correct answer__:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are push-based using webhooks, and pull-based. The push-based approach is more web-scalable but requires exposing a public endpoint which has IT security implications. The pull-based approach is easier to implement but makes unnecessary calls to the support ticket system.\n",
      "__Generated answer__:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are the push-based approach and the pull-based approach.\n",
      "\n",
      "1. **Push-Based Approach**: In this method, the support ticket system (e.g., Zendesk) triggers the integration by sending a webhook event to the routing service whenever a new ticket is created. This approach is more scalable because it allows for real-time processing of tickets as they are created, minimizing delays in routing. However, it requires exposing a public endpoint, which may raise IT security concerns.\n",
      "\n",
      "2. **Pull-Based Approach**: This method involves the integration code periodically checking for new tickets at scheduled intervals. While this approach is easier to implement since it does not require setting up a public endpoint, it can lead to unnecessary calls to the support ticket system, which may not be as efficient or scalable as the push-based approach.\n",
      "\n",
      "In summary, the push-based approach offers better scalability but comes with security implications, while the pull-based approach is simpler to implement but less efficient in terms of resource usage.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the two main approaches for integrating Claude into a support ticket workflow, including the push-based and pull-based methods. It correctly highlights the scalability and ease of implementation aspects of each approach, as well as the security implications of the push-based method and the inefficiencies of the pull-based method. Therefore, it conveys the same essential information as the Correct Answer without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [05:33<03:16,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 293, Query:\n",
      "When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\n",
      "__Correct answer__:\n",
      "On May 10th, 2024, Anthropic released a prompt generator tool that is available through the Developer Console.\n",
      "__Generated answer__:\n",
      "Anthropic released the prompt generator tool on May 10th, 2024. It is available through the Developer Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating both the release date of the prompt generator tool and the interface through which it is available. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [05:38<03:17,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 372, Query:\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?\n",
      "__Correct answer__:\n",
      "The Claude 3 Sonnet model balances intelligence and speed, making it well-suited for high-throughput tasks like sales forecasting and targeted marketing.\n",
      "__Generated answer__:\n",
      "The Claude 3 model that provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing is the **claude-3-haiku-20240307** model. It is noted for being the fastest and most cost-effective model in the Claude 3 family, making it ideal for such use cases. However, if deep subject matter expertise or highly complex reasoning is required, the larger **Claude 3 Sonnet** model may be a better choice despite its higher cost.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly identifies the **claude-3-haiku-20240307** model as the best balance of intelligence and speed for high-throughput tasks, while the correct answer states that the **Claude 3 Sonnet** model is the one that provides this balance. This is a critical piece of information that is missing in the generated answer, leading to a direct contradiction with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [05:42<03:06,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 413, Query:\n",
      "How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\n",
      "__Correct answer__:\n",
      "You can calculate the similarity between two Voyage embedding vectors using the dot product, which is equivalent to cosine similarity since Voyage embeddings are normalized to length 1.\n",
      "__Generated answer__:\n",
      "To calculate the similarity between two Voyage embedding vectors, you can use cosine similarity, which is a common method for this purpose. Since Voyage embeddings are normalized to length 1, cosine similarity is equivalent to the dot product of the two vectors. You can compute this in Python using the following code snippet:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "similarity = np.dot(embd1, embd2)\n",
      "```\n",
      "\n",
      "This will give you the cosine similarity between the two embedding vectors, which ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates orthogonality (no similarity), and -1 indicates that they are diametrically opposed.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can calculate the similarity between two Voyage embedding vectors using cosine similarity, which is equivalent to the dot product since the embeddings are normalized to length 1. It also provides a Python code snippet for calculating the dot product, which is relevant and helpful. Therefore, the generated answer captures the essential information from the correct answer without omitting any critical details or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [05:52<03:55,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 587, Query:\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?\n",
      "__Correct answer__:\n",
      "Well-chosen examples in prompts can boost Claude's ability to handle complex tasks by reducing misinterpretation of instructions, enforcing consistent structure and style, and serving as a guide for the desired output.\n",
      "__Generated answer__:\n",
      "Using examples in prompts can significantly enhance Claude's performance on complex tasks in several ways:\n",
      "\n",
      "1. **Reducing Misinterpretation**: Examples clarify instructions, minimizing the chances of Claude misinterpreting what is being asked. This leads to more accurate responses.\n",
      "\n",
      "2. **Enforcing Consistency**: By providing a uniform structure and style through examples, Claude can maintain a consistent approach in its outputs, which is crucial for tasks that require a specific format or tone.\n",
      "\n",
      "3. **Boosting Performance**: Well-chosen examples can enhance Claude's ability to handle complex tasks by providing context and demonstrating the expected outcome. This is particularly effective for intricate scenarios where the correct response may not be immediately obvious.\n",
      "\n",
      "4. **Addressing Edge Cases**: Including examples of edge cases in prompts helps Claude navigate situations it might misclassify or misunderstand, such as implicit requests or emotional prioritization. This guidance can improve its ability to identify and respond to customer needs accurately.\n",
      "\n",
      "5. **Clarifying Intent and Prioritization**: Examples can help Claude differentiate between similar intents and prioritize issues correctly, especially when multiple concerns are presented in a single interaction.\n",
      "\n",
      "6. **Iterative Improvement**: Regularly refining prompts based on performance metrics and feedback allows for the continuous enhancement of Claude's capabilities. Targeted examples can be retrieved using techniques like vector databases, which can significantly improve accuracy, as evidenced by case studies showing performance improvements from 71% to 93%.\n",
      "\n",
      "Overall, incorporating examples into prompts provides Claude with the necessary context and guidance to navigate complex tasks more effectively, leading to improved accuracy and efficiency.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the main points from the correct answer regarding how examples in prompts can improve Claude's performance on complex tasks. It discusses reducing misinterpretation, enforcing consistency, and providing guidance for desired outputs, which aligns with the correct answer's emphasis on these aspects. Additionally, the generated answer expands on the topic by including points about addressing edge cases and iterative improvement, which are relevant and enhance the understanding of the subject. Therefore, the generated answer is correct as it captures the essence of the correct answer without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 60/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [05:58<03:56,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 420, Query:\n",
      "What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\n",
      "__Correct answer__:\n",
      "When streaming responses with tool use, the two types of content block deltas are text deltas and input JSON deltas. Text deltas contain a \"text\" field with a string of the incrementally generated text. Input JSON deltas contain a \"partial_json\" field with a string containing part of the JSON object specifying the tool's input.\n",
      "__Generated answer__:\n",
      "The two types of content block deltas that can be emitted when streaming responses with tool use are:\n",
      "\n",
      "1. **Insertion Delta**: This type of delta indicates that new content is being added to the content block at a specific index.\n",
      "\n",
      "2. **Deletion Delta**: This type of delta indicates that existing content is being removed from the content block at a specific index.\n",
      "\n",
      "Each delta type contains information about how the content block is being modified, such as the specific index where the change occurs and the content being inserted or deleted.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it describes two types of deltas (Insertion Delta and Deletion Delta) that are not mentioned in the Correct Answer. The Correct Answer specifies the two types of content block deltas as text deltas and input JSON deltas, along with their specific contents. The Generated Answer fails to address the correct types and their respective fields, which are critical pieces of information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [06:02<03:27,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 385, Query:\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?\n",
      "__Correct answer__:\n",
      "Claude's question answering and text analysis capabilities enable it to build intelligent, interactive systems like chatbots and personalize user experiences by understanding sentiment and preferences.\n",
      "__Generated answer__:\n",
      "Two key capabilities of Claude that enable it to build interactive systems and personalized user experiences are:\n",
      "\n",
      "1. **Text and Code Generation**: Claude can summarize text, answer questions, extract data, translate text, and generate code. This capability allows for the creation of intelligent and interactive systems, such as customer support chatbots and educational AI tutors, which can engage users effectively.\n",
      "\n",
      "2. **Text Analysis and Recommendations**: Claude can understand sentiment, preferences, and patterns in user interactions, which helps in personalizing user experiences and offerings. This capability is crucial for tailoring interactions and content to meet individual user needs.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two key capabilities of Claude: text and code generation, and text analysis and recommendations. These capabilities align with the correct answer's mention of question answering and text analysis, as both involve understanding and processing text to create interactive systems and personalize user experiences. Therefore, the generated answer captures the essence of the correct answer without missing any critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [06:08<03:30,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 514, Query:\n",
      "What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\n",
      "__Correct answer__:\n",
      "A raw HTTP stream response includes a message_start event, followed by one or more content blocks (each with a content_block_start, content_block_delta events, and content_block_stop), a message_delta event, and a final message_stop event. Ping events may also be dispersed throughout.\n",
      "__Generated answer__:\n",
      "The key event types included in a raw HTTP stream response when using message streaming are:\n",
      "\n",
      "1. **message_start**: This event indicates the start of a message and contains a Message object with empty content.\n",
      "2. **content_block_start**: This event marks the beginning of a content block.\n",
      "3. **content_block_delta**: There can be multiple of these events within a content block, indicating incremental updates to the content.\n",
      "4. **content_block_stop**: This event signifies the end of a content block.\n",
      "5. **message_delta**: This event indicates top-level changes to the final Message object.\n",
      "6. **message_stop**: This event marks the end of the message.\n",
      "\n",
      "Additionally, there may be **ping events** dispersed throughout the response. The typical order of these events is:\n",
      "\n",
      "1. message_start\n",
      "2. content_block_start\n",
      "3. (multiple content_block_delta events)\n",
      "4. content_block_stop\n",
      "5. (one or more message_delta events)\n",
      "6. message_stop\n",
      "\n",
      "This sequence outlines the flow of events in a streaming response.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the key event types included in a raw HTTP stream response and their typical order, matching the Correct Answer in substance. It includes all the necessary events: message_start, content_block_start, content_block_delta, content_block_stop, message_delta, and message_stop, along with the mention of ping events. The order of events is also correctly outlined. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [06:12<03:01,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 314, Query:\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?\n",
      "__Correct answer__:\n",
      "The Messages API allows including up to 20 images per request, while the claude.ai interface has a lower limit of up to 5 images per turn.\n",
      "__Generated answer__:\n",
      "The maximum number of images that can be included in a single request using the Anthropic API is 20, while the claude.ai interface allows for up to 5 images per turn.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating that the Anthropic API allows for up to 20 images per request and the claude.ai interface allows for up to 5 images per turn. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [06:15<02:36,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 329, Query:\n",
      "When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\n",
      "__Correct answer__:\n",
      "If Claude's response hits the max_tokens limit and has an incomplete tool use block, you should retry the request with a higher max_tokens value to get Claude's full response including the complete tool use.\n",
      "__Generated answer__:\n",
      "If Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, you should retry the request with a higher max_tokens value to obtain the full tool use.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately conveys the same essential information as the correct answer, stating that if Claude's response is cut off due to the max_tokens limit, one should retry the request with a higher max_tokens value to obtain the full tool use. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [06:17<02:11,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 298, Query:\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?\n",
      "__Correct answer__:\n",
      "Before running a classification evaluation on Claude, you need to 1) develop your test cases, and 2) take a look at Anthropic's guide to developing test cases.\n",
      "__Generated answer__:\n",
      "Before running a classification evaluation on Claude, the two steps needed are:\n",
      "\n",
      "1. Build a strong input prompt.\n",
      "2. Develop your test cases.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it omits the critical step of taking a look at Anthropic's guide to developing test cases, which is explicitly mentioned in the correct answer. While it includes the step of developing test cases, it introduces a new step of building a strong input prompt that is not part of the correct answer. Therefore, the generated answer does not fully align with the requirements outlined in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [06:24<02:38,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 432, Query:\n",
      "How can you use the content parameter in the messages list to influence Claude's response?\n",
      "__Correct answer__:\n",
      "You can provide content in the last position of the messages list, with the \"assistant\" role, to pre-fill part of Claude's response. This allows you to shape the assistant's output.\n",
      "__Generated answer__:\n",
      "You can use the content parameter in the messages list to influence Claude's response by setting the last message in the conversation to have the assistant role. This allows you to pre-fill part of Claude's response, effectively guiding the direction of the conversation. For example, if you want Claude to continue from a specific statement, you can structure the messages like this:\n",
      "\n",
      "```python\n",
      "messages = [\n",
      "  {\"role\": \"human\", \"content\": \"Hello\"},\n",
      "  {\"role\": \"assistant\", \"content\": \"Hello, my name is\"},\n",
      "]\n",
      "```\n",
      "\n",
      "In this case, Claude will continue the response from \"Hello, my name is,\" allowing you to control the context and content of the reply. This method is particularly useful for simulating conversations or ensuring that Claude's responses align with your desired narrative.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes how to use the content parameter in the messages list to influence Claude's response. It correctly states that setting the last message with the \"assistant\" role allows for pre-filling part of Claude's response, which aligns with the Correct Answer. Both answers convey the same essential information about shaping the assistant's output and provide an example that illustrates the concept. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [06:29<02:29,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 418, Query:\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?\n",
      "__Correct answer__:\n",
      "Compared to fine-tuning, prompt engineering is far more effective at helping models understand and utilize external content like retrieved documents. Prompt engineering also preserves the model's broad general knowledge, while fine-tuning risks catastrophic forgetting where the model loses its general capabilities.\n",
      "__Generated answer__:\n",
      "Two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation are:\n",
      "\n",
      "1. **Comprehension Improvements**: Prompt engineering is more effective at helping models better understand and utilize external content, such as retrieved documents. This allows for enhanced comprehension of the task at hand without the need for extensive retraining.\n",
      "\n",
      "2. **Preservation of General Knowledge**: Fine-tuning can lead to catastrophic forgetting, where the model loses its broad general knowledge due to the focus on specific tasks. In contrast, prompt engineering maintains the model's wide-ranging capabilities, ensuring that general knowledge is preserved while adapting to new tasks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key advantages of prompt engineering over fine-tuning as stated in the correct answer. It discusses both the improved comprehension of external content and the preservation of general knowledge, while also mentioning the risk of catastrophic forgetting associated with fine-tuning. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [06:32<02:07,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 342, Query:\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?\n",
      "__Correct answer__:\n",
      "To get started making requests to Claude models on Anthropic's Bedrock API, you need to: 1) Install and configure the AWS CLI, and 2) Install an SDK for accessing Bedrock, such as the Python SDK shown in the example code.\n",
      "__Generated answer__:\n",
      "The two main steps to get started with making requests to Claude models on Anthropic's Bedrock API are:\n",
      "\n",
      "1. Follow the Quickstart guide to make your first API call.\n",
      "2. Explore the API Reference for detailed information on using the API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not accurately reflect the two main steps outlined in the correct answer. While it mentions following a Quickstart guide and exploring the API Reference, it omits the crucial steps of installing and configuring the AWS CLI and installing an SDK for accessing Bedrock. Therefore, it lacks critical information necessary to get started with making requests to Claude models on Anthropic's Bedrock API.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 70/100 [06:35<01:59,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 395, Query:\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?\n",
      "__Correct answer__:\n",
      "You can list the available Claude models in a specific AWS region by running the command `aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`, replacing `<region>` with the desired AWS region such as `us-west-2`.\n",
      "__Generated answer__:\n",
      "To check which Claude models are available in a specific AWS region using the AWS CLI, you can use the following command:\n",
      "\n",
      "```bash\n",
      "aws bedrock list-foundation-models --region=<your-region> --by-provider=anthropic --query \"modelSummaries[*].modelId\"\n",
      "```\n",
      "\n",
      "Replace `<your-region>` with the desired AWS region, such as `us-west-2`. This command will list all the available Claude models provided by Anthropic in that region.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is correct as it conveys the same essential information as the correct answer. Both answers provide the same AWS CLI command to list available Claude models in a specific AWS region, with only minor differences in wording. The instruction to replace the placeholder with the desired region is also present in both answers. Therefore, there are no critical pieces of information missing or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 70/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [06:39<01:51,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 337, Query:\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?\n",
      "__Correct answer__:\n",
      "The input_type argument can be passed with a value of \"query\" or \"document\" to specify the type of input text being embedded.\n",
      "__Generated answer__:\n",
      "The argument that can be passed to the `voyageai.Client.embed()` method or the Voyage HTTP API to specify whether the input text is a query or a document is `input_type`. This argument can take the values `\"query\"` or `\"document\"`. If not specified, it defaults to `None`.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the argument `input_type` and its possible values of `\"query\"` or `\"document\"`, which aligns with the correct answer. Additionally, it includes the detail about the default value being `None`, which, while not mentioned in the correct answer, does not contradict it. Therefore, the generated answer is essentially correct and provides complete information regarding the argument.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [06:49<02:38,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 588, Query:\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?\n",
      "__Correct answer__:\n",
      "Tool_use content block deltas contain partial JSON strings for the input field, whereas text content block deltas directly contain the text delta. Tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time.\n",
      "__Generated answer__:\n",
      "The streaming API delta formats for tool_use content blocks and text content blocks differ primarily in their structure and purpose.\n",
      "\n",
      "1. **Tool_use Content Blocks**: The deltas for tool_use content blocks correspond to updates for the input field of the block. These deltas are partial JSON strings, allowing for maximum granularity. The final output for a tool_use content block is always an object. The streaming format for tool_use includes multiple `content_block_delta` events, each containing a `partial_json` that represents incremental updates to the input. For example, a delta might look like this:\n",
      "   ```json\n",
      "   {\n",
      "       \"type\": \"content_block_delta\",\n",
      "       \"index\": 1,\n",
      "       \"delta\": {\n",
      "           \"type\": \"input_json_delta\",\n",
      "           \"partial_json\": \"{\\\"location\\\": \\\"San Fra\\\"}\"\n",
      "       }\n",
      "   }\n",
      "   ```\n",
      "   Clients can accumulate these string deltas and parse the JSON once they receive a `content_block_stop` event.\n",
      "\n",
      "2. **Text Content Blocks**: In contrast, the streaming format for text completions includes events such as completion, ping, and error server-sent-events. The response can contain multiple content blocks of varying types, making the streaming format more complex. The text completions do not utilize partial JSON strings in the same way as tool_use content blocks.\n",
      "\n",
      "In summary, tool_use content blocks focus on partial JSON updates for input fields, while text content blocks involve a broader range of events and do not use partial JSON for their deltas.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences between tool_use content blocks and text content blocks in terms of their delta formats. It mentions that tool_use content blocks contain partial JSON strings and that the final output is an object, which aligns with the correct answer. It also correctly states that text content blocks do not utilize partial JSON strings in the same way. However, it does not explicitly mention that tool_use deltas may have delays between streaming events, which is a critical piece of information from the correct answer. Therefore, while the generated answer is mostly correct, it lacks this important detail, making it incomplete.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [06:52<02:10,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 326, Query:\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?\n",
      "__Correct answer__:\n",
      "When uploading images to Claude, the API has a maximum file size limit of 5MB per image, while on claude.ai the limit is 10MB per image.\n",
      "__Generated answer__:\n",
      "The image file size limits for uploading images to Claude are as follows:\n",
      "\n",
      "- When using the API: Maximum 5MB per image.\n",
      "- When using claude.ai: Maximum 10MB per image.\n",
      "\n",
      "Images larger than these limits will be rejected and return an error.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the image file size limits for uploading images to Claude via the API and on claude.ai, stating that the API has a maximum of 5MB per image and claude.ai has a maximum of 10MB per image. It also includes the information that images larger than these limits will be rejected and return an error, which adds clarity. Therefore, the Generated Answer is correct and aligns with the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [06:55<01:53,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 333, Query:\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?\n",
      "__Correct answer__:\n",
      "When selecting a Claude model for an enterprise use case that requires low latency, it's important to choose the model that best balances speed and output quality based on the specific requirements of the use case.\n",
      "__Generated answer__:\n",
      "One key consideration when selecting a Claude model for an enterprise use case that needs low latency is to choose a smaller model, such as Claude 3 Haiku, which is typically ideal due to its speed and efficiency. This allows for quicker responses, which is crucial in low-latency scenarios.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a specific example of a model (Claude 3 Haiku) that is suitable for low latency, which aligns with the correct answer's emphasis on balancing speed and output quality. However, it lacks the broader context of considering the specific requirements of the use case, which is a critical piece of information mentioned in the correct answer. Therefore, while the generated answer is partially correct, it does not fully encompass the key consideration outlined in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [06:57<01:35,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 334, Query:\n",
      "What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\n",
      "__Correct answer__:\n",
      "For code retrieval, Voyage AI recommends using the voyage-code-2 embedding model, which they claim performs 17% better than alternatives and achieves state-of-the-art results on general-purpose corpora as well.\n",
      "__Generated answer__:\n",
      "Anthropic recommends using the \"voyage-code-2\" embedding model for code retrieval. According to Voyage AI, this model is optimized for code retrieval and performs 17% better than alternatives, while also achieving state-of-the-art performance on general-purpose corpora.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer, stating that the \"voyage-code-2\" embedding model is recommended for code retrieval and that it performs 17% better than alternatives while achieving state-of-the-art results on general-purpose corpora. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [07:02<01:34,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 355, Query:\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?\n",
      "__Correct answer__:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks that demonstrate how to upload PDFs and work with embeddings to help developers learn to use Anthropic's APIs.\n",
      "__Generated answer__:\n",
      "The Anthropic Cookbook can help developers learn to use Anthropic's APIs in two ways:\n",
      "\n",
      "1. It provides interactive Jupyter notebooks that demonstrate various functionalities, such as uploading PDFs and working with embeddings, allowing developers to see practical examples and engage with the API in a hands-on manner.\n",
      "\n",
      "2. The Cookbook serves as a resource for exploring different use cases and techniques, helping developers understand how to effectively implement and optimize their applications using the Anthropic API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies that the Anthropic Cookbook provides interactive Jupyter notebooks for demonstrating functionalities like uploading PDFs and working with embeddings. Additionally, it mentions that the Cookbook serves as a resource for exploring different use cases and techniques, which aligns with the goal of helping developers understand how to implement and optimize their applications using the Anthropic API. Therefore, the Generated Answer captures the essence of the Correct Answer and includes additional relevant information without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [07:08<01:45,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 473, Query:\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?\n",
      "__Correct answer__:\n",
      "The size of the context window determines how much retrieved information can be passed to the language model to augment its knowledge when generating a response using RAG. A larger context window allows more relevant retrieved information to be utilized by the model, improving the accuracy and groundedness of the generated text.\n",
      "__Generated answer__:\n",
      "The size of the context window significantly impacts a language model's ability to utilize retrieval augmented generation (RAG). A larger context window allows the model to incorporate more retrieved information from an external knowledge base along with the original query. This means that the model can reference a greater amount of relevant data when generating responses, leading to improved accuracy and coherence in the output.\n",
      "\n",
      "Conversely, a smaller context window may limit the amount of retrieved information that can be effectively utilized, potentially reducing the model's ability to generate relevant and contextually grounded responses. This limitation can hinder the model's performance, especially in tasks that require complex reasoning or the integration of multiple pieces of information.\n",
      "\n",
      "In summary, a larger context window enhances the effectiveness of RAG by allowing the model to process and integrate more information, while a smaller context window restricts this capability, impacting the overall quality and relevance of the generated text.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer regarding the impact of context window size on retrieval augmented generation (RAG). It explains that a larger context window allows for more retrieved information to be incorporated, which enhances the accuracy and coherence of the model's responses. Additionally, it correctly notes that a smaller context window can limit the effectiveness of RAG, potentially hindering performance. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [07:17<02:11,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 525, Query:\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?\n",
      "__Correct answer__:\n",
      "The Evaluation tool helps identify edge cases where prompts might falter, allows rating individual results to determine prompt performance, ensures consistent performance across inputs, and enables prompt refinement for better reliability. Reviewing results across test cases helps spot patterns to make informed adjustments that lead to more robust AI applications.\n",
      "__Generated answer__:\n",
      "The Evaluation tool in Anthropic's Claude platform can significantly enhance prompt development and the robustness of AI applications in several ways:\n",
      "\n",
      "1. **Identifying Edge Cases**: The tool helps users pinpoint specific scenarios where prompts may not perform as expected, allowing for targeted improvements.\n",
      "\n",
      "2. **Rating Results**: Users can evaluate individual outputs from the model, determining where prompts yield better or worse performance. This feedback loop is crucial for understanding the effectiveness of different prompt formulations.\n",
      "\n",
      "3. **Ensuring Consistency**: By testing prompts across a variety of inputs, the Evaluation tool helps ensure that the model maintains consistent performance, which is essential for reliable AI applications.\n",
      "\n",
      "4. **Refining Prompts**: The tool facilitates the refinement of prompts, enabling users to make informed adjustments based on the patterns observed in the results. This iterative process is key to enhancing the reliability of the prompts.\n",
      "\n",
      "5. **Pattern Recognition**: Reviewing results across multiple test cases allows users to spot trends and make data-driven decisions to optimize their prompts further.\n",
      "\n",
      "Overall, the Evaluation tool supports a systematic approach to prompt engineering, leading to more robust AI applications by enabling continuous improvement based on empirical evidence.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the key points from the Correct Answer regarding how the Evaluation tool in Anthropic's Claude platform aids in improving prompts and building robust AI applications. It mentions identifying edge cases, rating results, ensuring consistency, refining prompts, and recognizing patterns, all of which align with the Correct Answer's content. Therefore, the Generated Answer is correct as it conveys the same essential information without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [07:19<01:42,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 276, Query:\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?\n",
      "__Correct answer__:\n",
      "The Claude 3 Haiku model has the fastest comparative latency\n",
      "__Generated answer__:\n",
      "The Claude model with the fastest comparative latency is the Claude 3 Haiku, which is described as the \"Fastest and most compact model for near-instant responsiveness.\"\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the Claude 3 Haiku model as having the fastest comparative latency, which aligns with the correct answer. Additionally, it provides a description that emphasizes its speed and compactness, which does not contradict the original statement. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [07:27<01:53,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 718, Query:\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?\n",
      "__Correct answer__:\n",
      "To have a multi-turn conversation using the Anthropic Messages API in Python, send the full conversation history in the messages parameter each time, including any prior user and assistant messages. The API is stateless, so the entire context must be provided with each request.\n",
      "__Generated answer__:\n",
      "To build up a conversation with multiple turns using the Anthropic Messages API in Python, you need to maintain the entire conversational history and send it with each request. Here's how you can do it:\n",
      "\n",
      "1. **Initialize the Anthropic client** with your API key.\n",
      "2. **Create a list to hold the messages** that represent the conversation.\n",
      "3. **Add user and assistant messages** to this list as the conversation progresses.\n",
      "4. **Send the full message history** to the API with each request.\n",
      "\n",
      "Here’s an example implementation:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "# Initialize the Anthropic client\n",
      "client = anthropic.Anthropic(api_key=\"your_api_key\")\n",
      "\n",
      "# Initialize the conversation history\n",
      "conversation_history = []\n",
      "\n",
      "# Function to add a message to the conversation and get a response\n",
      "def chat_with_claude(user_message):\n",
      "    # Add the user's message to the conversation history\n",
      "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
      "\n",
      "    # Send the full conversation history to the API\n",
      "    response = client.messages.create(\n",
      "        model=\"claude-3-5-sonnet-20240620\",\n",
      "        max_tokens=1024,\n",
      "        messages=conversation_history\n",
      "    )\n",
      "\n",
      "    # Get the assistant's response\n",
      "    assistant_message = response.content\n",
      "    # Add the assistant's message to the conversation history\n",
      "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
      "\n",
      "    return assistant_message\n",
      "\n",
      "# Example conversation\n",
      "print(chat_with_claude(\"Hello, Claude!\"))\n",
      "print(chat_with_claude(\"Can you describe LLMs to me?\"))\n",
      "print(chat_with_claude(\"What are some applications of LLMs?\"))\n",
      "```\n",
      "\n",
      "In this example:\n",
      "- The `conversation_history` list stores all messages exchanged between the user and the assistant.\n",
      "- Each time the user sends a message, it is appended to the history, and the entire history is sent to the API to get a response.\n",
      "- The assistant's response is also appended to the history, allowing for a continuous conversation. \n",
      "\n",
      "This approach ensures that the context of the conversation is preserved across multiple turns.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the process of maintaining the entire conversation history and sending it with each request to the Anthropic Messages API, which aligns with the Correct Answer. It includes the necessary steps and provides an example implementation that illustrates how to build up a conversation with multiple turns. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 80/100 questions. Current Accuracy: 0.7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [07:36<02:09,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 622, Query:\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?\n",
      "__Correct answer__:\n",
      "Providing Claude with a specific role, such as being the General Counsel of a company, using XML tags can help it catch critical legal issues and risks in a contract that it might miss without the role context, potentially saving the company millions of dollars.\n",
      "__Generated answer__:\n",
      "Using XML tags to provide a specific role or context can significantly enhance Claude's analysis of a legal contract compared to not using a role prompt. Here are the key benefits:\n",
      "\n",
      "1. **Clarity and Structure**: XML tags help clearly separate different parts of the input, making it easier for Claude to understand the structure of the prompt. This clarity can lead to more accurate interpretations of the legal contract's components.\n",
      "\n",
      "2. **Enhanced Accuracy**: By defining a specific role (e.g., legal analyst, contract lawyer), Claude can tailor its analysis to the nuances of legal language and context. This role prompting improves accuracy, especially in complex scenarios like legal analysis, where precise interpretation is crucial.\n",
      "\n",
      "3. **Improved Focus**: Setting a role context helps Claude stay focused on the specific requirements of the task. For instance, if Claude is prompted as a legal expert, it will prioritize legal implications and relevant clauses, reducing the likelihood of irrelevant information being included in the analysis.\n",
      "\n",
      "4. **Flexibility and Modifyability**: XML tags allow for easy modification of the prompt structure without needing to rewrite everything. This flexibility can be particularly useful when iterating on the analysis or adjusting the focus based on feedback.\n",
      "\n",
      "5. **Parseability**: When Claude outputs its analysis, using XML tags makes it easier to extract specific parts of the response through post-processing. This can be beneficial for users who need to pull out particular sections of the contract analysis for further review or action.\n",
      "\n",
      "Overall, incorporating XML tags and role prompts leads to a more organized, accurate, and contextually relevant analysis of legal contracts, enhancing the overall effectiveness of Claude's capabilities.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly elaborates on the benefits of using XML tags and a specific role in enhancing Claude's analysis of legal contracts. It emphasizes clarity, accuracy, focus, flexibility, and parseability, which align with the essence of the Correct Answer. While the Correct Answer specifically mentions the potential financial savings for the company, the Generated Answer does not include this detail. However, the overall substance and reasoning presented in the Generated Answer are consistent with the Correct Answer's main points. Therefore, it can be considered correct despite the omission of the financial impact detail.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [07:42<01:55,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 459, Query:\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?\n",
      "__Correct answer__:\n",
      "When required parameters are missing, Claude 3 Opus is more likely to ask the user for the missing information, while Claude 3 Sonnet is more likely to try to infer reasonable values on its own to proceed with the tool call.\n",
      "__Generated answer__:\n",
      "Claude 3 Opus and Claude 3 Sonnet handle missing information differently when making tool calls. \n",
      "\n",
      "Claude 3 Opus is designed to manage complex tools and ambiguous queries effectively. It has the capability to handle multiple tools simultaneously and actively seeks clarification when it encounters missing information. This makes it particularly adept at navigating situations where parameters may be unclear or incomplete.\n",
      "\n",
      "In contrast, Claude 3 Sonnet is more suited for tasks that require a balance of intelligence and speed but may not handle ambiguity as robustly as Opus. While it can infer missing parameters, it does not explicitly seek clarification like Opus does. This means that Sonnet might make assumptions about missing information rather than prompting for more details.\n",
      "\n",
      "In summary, Opus is proactive in seeking clarification for missing information, while Sonnet may infer or assume missing parameters without seeking further input.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key differences between Claude 3 Opus and Claude 3 Sonnet in handling missing information when making tool calls. It states that Opus actively seeks clarification for missing parameters, while Sonnet is more likely to infer reasonable values without prompting for further details. This aligns well with the correct answer provided. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [07:52<02:10,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 601, Query:\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?\n",
      "__Correct answer__:\n",
      "To ensure a reliable production deployment of Claude for ticket routing, key steps include implementing retry logic to handle errors, conducting thorough staging and load testing, setting up error handling and logging, using a gradual rollout process, providing documentation and training, and establishing monitoring and alerting.\n",
      "__Generated answer__:\n",
      "To ensure a reliable deployment of an automated ticket routing system using Claude into a production environment, the following steps should be taken:\n",
      "\n",
      "1. **Implement Retry Logic**: Add try/except logic to handle cases where Claude may not return the expected output or is temporarily unavailable. Include back-off logic to retry after increasing intervals or adjust the temperature for output variations.\n",
      "\n",
      "2. **Thorough Staging Testing**: Conduct extensive testing in a staging environment that closely resembles your production setup to identify potential issues or incompatibilities before deployment.\n",
      "\n",
      "3. **Load Testing**: Perform load testing to verify that the system can handle the anticipated volume of tickets without performance degradation, ensuring responsiveness and efficiency under real-world conditions.\n",
      "\n",
      "4. **Error Handling and Logging**: Implement comprehensive error handling and logging mechanisms to facilitate debugging and monitoring in production, allowing for quick identification and resolution of issues.\n",
      "\n",
      "5. **Gradual Rollout**: Establish a phased rollout plan, starting with a small percentage of traffic and gradually increasing it while closely monitoring the system’s behavior to minimize risk.\n",
      "\n",
      "6. **Documentation and Training**: Prepare detailed documentation and provide training to relevant stakeholders on how to use and maintain the new system effectively, promoting a smooth transition and adoption.\n",
      "\n",
      "7. **Monitoring and Alerting**: Set up robust monitoring and alerting mechanisms to proactively detect and address any issues that may arise in production, enabling quick responses to minimize downtime.\n",
      "\n",
      "By following these steps, you can ensure a successful and reliable deployment of your automated ticket routing system, providing a seamless experience for users.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes all the key steps outlined in the correct answer, such as implementing retry logic, conducting thorough staging and load testing, setting up error handling and logging, using a gradual rollout process, providing documentation and training, and establishing monitoring and alerting. The details provided in the generated answer expand on the correct answer but do not omit any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [08:01<02:08,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 514, Query:\n",
      "How should you evaluate a model's performance on a ticket routing classifier?\n",
      "__Correct answer__:\n",
      "You should evaluate performance in terms of accuracy, cost, and speed.\n",
      "__Generated answer__:\n",
      "To evaluate a model's performance on a ticket routing classifier, consider the following key metrics and factors:\n",
      "\n",
      "1. **Accuracy**: Measure how often the model's predictions match the correct classifications. This is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a better measure of the model's performance, especially in cases of class imbalance.\n",
      "\n",
      "3. **Consistency**: Evaluate whether the model produces consistent outputs for similar inputs, ensuring reliability in its predictions.\n",
      "\n",
      "4. **Structure**: Check if the model's output adheres to the expected format, such as JSON, making it easy to parse and interpret.\n",
      "\n",
      "5. **Speed**: Assess the response time of the model to ensure it meets acceptable latency thresholds for ticket routing tasks.\n",
      "\n",
      "6. **Bias and Fairness**: If the model classifies data related to people, it is important to ensure that it does not exhibit biases based on gender, ethnicity, or other characteristics that could lead to misclassification.\n",
      "\n",
      "Additionally, before deploying the classifier, implement thorough staging testing, conduct load testing to verify performance under expected ticket volumes, and establish robust error handling and logging mechanisms. A gradual rollout plan can help mitigate risks, and setting up monitoring and alerting systems will allow for proactive issue detection and resolution.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes accuracy, speed, and adds additional relevant metrics such as F1 Score, consistency, structure, bias and fairness, as well as practical considerations for deployment. While it expands on the evaluation criteria, it does not explicitly mention cost, which is a critical piece of information from the correct answer. Therefore, the generated answer is missing a key component and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [08:06<01:45,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 341, Query:\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?\n",
      "__Correct answer__:\n",
      "Anthropic recommends trying their interactive GitHub prompting tutorial and Google Sheets prompting tutorial to learn prompt engineering concepts before diving into the techniques in the documentation.\n",
      "__Generated answer__:\n",
      "Anthropic recommends two methods for learning how to prompt engineer with Claude before diving into the techniques: \n",
      "\n",
      "1. **Prompt Engineering Guide** - This guide helps users optimize Claude’s performance through effective prompting.\n",
      "2. **Anthropic Cookbook** - This resource offers interactive Jupyter notebooks that demonstrate various functionalities, including uploading PDFs and embeddings, providing practical examples for users to learn from.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not correctly identify the two methods recommended by Anthropic for learning prompt engineering with Claude. Instead, it mentions a \"Prompt Engineering Guide\" and an \"Anthropic Cookbook,\" which are not the same as the \"interactive GitHub prompting tutorial\" and \"Google Sheets prompting tutorial\" specified in the correct answer. Therefore, it is missing critical information and does not align with the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [08:14<01:41,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 556, Query:\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?\n",
      "__Correct answer__:\n",
      "Pretrained large language models are trained on unlabeled text data to predict the next word given the previous context, but are not inherently good at answering questions or following instructions without prompt engineering. In contrast, Claude is a large language model that has been further fine-tuned and trained using RLHF to be more helpful, honest, and capable of performing a wider range of useful tasks.\n",
      "__Generated answer__:\n",
      "The key differences between a pretrained large language model (LLM) and Claude primarily lie in their training processes and capabilities. \n",
      "\n",
      "1. **Pretraining Process**: Pretrained LLMs are initially trained on large unlabeled corpora of text to predict the next word in a sequence, which is a foundational autoregressive training method. In contrast, Claude is built upon this autoregressive model but has undergone additional refinement through fine-tuning and Reinforcement Learning from Human Feedback (RLHF), making it more adept at specific tasks.\n",
      "\n",
      "2. **Capabilities**: While pretrained LLMs may not be inherently good at answering questions or following instructions without significant prompt engineering, Claude has been specifically fine-tuned to enhance its performance in these areas. This fine-tuning process aims to make Claude more helpful, honest, and harmless, thus improving its utility as a conversational assistant.\n",
      "\n",
      "3. **Model Family**: Claude is part of a family of models designed to balance intelligence, speed, and cost, indicating a more tailored approach to various applications compared to generic pretrained LLMs.\n",
      "\n",
      "In summary, Claude benefits from additional training and refinement beyond the initial pretraining phase, resulting in enhanced capabilities for conversational tasks and user interactions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the key differences between pretrained large language models and Claude, including the training processes and capabilities. It mentions the autoregressive training method, the additional fine-tuning with RLHF, and the improvements in helpfulness, honesty, and task performance that Claude exhibits. The additional point about Claude being part of a model family designed for various applications adds value but is not critical to the core differences. Overall, the substance of the answer aligns well with the Correct Answer, and no critical information is missing or contradicted.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [08:22<01:38,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 667, Query:\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?\n",
      "__Correct answer__:\n",
      "Prompt engineering is typically faster, more cost-effective, requires less data and compute resources, and preserves the model's general knowledge compared to fine-tuning. It also allows for greater flexibility, rapid iteration, and transparency.\n",
      "__Generated answer__:\n",
      "Some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain include:\n",
      "\n",
      "1. **Resource Efficiency**: Prompt engineering requires only text input and does not need high-end GPUs or large memory, making it more resource-friendly compared to fine-tuning, which demands significant computational resources.\n",
      "\n",
      "2. **Cost-Effectiveness**: Fine-tuning can incur substantial costs, especially when using cloud-based AI services. In contrast, prompt engineering typically utilizes the base model, which is cheaper.\n",
      "\n",
      "3. **Time-Saving**: Fine-tuning can take hours or even days to complete, while prompt engineering allows for nearly instantaneous results, facilitating quick problem-solving.\n",
      "\n",
      "4. **Minimal Data Needs**: Fine-tuning requires a large amount of task-specific, labeled data, which can be scarce or expensive. Prompt engineering can effectively operate with few-shot or even zero-shot learning.\n",
      "\n",
      "5. **Flexibility and Rapid Iteration**: Prompt engineering enables quick experimentation with different approaches and immediate feedback, making it easier to tweak prompts and see results compared to the slower process of fine-tuning.\n",
      "\n",
      "6. **Domain Adaptation**: It allows for easy adaptation to new domains by providing domain-specific context directly in the prompts without the need for retraining.\n",
      "\n",
      "7. **Preservation of General Knowledge**: Fine-tuning can lead to catastrophic forgetting, where the model loses its general knowledge. Prompt engineering helps maintain the model's broad capabilities.\n",
      "\n",
      "8. **Transparency**: Prompts are human-readable, which provides clarity on what information the model receives. This transparency aids in understanding and debugging the model's behavior.\n",
      "\n",
      "9. **Comprehension Improvements**: Prompt engineering is often more effective than fine-tuning at helping models understand and utilize external content, such as retrieved documents. \n",
      "\n",
      "These advantages make prompt engineering a preferred method for quickly adapting models to specific tasks or domains.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly captures the key advantages of prompt engineering over fine-tuning as outlined in the correct answer. It discusses resource efficiency, cost-effectiveness, time-saving, minimal data needs, flexibility, preservation of general knowledge, and transparency. Additionally, it includes the point about domain adaptation and comprehension improvements, which are relevant and enhance the explanation. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [08:25<01:15,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 341, Query:\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?\n",
      "__Correct answer__:\n",
      "Before running requests to access Claude models on Vertex AI, you may need to run `gcloud auth application-default login` to authenticate with GCP.\n",
      "__Generated answer__:\n",
      "To authenticate with Google Cloud Platform (GCP) before running requests to access Claude models on Vertex AI, you need to execute the command `gcloud auth application-default login`. This command will allow you to authenticate your application with GCP, enabling you to make requests to the Claude models. After authenticating, you can proceed to use the client SDKs to interact with the models.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you need to execute the command `gcloud auth application-default login` to authenticate with GCP before accessing Claude models on Vertex AI. It also explains the purpose of the command, which is to authenticate the application with GCP, allowing requests to the Claude models. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [08:30<01:02,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 509, Query:\n",
      "What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\n",
      "__Correct answer__:\n",
      "According to the information provided, on May 10th, 2024, Anthropic introduced a new \"Prompt Generator\" tool in the Developer Console. This tool is designed to help users guide Claude to generate high-quality prompts tailored to their specific tasks. The text states that the Prompt Generator \"makes it easy to guide Claude to generate a high-quality prompts tailored to your specific tasks.\" This indicates that the Prompt Generator feature provides users with the ability to create customized prompts for Claude, going beyond the standard prompting capabilities. By combining this information with the details about the Claude iOS app and the Claude Team plan released around the same time, we can infer that Anthropic was expanding its platform and tools to provide users with more advanced capabilities for interacting with and leveraging the Claude AI assistant for their specific needs and use cases.\n",
      "__Generated answer__:\n",
      "On May 10th, 2024, Anthropic introduced a new Prompt Generator tool in the Developer Console. This tool enables users to easily create high-quality prompts that are specifically tailored to their tasks. By guiding users in the prompt creation process, the Prompt Generator enhances the ability to interact effectively with Claude, Anthropic's AI model. This feature is designed to streamline the prompt development process, making it more accessible and efficient for users to achieve their desired outcomes with the AI.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key information from the Correct Answer regarding the introduction of the Prompt Generator tool on May 10th, 2024, and its purpose in helping users create tailored prompts for their tasks. Both answers convey the same essential information about the tool's functionality and its impact on user interaction with Claude. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [08:32<00:47,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 299, Query:\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?\n",
      "__Correct answer__:\n",
      "Both Claude 3.5 Sonnet and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "__Generated answer__:\n",
      "Both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer matches the correct answer exactly, providing the same date and information about the availability of both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 90/100 questions. Current Accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [08:35<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 306, Query:\n",
      "When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\n",
      "__Correct answer__:\n",
      "You can use \"max_tokens\": 1 in the request to limit Claude's response to a single token when putting words in its mouth.\n",
      "__Generated answer__:\n",
      "To limit Claude's response to a single token, you can use the header `max_tokens` with a value of `1` in your request.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can use the header `max_tokens` with a value of `1` to limit Claude's response to a single token, which aligns with the correct answer provided. There are no critical pieces of information missing, and the substance of the answers is the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [08:38<00:31,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 332, Query:\n",
      "What does the temperature parameter do when working with large language models?\n",
      "__Correct answer__:\n",
      "Temperature is a parameter that controls the randomness of the model during generation\n",
      "__Generated answer__:\n",
      "The temperature parameter in large language models controls the randomness of the model's predictions during text generation. A higher temperature results in more creative and diverse outputs, allowing for variations in phrasing and responses. Conversely, a lower temperature leads to more conservative and deterministic outputs, favoring the most probable choices. By adjusting the temperature, users can encourage the model to explore less common or surprising word choices and sequences, rather than solely relying on the most likely predictions.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the function of the temperature parameter in large language models, emphasizing its role in controlling randomness and the effects of higher and lower temperatures on output creativity and determinism. This aligns well with the correct answer, which states that temperature controls randomness during generation. Therefore, the generated answer is correct as it captures the essence of the correct answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [08:45<00:33,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 460, Query:\n",
      "What are two ways to specify API parameters when calling the Claude API using Claude for Sheets?\n",
      "__Correct answer__:\n",
      "When calling the Claude API using Claude for Sheets, you can specify API parameters in two ways: 1) As additional arguments after the prompt and model in the CLAUDE() function, like =CLAUDE(prompt, model, \"max_tokens\", 3). 2) By passing in an API key to be used just for a specific cell, like \"api_key\", \"sk-ant-api03-j1W...\"\n",
      "__Generated answer__:\n",
      "Two ways to specify API parameters when calling the Claude API using Claude for Sheets are:\n",
      "\n",
      "1. **Simple Prompt**: You can call the function with just a prompt, like this: `=CLAUDE(\"Your prompt here\")`.\n",
      "\n",
      "2. **Adding Parameters**: You can include additional parameters after the initial prompt. The syntax is `=CLAUDE(prompt, model, params...)`, where the model is always the second argument. For example: `=CLAUDE(\"Hi, Claude!\", \"claude-3-haiku-20240307\", \"max_tokens\", 3)`. This allows you to set any API parameter, including an API key for that specific cell.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways to specify API parameters when calling the Claude API using Claude for Sheets. It mentions adding parameters after the prompt and model, which aligns with the correct answer. However, it does not explicitly mention the ability to pass an API key for a specific cell, which is a critical piece of information in the correct answer. Therefore, while the generated answer is mostly accurate, it is missing a key detail that makes it incomplete.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [08:51<00:31,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 397, Query:\n",
      "How does prefilling the response with an opening curly brace ({ ) affect Claude's output when extracting structured data from text?\n",
      "__Correct answer__:\n",
      "Prefilling Claude's response with { causes it to skip the preamble explanation and directly output the extracted data as a JSON object, resulting in a more concise response that is easier for programs to parse without additional processing.\n",
      "__Generated answer__:\n",
      "Prefilling the response with an opening curly brace ({) can signal to Claude that the output should be structured data, such as JSON. This can influence Claude's output by encouraging it to format the response in a way that aligns with structured data expectations. When extracting structured data from text, this prefilled character can help Claude understand that the response should follow a specific format, potentially leading to more organized and coherent outputs that are easier to parse programmatically. It sets the context for the type of response expected, which can enhance the accuracy and relevance of the structured data extracted.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the essence of how prefilling the response with an opening curly brace ({) influences Claude's output by indicating that the response should be structured data, such as JSON. It mentions that this can lead to more organized and coherent outputs, which aligns with the Correct Answer's emphasis on skipping the preamble and producing a concise JSON object. However, the Generated Answer does not explicitly mention the aspect of skipping the preamble explanation, which is a critical piece of information in the Correct Answer. Therefore, while the Generated Answer is largely correct in its understanding, it is missing a key detail that affects its completeness.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [08:56<00:25,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 434, Query:\n",
      "What are some helpful resources provided by Anthropic to dive deeper into building with images using Claude?\n",
      "__Correct answer__:\n",
      "Anthropic provides a multimodal cookbook with tips on getting started with images and best practices, as well as API reference documentation for the Messages API that includes example API calls involving images.\n",
      "__Generated answer__:\n",
      "To dive deeper into building with images using Claude, you can explore the following helpful resources provided by Anthropic:\n",
      "\n",
      "1. **Multimodal Cookbook**: This resource offers tips on getting started with images and best practices for ensuring high-quality performance. It includes guidance on effectively prompting Claude to interpret and analyze charts or extract content from forms.\n",
      "\n",
      "2. **API Reference**: Check the documentation for the Messages API, which includes example API calls that involve images.\n",
      "\n",
      "3. **Additional Resources**: The documentation also covers important topics such as evaluating image size, calculating image costs, ensuring image quality, and provides prompt examples, limitations, and FAQs related to using vision capabilities.\n",
      "\n",
      "For further assistance, you can reach out to Anthropic's support team or join their developer community to connect with other creators and get help from experts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the multimodal cookbook and the API reference documentation as resources provided by Anthropic for building with images using Claude. It also expands on the information by mentioning additional topics covered in the documentation, which enhances the response without contradicting the correct answer. Therefore, the generated answer is essentially aligned with the correct answer and includes relevant details that are not present in the correct answer. </explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [09:02<00:21,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 499, Query:\n",
      "How do you specify the API key when creating a new Anthropic client in the Python and TypeScript SDK examples?\n",
      "__Correct answer__:\n",
      "In both the Python and TypeScript examples, you can specify the API key as a string parameter when creating a new Anthropic client object. If no API key is provided, it defaults to using the ANTHROPIC_API_KEY environment variable.\n",
      "__Generated answer__:\n",
      "In the Python and TypeScript SDK examples for creating a new Anthropic client, you can specify the API key in the following ways:\n",
      "\n",
      "**Python:**\n",
      "You can set the API key by exporting the `ANTHROPIC_API_KEY` environment variable or by supplying it directly when initializing the client. The specific Python example is not provided in the documents, but it would typically look like this:\n",
      "\n",
      "```python\n",
      "from anthropic import Anthropic\n",
      "\n",
      "client = Anthropic(api_key='your_api_key')  # or use the environment variable\n",
      "```\n",
      "\n",
      "**TypeScript:**\n",
      "In TypeScript, you specify the API key when initializing the Anthropic client like this:\n",
      "\n",
      "```typescript\n",
      "import Anthropic from '@anthropic-ai/sdk';\n",
      "\n",
      "const anthropic = new Anthropic({\n",
      "  apiKey: 'my_api_key', // defaults to process.env[\"ANTHROPIC_API_KEY\"]\n",
      "});\n",
      "```\n",
      "\n",
      "In both cases, if you do not provide the API key directly, the SDK will default to using the `ANTHROPIC_API_KEY` environment variable.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to specify the API key when creating a new Anthropic client in both Python and TypeScript. It mentions that the API key can be provided directly or through the `ANTHROPIC_API_KEY` environment variable, which aligns with the correct answer. The examples given for both languages are appropriate and convey the same information as the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [09:06<00:14,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 396, Query:\n",
      "What are two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application?\n",
      "__Correct answer__:\n",
      "The Evaluation tool helps identify edge cases where the prompt might falter, and ensures consistent performance across a range of test case inputs. This allows you to refine the prompt for better reliability in the AI classification application.\n",
      "__Generated answer__:\n",
      "Two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application are:\n",
      "\n",
      "1. **Identifying Edge Cases**: The Evaluation tool helps users pinpoint edge cases where the prompt may not perform well. This allows developers to understand the limitations of their prompts and make necessary adjustments to improve overall performance.\n",
      "\n",
      "2. **Ensuring Consistent Performance**: The tool enables users to rate individual results and assess the prompt's performance across a range of inputs. This consistency check helps in refining prompts for better reliability, ensuring that the AI application behaves predictably under various scenarios.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the two key benefits of using the Anthropic Evaluation tool as described in the Correct Answer. Both answers emphasize the importance of identifying edge cases and ensuring consistent performance across various inputs. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [09:14<00:11,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 544, Query:\n",
      "What are the key differences between a pretrained language model like Claude's underlying model, and the final version of Claude available through Anthropic's API?\n",
      "__Correct answer__:\n",
      "The pretrained language model that forms Claude's foundation is not inherently good at answering questions or following instructions. To create the helpful, honest and safe Claude assistant available through the API, the pretrained model underwent fine-tuning and reinforcement learning from human feedback (RLHF).\n",
      "__Generated answer__:\n",
      "The key differences between a pretrained language model like Claude's underlying model and the final version of Claude available through Anthropic's API are as follows:\n",
      "\n",
      "1. **Training Process**: The underlying model is pretrained on a large unlabeled corpus of text to predict the next word based on previous context. This pretraining phase does not make the model inherently good at tasks like answering questions or following instructions.\n",
      "\n",
      "2. **Refinement**: The final version of Claude, available through the API, undergoes additional processes such as fine-tuning and Reinforcement Learning from Human Feedback (RLHF). These processes refine the pretrained model, enhancing its ability to perform a wide range of tasks effectively.\n",
      "\n",
      "3. **Performance**: The pretrained model may require significant prompt engineering to elicit desired behaviors, while the final version is designed to be more user-friendly and capable of handling various tasks without extensive prompt manipulation.\n",
      "\n",
      "4. **Capabilities**: The final version of Claude is optimized for better performance in terms of accuracy, speed, and task versatility compared to the raw pretrained model, which is primarily focused on language prediction.\n",
      "\n",
      "In summary, the pretrained model serves as a foundational layer, while the final version is a polished product that is more adept at practical applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key differences between the pretrained language model and the final version of Claude. It mentions the training process, refinement through fine-tuning and RLHF, and improvements in performance and capabilities, which align with the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [09:18<00:05,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 270, Query:\n",
      "What is the IPv6 address range used by Anthropic?\n",
      "__Correct answer__:\n",
      "The IPv6 address range used by Anthropic is 2607:6bc0::/48.\n",
      "__Generated answer__:\n",
      "The IPv6 address range used by Anthropic is 2607:6bc0::/48.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer matches the correct answer exactly, providing the same IPv6 address range used by Anthropic. There are no missing pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [09:24<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 348, Query:\n",
      "When using the Python SDK to create a message with Claude, what are two ways you can specify your API key?\n",
      "__Correct answer__:\n",
      "When using the Python SDK, you can specify your API key either by passing it as the api_key parameter when initializing the Anthropic client, or by setting it as an environment variable named ANTHROPIC_API_KEY which the client will use by default.\n",
      "__Generated answer__:\n",
      "You can specify your API key in two ways when using the Python SDK to create a message with Claude:\n",
      "\n",
      "1. By exporting the API key as an environment variable named `ANTHROPIC_API_KEY`.\n",
      "2. By supplying the API key directly to the Anthropic client when initializing it.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two ways to specify the API key when using the Python SDK: setting it as an environment variable and supplying it directly to the Anthropic client. It captures the essence of the correct answer without omitting any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 100/100 questions. Current Accuracy: 0.7300\n",
      "Detailed results saved to evaluation/csvs/evaluation_results_summary_enhanced.csv\n",
      "Average Precision: 0.4033\n",
      "Average Recall: 0.6508\n",
      "Average MRR: 0.7633\n",
      "Average F1: 0.4980\n",
      "End-to-End Accuracy: 0.7300\n",
      "Evaluation complete. Results saved to evaluation/json_results/evaluation_results_summary_enhanced.json, evaluation/csvs/evaluation_results_summary_enhanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "# level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "# level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the evaluations\n",
    "eval_data_range = eval_data[0:100]\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_similar_level_two, eval_data_range, level_two_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context_level_two, level_two_db, eval_data_range)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data_range],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_summary_enhanced.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a json file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_summary_enhanced.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Enhanced\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ea28c0-4e1c-4398-ac76-0739a0a07553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "from utils.plot_perf import plot_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec177621-9804-425e-a2c7-db0a00d6a0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's visualize our accuracy\n",
    "# plot_performance('evaluation/json_results', ['Basic RAG', 'Summary Enhanced'], colors=['skyblue', 'green'])\n",
    "plot_performance('evaluation/json_results', [ 'Summary Enhanced'], colors=['green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d19c2-a063-4bb9-ad29-beedbdf24e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
