{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4834a618-cb3c-4f71-b4f1-a18063afcc11",
   "metadata": {},
   "source": [
    "# RAG Retrieval Enhanced with Document Summaries\n",
    "In this section, we'll implement an improved approach to our retrieval system by incorporating document summaries. Instead of embedding chunks directly from the documents, we'll create a concise summary for each chunk and use this summary along with the original content in our embedding process.\n",
    "\n",
    "This approach aims to capture the essence of each document chunk more effectively, potentially leading to improved retrieval performance.\n",
    "\n",
    "Key steps in this process:\n",
    "\n",
    "1. We load the original document chunks.\n",
    "2. For each chunk, we generate a 2-3 sentence summary using OpenAI (or an OpenAI compatible API).\n",
    "3. We store both the original content and the summary for each chunk in a new json file: data/anthropic_summary_indexed_docs.json\n",
    "\n",
    "This summary-enhanced approach is designed to provide more context during the embedding and retrieval phases, potentially improving the system's ability to understand and match the most relevant documents to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b80e4-3558-445c-a17c-5a4b8db4cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## silent setup (-q), may take a while\n",
    "!pip install openai -q\n",
    "!pip install --upgrade tiktoken -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af37b9a7-0878-4b8d-ae76-ad694cb512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "embeddings_model_name = \"intfloat/multilingual-e5-large-instruct\"; generation_model = \"gpt-4o-mini\"; judge_model = \"gpt-4o-mini\"\n",
    "embeddings_model_name = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "model_temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d1e786-d81b-411c-b2e0-8d618a5f5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262fa9ab-559b-41be-9de3-4ae757c2fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944cd19f2f5a4a77a2e830fe20932783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568f65cc0170411995c4cb1f049c303d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e408a014684445b78f75a87e8f6879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/71.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715a49c3321a4535a3d423e49fdcb9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd3c63229244721b33dccf4ece343d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d8315c56a5435ab029331b8ec48dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_bert.py:   0%|          | 0.00/8.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
      "- configuration_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff65f305d48422084bff7ac93ad6a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_bert.py:   0%|          | 0.00/97.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
      "- modeling_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7c9d6d710e437988da8e8c9dea7899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/275M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461ae49f56804755a49cc224d9d1759d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf3ee0fc88949b49fdb2b7795826762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3651a9b1e3488d83ff46c0dec56c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f260f694ee4fc1b21ab56e9b0f3d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0845f0a35e413580571a52ad61244c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length of model, jinaai/jina-embeddings-v2-base-en:, 4096, about 3072.0 words\n",
      "tensor([[0.9341]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# embeddings_model = SentenceTransformer(embeddings_model_name)\n",
    "# max_len = embeddings_model.max_seq_length\n",
    "\n",
    "# try a difference model with longer context window\n",
    "embeddings_model = SentenceTransformer(\n",
    "    embeddings_model_name, # switch to en/zh for English or Chinese\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# control your input sequence length up to 8192\n",
    "embeddings_model.max_seq_length = 4096\n",
    "\n",
    "# max_word_len = max_len * 0.75\n",
    "max_word_len = embeddings_model.max_seq_length * 0.75\n",
    "\n",
    "# print(f\"Max Sequence Length of model, {embeddings_model_name}:, {max_len}, about {max_word_len} words\")\n",
    "print(f\"Max Sequence Length of model, {embeddings_model_name}:, {embeddings_model.max_seq_length}, about {max_word_len} words\")\n",
    "\n",
    "# run a short test\n",
    "from sentence_transformers.util import cos_sim\n",
    "embeddings = embeddings_model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54311b-bba4-40d4-b4fe-f17e54991e10",
   "metadata": {},
   "source": [
    "### Generating the Summaries and Storing Them\n",
    "You can invoke this depending on whether its already available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b741b86c-f53e-4220-9c5b-bbbe6b7db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, this is for Claud-3-haiku, need to be changed to OpenAI or Llama\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summaries(input_file, output_file):\n",
    " \n",
    "    # Load the original documents\n",
    "    with open(input_file, 'r') as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    # Prepare the context about the overall knowledge base\n",
    "    knowledge_base_context = \"This is documentation for Anthropic's, a frontier AI lab building Claude, an LLM that excels at a variety of general purpose tasks. These docs contain model details and documentation on Anthropic's APIs.\"\n",
    "\n",
    "    summarized_docs = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Generating summaries\"):\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with creating a short summary of the following content from Anthropic's documentation. \n",
    "\n",
    "        Context about the knowledge base:\n",
    "        {knowledge_base_context}\n",
    "\n",
    "        Content to summarize:\n",
    "        Heading: {doc['chunk_heading']}\n",
    "        {doc['text']}\n",
    "\n",
    "        Please provide a brief summary of the above content in 2-3 sentences. The summary should capture the key points and be concise. We will be using it as a key part of our search pipeline when answering user queries about this content. \n",
    "\n",
    "        Avoid using any preamble whatsoever in your response. Statements such as 'here is the summary' or 'the summary is as follows' are prohibited. You should get straight into the summary itself and be concise. Every word matters.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=150,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        summary = response.content[0].text.strip()\n",
    "\n",
    "        summarized_doc = {\n",
    "            \"chunk_link\": doc[\"chunk_link\"],\n",
    "            \"chunk_heading\": doc[\"chunk_heading\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"summary\": summary\n",
    "        }\n",
    "        summarized_docs.append(summarized_doc)\n",
    "\n",
    "    # Save the summarized documents to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(summarized_docs, f, indent=2)\n",
    "\n",
    "    print(f\"Summaries generated and saved to {output_file}\")\n",
    "    \n",
    "# this is already available, so the call is commented out\n",
    "# generate_summaries('data/anthropic_docs.json', 'data/anthropic_summary_indexed_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cfea-b81d-4d06-9c8f-337f9ecc95c9",
   "metadata": {},
   "source": [
    "### Summary-Enhanced Vector Database Creation (heading + summary + chunk)\n",
    "Here, we're creating a new vector database that incorporates our summary-enhanced document chunks. This approach combines the original text, the chunk heading, and the newly generated summary into a single text for embedding.\n",
    "\n",
    "Key features of this process:\n",
    "\n",
    "1. We create embeddings for the combined text (heading + summary + original content) using the Voyage AI API.\n",
    "2. The embeddings and full metadata (including summaries) are stored in our vector database.\n",
    "3. We implement caching mechanisms to improve efficiency in repeated queries.\n",
    "4. The database is saved to disk for persistence and quick loading in future sessions.\n",
    "\n",
    "This summary-enhanced approach aims to create more informative embeddings, potentially leading to more accurate and contextually relevant document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351e638a-d09a-4295-8de6-6c54ac6e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class SummaryEnhancedVectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/summary_indexed_vector_db.pkl\"\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        \"\"\"not called for now\"\"\"\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size], normalize_embeddings=True)\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "        \n",
    "    def load_data(self, data_file):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Loading vector database from file: {self.db_path}.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "            \n",
    "        # well, if not...\n",
    "        print(f'file {self.db_path} does not exist')\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Embed Chunk Heading + Text + Summary Together\n",
    "        texts = [f\"{item['chunk_heading']}\\n\\n{item['text']}\\n\\n{item['summary']}\" for item in data]\n",
    "        print(f'****Total Chunks: {len(texts)}')\n",
    "        texts_exceeding_max_len = [s for s in texts if len(s) > max_word_len]\n",
    "        print(f'****Chunks greater that {max_word_len} words: {len(texts_exceeding_max_len)}')\n",
    "        \n",
    "        # Embed more than 128 documents with a for loop\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size], normalize_embeddings=True)\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data  # Store the entire item as metadata\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=3, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            # print(f'found in cache!')\n",
    "            query_embedding = np.array(self.query_cache[query])  #\n",
    "            # print(f'type:{type(query_embedding)}')\n",
    "        else:\n",
    "            query_embedding = embeddings_model.encode(query, normalize_embeddings=True)\n",
    "            # print(f'query embedding:\\n {query_embedding}')\n",
    "            self.query_cache[query] = query_embedding.tolist()\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        print(f'Saving DB in: {self.db_path}')\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c306c5-1d54-4ca0-9a14-4947caae1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 3 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=3):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4e912-1419-4078-a063-c83d30b9a9de",
   "metadata": {},
   "source": [
    "### Enhanced Retrieval Using Summary-Enhanced Embeddings\n",
    "In this section, we implement the retrieval process using our new summary-enhanced vector database. This approach leverages the enhanced embeddings we created, which incorporate document summaries along with the original content.\n",
    "\n",
    "Key aspects of this updated retrieval process:\n",
    "\n",
    "1. We search the vector database using the query embedding, retrieving the top k most similar documents.\n",
    "2. For each retrieved document, we include the chunk heading, summary, and full text in the context provided to the LLM.\n",
    "3. This enriched context is then used to generate an answer to the user's query.\n",
    "\n",
    "By including summaries in both the embedding and retrieval phases, we aim to provide the LLM with a more comprehensive and focused context. This could potentially lead to more accurate and relevant answers, as the LLM has access to both a concise overview (the summary) and the detailed information (the full text) for each relevant document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a875906-ca83-4bb2-bdce-d8508e45025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl does not exist\n",
      "****Total Chunks: 232\n",
      "****Chunks greater that 3072.0 words: 63\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n",
      "Vector database loaded and saved.\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "def retrieve_similar_level_two(query, db):\n",
    "    # print(f'_______Query used for retrieval________:\\n {query}')\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        chunk = result['metadata']\n",
    "        # show model all 3 items; heading, text, summary\n",
    "        context += f\"\\n <document> \\n Heading:\\n{chunk['chunk_heading']}\\n\\nText:\\n {chunk['text']} \\n\\nSummary: \\n {chunk['summary']} \\n </document> \\n\" \n",
    "    \n",
    "        # print(f'-----------start retrieval {i} --------------')\n",
    "        # print(f\"__Retrieved results heading__:\\n{result['metadata']['chunk_heading']}\")\n",
    "        # print(f\"__Retrieved results text__:\\n{result['metadata']['text']}\")\n",
    "        # print(f\"__Retrieved results summary__:\\n{result['metadata']['summary']}\")\n",
    "        # print(f'-----------end retrieval {i} ----------------')\n",
    "        \n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context_level_two(query, db):\n",
    "    documents, context = retrieve_similar_level_two(query, db)\n",
    "    # print(f'query + context:\\n{construct_prompt(query, context)}')\n",
    "    completion = client.chat.completions.create(\n",
    "    model=generation_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=model_temperature\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Initialize the SummaryEnhancedVectorDB\n",
    "level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "level_two_db.save_db()\n",
    "\n",
    "# # Load the Anthropic documentation\n",
    "# with open('data/anthropic_docs.json', 'r') as f:\n",
    "#     anthropic_docs = json.load(f)\n",
    "\n",
    "# test\n",
    "#query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\n",
    "query = \"What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\"\n",
    "test_results, test_contexts = retrieve_similar_level_two(query, level_two_db)\n",
    "# for i, test_result in enumerate(test_results):\n",
    "#     print(f'ith:{i}\\n {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daa157-bd0c-4462-be43-f2a7d1f06bc4",
   "metadata": {},
   "source": [
    "### Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51398e7-2da9-47ca-90f8-e4e565f6108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"For OpenAI models, returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db) # ??\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_tokens = num_tokens_from_string(comparision_prompt, \"o200k_base\")  # note, this encoding name is for gpt-4o, gpt-4o-mini\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful judge.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=model_temperature,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'Number of query tokens: {nb_tokens}, Query:\\n{query}')\n",
    "            print(f'__Correct answer__:\\n{correct_answer}')\n",
    "            print(f'__Generated answer__:\\n{generated_answer}')\n",
    "            print(f'__Response from judge LLM__:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8852c4-15df-422c-bdb7-cccf25f8da08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  20%|██        | 20/100 [00:00<00:00, 93.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.4667, Avg Recall: 0.7500, Avg MRR: 1.0000\n",
      "Processed 20/100 items. Current Avg Precision: 0.3667, Avg Recall: 0.6000, Avg MRR: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  40%|████      | 40/100 [00:00<00:00, 90.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.4111, Avg Recall: 0.6500, Avg MRR: 0.7444\n",
      "Processed 40/100 items. Current Avg Precision: 0.4167, Avg Recall: 0.6542, Avg MRR: 0.7708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  60%|██████    | 60/100 [00:00<00:00, 93.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4200, Avg Recall: 0.6733, Avg MRR: 0.7633\n",
      "Processed 60/100 items. Current Avg Precision: 0.4333, Avg Recall: 0.7028, Avg MRR: 0.8028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  80%|████████  | 80/100 [00:00<00:00, 93.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.4095, Avg Recall: 0.6702, Avg MRR: 0.7643\n",
      "Processed 80/100 items. Current Avg Precision: 0.4250, Avg Recall: 0.6927, Avg MRR: 0.7813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:01<00:00, 92.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4222, Avg Recall: 0.6824, Avg MRR: 0.7722\n",
      "Processed 100/100 items. Current Avg Precision: 0.4033, Avg Recall: 0.6508, Avg MRR: 0.7533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:07<12:28,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 420, Query:\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\n",
      "__Correct answer__:\n",
      "To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\n",
      "__Generated answer__:\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. Access the Evaluation screen after navigating to the prompt editor in the Anthropic Console.\n",
      "2. Click the ‘Add Test Case’ button to add a new test case.\n",
      "3. Fill in the values for each variable in your prompt for the new test case.\n",
      "4. Repeat the process to create as many test cases as needed.\n",
      "\n",
      "Make sure your original prompt includes at least 1-2 dynamic variables using the double brace syntax ({{variable}}) to facilitate the creation of evaluation test sets. If you update your original prompt text, you can re-run the entire evaluation suite to see how changes affect performance across all test cases.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the process of creating multiple test cases in the Anthropic Evaluation tool, including the steps to access the evaluation screen, add test cases, fill in variables, and repeat the process. It also includes additional relevant information about using dynamic variables and re-running the evaluation suite, which enhances the understanding of the process. Therefore, it aligns well with the Correct Answer and does not omit any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:11<09:07,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 373, Query:\n",
      "What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\n",
      "__Correct answer__:\n",
      "Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\n",
      "__Generated answer__:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models and provides options for customization, including bespoke fine-tuned models tailored for specific industries such as finance and healthcare. They also have a variety of models optimized for different tasks, such as code retrieval and general-purpose use, and are actively developing more specialized models. Additionally, Voyage AI offers fine-tuning services to customize models for individual customers.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies Voyage AI as the recommended embeddings provider and mentions the customization options, including bespoke fine-tuned models for specific industries like finance and healthcare. It also adds details about state-of-the-art models and various optimizations, which do not contradict the correct answer. Therefore, the generated answer captures the essential information and is essentially aligned with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:18<09:52,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 621, Query:\n",
      "What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\n",
      "__Correct answer__:\n",
      "When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\n",
      "__Generated answer__:\n",
      "Key success metrics to consider when evaluating Claude's performance on a classification task include:\n",
      "\n",
      "1. **Accuracy**: Measures the proportion of correct predictions made by the model. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive instances and its ability to avoid false positives.\n",
      "\n",
      "3. **Consistency**: Evaluates whether the model produces similar outputs for similar inputs, ensuring logical and predictable behavior.\n",
      "\n",
      "4. **Structure**: Assesses whether the model's output adheres to the expected format, such as JSON, which is important for ease of parsing and interpretation.\n",
      "\n",
      "5. **Speed**: Refers to the model's response time, which is crucial for meeting latency requirements in real-time applications.\n",
      "\n",
      "6. **Bias and Fairness**: Important for ensuring that the model does not exhibit biases based on sensitive attributes like gender or ethnicity, which could lead to unfair classifications.\n",
      "\n",
      "When choosing the right model to reduce latency, the speed metric becomes particularly relevant. For classification tasks, smaller models like Claude 3 Haiku are typically preferred due to their speed and efficiency. In contrast, larger models like Sonnet or Opus may be more suitable for tasks requiring complex reasoning but could introduce higher latency. Therefore, evaluating these success metrics helps in selecting a model that not only performs well in terms of accuracy and fairness but also meets the necessary speed requirements to minimize latency.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key success metrics mentioned in the Correct Answer, including accuracy, F1 score, consistency, structure, speed, and bias and fairness. It also correctly emphasizes the importance of speed in choosing the right model to reduce latency. Additionally, it provides further context about model selection based on size and complexity, which aligns with the intent of the Correct Answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:24<09:25,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 415, Query:\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\n",
      "__Correct answer__:\n",
      "Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\n",
      "__Generated answer__:\n",
      "Claude for Sheets can improve prompt engineering workflows compared to using chained prompts in two key ways:\n",
      "\n",
      "1. **Parallel Testing of Prompts**: Claude for Sheets allows users to test prompts across evaluation suites in parallel. This capability enables more efficient experimentation and optimization of prompts, as multiple variations can be assessed simultaneously, leading to faster iterations and improvements.\n",
      "\n",
      "2. **Resource for Learning and Exploration**: The Claude for Sheets prompting examples workbench serves as a comprehensive resource that houses example prompts and prompt engineering structures. This facilitates exploration and learning about prompt engineering, providing users with a structured environment to develop and refine their prompts, which may not be as easily achievable with a simple chained prompt approach.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts: parallel testing of prompts and the resource for learning and exploration. While the second point in the generated answer focuses on the prompting examples workbench, which is not explicitly mentioned in the correct answer, it still aligns with the idea of facilitating exploration and learning. The key aspects of parallel testing and efficiency are present in both answers. Therefore, the generated answer captures the essence of the correct answer without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:26<07:36,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 353, Query:\n",
      "What happens if a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns?\n",
      "__Correct answer__:\n",
      "If a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error.\n",
      "__Generated answer__:\n",
      "If a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. The API requires these specific markers to delineate the turns between the human and the assistant. Without them, the prompt does not conform to the expected format, leading to a failure in processing the request.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. Both state that the absence of the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns will result in an API error, and the generated answer further explains the necessity of these markers for proper formatting. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:31<07:12,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 476, Query:\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?\n",
      "__Correct answer__:\n",
      "Tool use requests in the Claude API are priced the same as regular API requests, based on the total input and output tokens. However, tool use requests have additional tokens beyond the regular input and output, including the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt that enables tool use, which add to the total tokens and cost.\n",
      "__Generated answer__:\n",
      "The additional tokens required for tool use in Claude API requests impact pricing by increasing the total number of input and output tokens counted for the request. Tool use requests are priced the same as regular API requests, based on the total number of tokens, which now includes:\n",
      "\n",
      "1. Tokens from the tools parameter (tool names, descriptions, and schemas).\n",
      "2. Tokens from tool_use content blocks in API requests and responses.\n",
      "3. Tokens from tool_result content blocks in API requests.\n",
      "4. A special system prompt for the model that enables tool use, which varies by model and adds between 159 to 530 tokens.\n",
      "\n",
      "These additional tokens are added to the normal input and output tokens to calculate the total cost of a request. Therefore, using tools will generally increase the cost of API requests compared to regular requests that do not utilize tools.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that tool use requests in the Claude API are priced the same as regular API requests based on the total number of tokens, which includes additional tokens for the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt. It also mentions that these additional tokens increase the total cost of the request, which aligns with the correct answer. Therefore, the generated answer captures the essential information and does not omit any critical details. </explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:33<06:12,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 311, Query:\n",
      "When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\n",
      "__Correct answer__:\n",
      "The new Usage, Cost, and Rate Limits tabs in the Anthropic Developer Console that show API usage, billing details, and current rate limits will be available on June 27th, 2024.\n",
      "__Generated answer__:\n",
      "The new Anthropic Developer Console features that show API usage, billing details, and rate limits will be available on June 27th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating the same date for the availability of the new features in the Anthropic Developer Console. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:41<07:58,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 436, Query:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\n",
      "__Correct answer__:\n",
      "When deciding whether to use CoT, consider if the task requires in-depth thinking that a human would need to work through, and be aware that the increased output length from CoT may impact latency.\n",
      "__Generated answer__:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, two key factors to consider are:\n",
      "\n",
      "1. **Task Complexity**: CoT should be used for tasks that require in-depth thinking, such as complex math, multi-step analysis, or decisions involving multiple factors. If the task does not necessitate extensive processing, it may be better to avoid CoT to maintain efficiency.\n",
      "\n",
      "2. **Latency Impact**: The use of CoT can increase output length, which may negatively affect latency. It is important to balance the need for thorough reasoning with the acceptable response time for the task at hand. Choosing the right model that aligns with your performance and speed requirements can also help mitigate latency issues.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two key factors to consider when deciding whether to use chain-of-thought (CoT): task complexity and latency impact. It emphasizes the need for in-depth thinking for complex tasks and acknowledges that CoT can increase output length, affecting latency. Both answers convey the same essential information, so the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:44<06:56,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 357, Query:\n",
      "How can I use Claude to more easily digest the content of long PDF documents?\n",
      "__Correct answer__:\n",
      "You can upload PDFs and have Claude summarize their content, making it easier to understand the key points of long documents without having to read through everything.\n",
      "__Generated answer__:\n",
      "To use Claude for digesting the content of long PDF documents, you can upload the PDFs directly to Claude. It has a built-in feature for PDF upload and summarization, which allows it to summarize the content of the documents, making it easier to understand and extract key information. This functionality is designed to handle long documents effectively, providing concise summaries that highlight the main points. Additionally, Claude's capabilities include processing detailed documents with a long context window, ensuring accurate and relevant outputs.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to use Claude for digesting long PDF documents by mentioning the ability to upload PDFs and summarize their content. It also adds details about the built-in feature for PDF upload and summarization, as well as Claude's capability to process detailed documents with a long context window. These additions do not contradict the correct answer and provide a more comprehensive understanding of the functionality. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:47<05:44,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 275, Query:\n",
      "According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\n",
      "__Correct answer__:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab of the Developer Console.\n",
      "__Generated answer__:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab in the Developer Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer conveys the same essential information as the correct answer, stating that the organization's current API rate limits can be viewed in the Rate Limits tab of the Developer Console. The slight difference in wording does not affect the substance of the answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/100 questions. Current Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [00:54<07:29,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 709, Query:\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?\n",
      "__Correct answer__:\n",
      "In addition to accuracy, we can measure the 95th percentile response time and average cost per classification to assess the ticket classification system's performance and production-readiness.\n",
      "__Generated answer__:\n",
      "To measure the performance of the ticket classification system implemented using Claude beyond just accuracy, consider the following metrics and evaluation methods:\n",
      "\n",
      "1. **Precision, Recall, and F1 Score**: These metrics provide a more nuanced view of classification performance. Precision measures the accuracy of positive predictions, recall measures the ability to find all relevant instances, and the F1 score is the harmonic mean of precision and recall. This is especially important in cases where class distributions are imbalanced.\n",
      "\n",
      "2. **Cost Efficiency**: Evaluate the cost associated with the classification system, including operational costs, maintenance, and the cost of any misclassifications. This can help assess the economic viability of the system.\n",
      "\n",
      "3. **Speed and Latency**: Measure the response time of the classifier when processing tickets. Speed is crucial for user satisfaction and operational efficiency, especially in high-volume environments.\n",
      "\n",
      "4. **Interpretable Reasoning**: Since Claude can generate human-readable explanations for its classification decisions, assess the quality and clarity of these explanations. This can help build trust in the system and facilitate easier troubleshooting.\n",
      "\n",
      "5. **Adaptability to Changes**: Evaluate how well the system adapts to new classes or changes in class definitions without extensive relabeling. This can be measured by the time and effort required to implement changes and the system's performance post-change.\n",
      "\n",
      "6. **Load Testing**: Conduct load testing to ensure the system can handle the anticipated volume of tickets without performance degradation. This helps verify that the system remains responsive under real-world conditions.\n",
      "\n",
      "7. **Error Handling and Logging**: Implement and evaluate the effectiveness of error handling and logging mechanisms. This can provide insights into the robustness of the system and help identify areas for improvement.\n",
      "\n",
      "8. **User Feedback**: Collect feedback from users regarding the classification results and overall experience. This qualitative data can provide valuable insights into the system's effectiveness and areas for enhancement.\n",
      "\n",
      "9. **Monitoring and Alerting**: Set up monitoring systems to track performance metrics over time. This allows for proactive identification of issues and continuous improvement of the classification system.\n",
      "\n",
      "By using a combination of these metrics and evaluation methods, you can gain a comprehensive understanding of the performance of the ticket classification system beyond just accuracy.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of metrics and evaluation methods to measure the performance of the ticket classification system beyond just accuracy. It includes precision, recall, F1 score, cost efficiency, speed and latency, interpretable reasoning, adaptability to changes, load testing, error handling and logging, user feedback, and monitoring and alerting. However, it does not specifically mention measuring the 95th percentile response time, which is a critical piece of information included in the correct answer. Therefore, while the generated answer is informative, it lacks this specific metric, making it incomplete in comparison to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [00:58<07:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 484, Query:\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?\n",
      "__Correct answer__:\n",
      "With the Text Completions API, the system prompt is added as text before the first \"\\n\\nHuman:\" turn. With the Messages API, the system prompt is specified using the separate \"system\" parameter when making the API request.\n",
      "__Generated answer__:\n",
      "To specify a system prompt using the Text Completions API, you add the text before the first `\\n\\nHuman:` turn in your prompt. For example:\n",
      "\n",
      "```python\n",
      "prompt = \"Today is January 1, 2024.\\n\\nHuman: Hello, Claude\\n\\nAssistant:\"\n",
      "```\n",
      "\n",
      "In contrast, when using the Messages API, you specify the system prompt with the `system` parameter in the API call. For example:\n",
      "\n",
      "```python\n",
      "anthropic.Anthropic().messages.create(\n",
      "    model=\"claude-3-opus-20240229\",\n",
      "    max_tokens=1024,\n",
      "    system=\"Today is January 1, 2024.\",  # <-- system prompt\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "In summary, the Text Completions API requires the system prompt to be part of the raw string prompt, while the Messages API uses a dedicated parameter for the system prompt.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. It explains how to specify a system prompt using both the Text Completions API and the Messages API, including the correct method for each. The examples provided in the generated answer further clarify the instructions, and there are no critical pieces of information missing or contradictions present. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 705\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [01:05<07:47,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 670, Query:\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?\n",
      "__Correct answer__:\n",
      "You can combine XML tags like <thinking> and <answer> with chain of thought reasoning, where Claude explains its step-by-step reasoning process, to create structured, high-performance prompts. For example, you can prompt Claude to show its reasoning by including \"Before answering, explain your reasoning step-by-step in <thinking> tags.\" in the user message or system prompt.\n",
      "__Generated answer__:\n",
      "To combine XML tags with chain of thought reasoning for creating high-performance prompts for Claude, follow these guidelines:\n",
      "\n",
      "1. **Use XML Tags for Structure**: Clearly separate different parts of your prompt using XML tags. This enhances clarity and reduces the chances of misinterpretation by Claude. For example, you can use tags like `<task>`, `<subtask>`, `<input>`, and `<output>` to delineate various components of your prompt.\n",
      "\n",
      "2. **Identify and Break Down Subtasks**: Decompose your main task into distinct, sequential subtasks. Each subtask should have a single, clear objective. This helps in managing the complexity of the overall task.\n",
      "\n",
      "3. **Nest Tags for Hierarchical Content**: When dealing with complex information, nest XML tags to represent hierarchical relationships. For example, you can use `<outer><inner></inner></outer>` to show that one piece of information is a subset of another.\n",
      "\n",
      "4. **Incorporate Chain of Thought Reasoning**: Use tags like `<thinking>` to indicate the reasoning process and `<answer>` to denote the final response. This allows Claude to follow a logical progression in its reasoning, leading to more coherent and contextually relevant outputs.\n",
      "\n",
      "5. **Iterate and Refine**: After receiving outputs from Claude, refine your prompts based on its performance. Adjust the structure and content of your XML tags and the subtasks as necessary to improve clarity and effectiveness.\n",
      "\n",
      "6. **Be Consistent with Tag Names**: Maintain consistency in the tag names you use throughout your prompts. This helps Claude to better understand and process the information you provide.\n",
      "\n",
      "By combining these practices, you can create super-structured, high-performance prompts that leverage both XML tagging for clarity and organization, and chain of thought reasoning for logical coherence in responses.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides a comprehensive approach to combining XML tags with chain of thought reasoning for creating high-performance prompts for Claude. It includes the use of XML tags for structure, breaking down tasks, nesting tags, incorporating reasoning, iterating and refining prompts, and maintaining consistency in tag names. However, it does not explicitly mention the specific XML tags like <thinking> and <answer> as highlighted in the Correct Answer, which is a critical piece of information. Therefore, while the Generated Answer covers many relevant points, it lacks the specific mention of the tags that are essential for the prompt structure as per the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [01:14<09:07,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 448, Query:\n",
      "When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\n",
      "__Correct answer__:\n",
      "When evaluating the claude-3-haiku-20240307 model's performance on the 91 test samples, the three key metrics calculated are accuracy (89.01%), 95th percentile response time (1.61 seconds), and average cost per request routing ($0.0004).\n",
      "__Generated answer__:\n",
      "The three key metrics calculated when evaluating the Claude model's performance for ticket routing are:\n",
      "\n",
      "1. **Accuracy**: This measures how often the model's output exactly matches the correct classification.\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects the model's performance on both false positives and false negatives.\n",
      "3. **Consistency**: This assesses whether the model's outputs are consistent for similar inputs, indicating reliability in its predictions.\n",
      "\n",
      "The results for the claude-3-haiku-20240307 model on the 91 test samples are not provided in the documents, so specific performance results cannot be stated.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it lists different key metrics (accuracy, F1 Score, and consistency) compared to the Correct Answer, which specifies accuracy, 95th percentile response time, and average cost per request routing. Additionally, the Generated Answer states that the results for the model are not provided, which contradicts the Correct Answer that provides specific performance results. Therefore, critical information is missing and there are direct contradictions.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:18<08:05,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 423, Query:\n",
      "Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\n",
      "__Correct answer__:\n",
      "Before prompt engineering, Anthropic highly recommends having a clear definition of success criteria for your use case, some ways to empirically test against those criteria, and a first draft prompt you want to improve.\n",
      "__Generated answer__:\n",
      "Before starting to engineer and improve a prompt in Claude, Anthropic recommends having the following key things in place:\n",
      "\n",
      "1. **Understanding of Claude's Capabilities**: Familiarize yourself with Claude's capabilities and development flow to effectively utilize the model.\n",
      "\n",
      "2. **API Setup**: Ensure that you have installed the SDK, set your API key, and are able to make API calls. This is essential for interacting with Claude.\n",
      "\n",
      "3. **Access to Resources**: Utilize the Prompt Engineering Guide to master prompt crafting, the Prompt Library for inspiration from pre-crafted prompts, and the API Documentation for details on request formats and response handling.\n",
      "\n",
      "These steps will provide a solid foundation for optimizing Claude's performance through effective prompt engineering.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes additional recommendations such as understanding Claude's capabilities, setting up the API, and accessing resources, which are useful but do not directly address the key things Anthropic specifically recommends before prompt engineering. The correct answer emphasizes having a clear definition of success criteria, ways to test against those criteria, and a first draft prompt, which are critical elements missing from the generated answer. Therefore, the generated answer is not correct based on the correct answer provided.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:34<12:12,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 691, Query:\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?\n",
      "__Correct answer__:\n",
      "The Messages API allows you to continue a response by making the last input message have the \"assistant\" role, whereas the Text Completions API lets you pre-fill part of Claude's response directly in the prompt string.\n",
      "__Generated answer__:\n",
      "The Messages API handles mid-response prompting differently compared to the Text Completions API primarily in how inputs are structured and how responses are generated.\n",
      "\n",
      "In the Text Completions API, inputs are provided as raw strings that alternate between \"Human\" and \"Assistant\" prompts. For example, you might have a prompt like:\n",
      "\n",
      "```\n",
      "\"\\n\\nHuman: Hello there\\n\\nAssistant: Hi, I'm Claude. How can I help?\\n\\nHuman: Can you explain Glycolysis to me?\\n\\nAssistant:\"\n",
      "```\n",
      "\n",
      "In contrast, the Messages API uses a structured format where inputs are specified as a list of messages, each with a defined role (either \"user\" or \"assistant\") and content. For example:\n",
      "\n",
      "```python\n",
      "messages = [\n",
      "  {\"role\": \"user\", \"content\": \"Hello there.\"},\n",
      "  {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help?\"},\n",
      "  {\"role\": \"user\", \"content\": \"Can you explain Glycolysis to me?\"}\n",
      "]\n",
      "```\n",
      "\n",
      "When it comes to generating responses, the Text Completions API returns the generated text directly in the completion field of the response. For example:\n",
      "\n",
      "```python\n",
      "response.completion\n",
      "\" Hi, I'm Claude\"\n",
      "```\n",
      "\n",
      "On the other hand, the Messages API returns the response as a list of content blocks, which may contain multiple types of content. For example:\n",
      "\n",
      "```python\n",
      "response.content\n",
      "[{\"type\": \"text\", \"text\": \"Hi, I'm Claude\"}]\n",
      "```\n",
      "\n",
      "Additionally, both APIs allow for mid-response prompting, but they achieve this in different ways. In Text Completions, you can pre-fill part of the assistant's response by including it in the prompt. In the Messages API, you can achieve a similar effect by making the last input message have the assistant role, allowing the response to continue from that content.\n",
      "\n",
      "Overall, the key differences lie in the input structure (raw strings vs. structured messages) and the output format (direct text vs. content blocks).\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences between the Messages API and the Text Completions API in terms of input structure and output format. It also correctly mentions how mid-response prompting is handled in both APIs, aligning with the correct answer. Therefore, the generated answer is correct as it conveys the same essential information as the correct answer without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:40<10:54,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 560, Query:\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?\n",
      "__Correct answer__:\n",
      "When given the role of CFO through a system prompt, Claude provides a much more insightful, structured, and actionable financial analysis compared to not having a specific role. The role-based response breaks down key financial metrics, provides strategic commentary, and makes specific recommendations.\n",
      "__Generated answer__:\n",
      "Claude's response differs significantly when given a specific role through a system prompt compared to not having a defined role. When a role is assigned, such as \"You are a seasoned data scientist at a Fortune 500 company,\" Claude's performance is enhanced in several ways:\n",
      "\n",
      "1. **Enhanced Accuracy**: With a defined role, Claude can provide more accurate analyses tailored to the complexities of financial scenarios. This specificity allows Claude to better understand the context and nuances of the task.\n",
      "\n",
      "2. **Tailored Tone**: The communication style can be adjusted based on the role. For instance, a data scientist's response may be more technical and concise, while a marketing strategist might adopt a more persuasive and broad approach. This tailoring helps ensure that the response aligns with the expectations of the audience.\n",
      "\n",
      "3. **Improved Focus**: By setting a role context, Claude is more likely to stay within the bounds of the specific requirements of the task. This focus helps in delivering relevant insights and analyses that are aligned with the expectations of the role.\n",
      "\n",
      "In contrast, without a specific role, Claude may lack the necessary context to provide precise or relevant insights, leading to less accurate or generalized responses that may not meet the specific needs of the financial analysis task. Overall, role prompting significantly enhances Claude's ability to deliver targeted and effective responses in complex scenarios.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies that Claude's response is enhanced when given a specific role through a system prompt, highlighting aspects such as enhanced accuracy, tailored tone, and improved focus. It also contrasts this with the lack of specificity when no role is assigned, which aligns with the Correct Answer's emphasis on the structured and actionable nature of the analysis when a role is defined. While the Generated Answer provides additional details about the impact of role assignment, it does not contradict or omit any critical information from the Correct Answer. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:56<14:10, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 722, Query:\n",
      "What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\n",
      "__Correct answer__:\n",
      "Quantitative metrics for evaluating a sentiment analysis model include task-specific metrics like F1 score, as well as generic metrics like accuracy, precision, and recall. Specific targets should be based on industry benchmarks, prior experiments, AI research, or expert knowledge, and should represent an improvement over the current baseline.\n",
      "__Generated answer__:\n",
      "Quantitative metrics that can be used to measure the success of a sentiment analysis model include:\n",
      "\n",
      "1. **Accuracy**: The proportion of correct predictions made by the model compared to the total predictions. It is calculated as (Number of correct predictions) / (Total number of predictions).\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive instances and its ability to avoid false positives.\n",
      "\n",
      "3. **Precision**: The ratio of true positive predictions to the total predicted positives. It measures the accuracy of the positive predictions.\n",
      "\n",
      "4. **Recall**: The ratio of true positive predictions to the total actual positives. It assesses the model's ability to identify all relevant instances.\n",
      "\n",
      "5. **Response Time**: The time taken by the model to provide a prediction, measured in milliseconds. This is crucial for applications requiring real-time responses.\n",
      "\n",
      "6. **Uptime**: The percentage of time the model is operational and available for use, which is important for reliability.\n",
      "\n",
      "7. **Bias and Fairness Metrics**: These assess whether the model's predictions are biased against certain groups, ensuring fairness in classification.\n",
      "\n",
      "Targets for these metrics can be determined based on:\n",
      "\n",
      "- **Industry Benchmarks**: Research existing models and their performance in similar tasks to set realistic targets.\n",
      "  \n",
      "- **Prior Experiments**: Analyze the performance of earlier versions of the model or similar models to establish a baseline for improvement.\n",
      "\n",
      "- **Expert Knowledge**: Consult with domain experts to understand what constitutes acceptable performance in the specific context of sentiment analysis.\n",
      "\n",
      "- **User Needs**: Align targets with the expectations and requirements of the end-users, ensuring that the model meets their needs effectively.\n",
      "\n",
      "For example, a well-defined target for a sentiment analysis model might be to achieve an F1 score of at least 0.85 on a held-out test set of 10,000 diverse Twitter posts, which represents a 5% improvement over the current baseline. This target is specific, measurable, achievable, and relevant to the application.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several quantitative metrics for evaluating a sentiment analysis model, including accuracy, F1 score, precision, and recall, which align with the correct answer. Additionally, it includes metrics like response time, uptime, and bias and fairness metrics, which, while not mentioned in the correct answer, are relevant to the evaluation of a model's performance. The explanation for determining specific targets is also comprehensive and aligns with the guidance provided in the correct answer. Overall, the generated answer captures the essence of the correct answer and expands on it without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  19%|█▉        | 19/100 [01:58<10:50,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 338, Query:\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?\n",
      "__Correct answer__:\n",
      "Combining XML tags with other prompt engineering techniques like multishot prompting (using <examples> tags) or chain of thought (using <thinking> and <answer> tags) to create super-structured, high-performance prompts.\n",
      "__Generated answer__:\n",
      "A power user tip mentioned in the documentation for creating high-performance prompts using XML tags is to combine XML tags with other techniques like multishot prompting (`<examples>`) or chain of thought (`<thinking>`, `<answer>`). This approach creates super-structured, high-performance prompts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points of the correct answer, mentioning the combination of XML tags with multishot prompting and chain of thought techniques to create high-performance prompts. There are no critical pieces of information missing, and the substance of both answers is essentially the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [02:05<10:04,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 613, Query:\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?\n",
      "__Correct answer__:\n",
      "You can use an LLM like Claude to grade the outputs of other LLMs by providing it with the output to grade along with a detailed rubric. Instruct the LLM to think through its reasoning and then output a simple 'correct' or 'incorrect' result based on how well the output matches the criteria in the rubric.\n",
      "__Generated answer__:\n",
      "To use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric, you can follow these steps:\n",
      "\n",
      "1. **Define the Grading Rubric**: Establish clear criteria for evaluation that outlines what constitutes a good response. This rubric should include aspects such as relevance, coherence, completeness, and adherence to the prompt.\n",
      "\n",
      "2. **Craft Effective Prompts**: Design prompts that clearly communicate the grading criteria to Claude. This involves specifying what you want Claude to evaluate in the outputs, ensuring that the prompts maximize the evaluation scores.\n",
      "\n",
      "3. **Input the Outputs for Grading**: Feed the outputs generated by the other LLMs into Claude along with the rubric. Claude can process these inputs and apply the grading criteria to assess the quality of the responses.\n",
      "\n",
      "4. **Utilize Few-Shot Learning**: If you have limited labeled examples of graded outputs, leverage Claude's few-shot learning capabilities. Provide a few examples of graded responses to help Claude understand the grading context better.\n",
      "\n",
      "5. **Iterate and Refine**: After initial grading, review the results and refine your prompts or rubric as necessary. This iterative process can help improve the accuracy and reliability of the grading.\n",
      "\n",
      "6. **Access Resources**: Utilize resources such as the \"Evals cookbook\" for code examples and tips on LLM-based grading to enhance your approach.\n",
      "\n",
      "By following these steps, you can effectively use Claude to automate the grading of outputs from other LLMs, ensuring a consistent and objective evaluation process.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive and detailed approach to using an LLM like Claude for grading outputs from other LLMs, covering the definition of a grading rubric, crafting effective prompts, inputting outputs, utilizing few-shot learning, iterating and refining the process, and accessing additional resources. It aligns well with the correct answer's main points, including the emphasis on using a rubric and instructing Claude to evaluate outputs based on that rubric. Therefore, the generated answer is correct as it captures the essence of the correct answer without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 20/100 questions. Current Accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [02:11<09:12,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 572, Query:\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?\n",
      "__Correct answer__:\n",
      "To access Voyage embeddings on AWS, subscribe to the model package on AWS Marketplace, select the model to deploy, agree to the terms, and copy the Product ARN for your selected region. Then create a JupyterLab space in SageMaker Studio, upload Voyage's notebook, and follow the instructions to deploy the model package using the ARN.\n",
      "__Generated answer__:\n",
      "To access and deploy Voyage embeddings on AWS Marketplace, follow these steps:\n",
      "\n",
      "1. **Subscribe to the Model Package**:\n",
      "   - Navigate to the model package listing page for Voyage on AWS Marketplace.\n",
      "   - Select the model you wish to deploy.\n",
      "   - Click on the \"Continue to subscribe\" button.\n",
      "   - Review the details on the \"Subscribe to this software\" page. If you agree with the End-User License Agreement (EULA), pricing, and support terms, click on \"Accept Offer\".\n",
      "   - After selecting \"Continue to configuration\" and choosing a region, you will receive a Product ARN (Amazon Resource Name). This ARN is required for creating a deployable model using Boto3.\n",
      "\n",
      "2. **Deploy the Model Package**:\n",
      "   - Again, navigate to the model package listing page and select the model to deploy.\n",
      "   - Click on \"Continue to subscribe\" and review the details.\n",
      "   - Accept the offer as before and choose your region to obtain the Product ARN.\n",
      "   - Copy the ARN corresponding to your selected region.\n",
      "\n",
      "3. **Create a JupyterLab Space**:\n",
      "   - Create a JupyterLab space in SageMaker Studio.\n",
      "   - Upload Voyage’s notebook to this space.\n",
      "   - Follow the instructions provided within the notebook to complete the deployment process.\n",
      "\n",
      "By following these steps, you can successfully access and deploy Voyage embeddings on AWS Marketplace.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed step-by-step process for accessing and deploying Voyage embeddings on AWS Marketplace, which aligns closely with the correct answer. It includes subscribing to the model package, obtaining the Product ARN, creating a JupyterLab space in SageMaker Studio, and following the instructions in the notebook. Although there are some minor differences in wording and the order of steps, all critical pieces of information are present and accurately conveyed. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [02:18<09:08,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 608, Query:\n",
      "When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\n",
      "__Correct answer__:\n",
      "When using tools to get JSON output, you should provide a single tool, set the tool_choice to explicitly instruct the model to use that tool, and ensure the tool name and description are from the model's perspective since it will pass the input to the tool.\n",
      "__Generated answer__:\n",
      "To get Claude to produce JSON output following a particular schema using tools, you should focus on the following key aspects of tool setup and prompting:\n",
      "\n",
      "1. **Define Tools Clearly**: When setting up the tools, provide clear names, descriptions, and input schemas in your API request. This helps Claude understand what each tool does and how to use it effectively.\n",
      "\n",
      "2. **Use Specific Prompts**: Include a user prompt that explicitly requests JSON output in the desired schema. For example, you might say, \"Please provide the data in the following JSON format: [insert schema].\"\n",
      "\n",
      "3. **Utilize Tool Capabilities**: Remember that tools can be used not just for client-side functions but also to format output. You can create a tool specifically for generating JSON output that adheres to your schema.\n",
      "\n",
      "4. **Monitor Tool Use**: After Claude decides to use a tool, ensure that you extract the tool name and input from Claude’s request correctly. Execute the tool code on your end and return the results to Claude.\n",
      "\n",
      "5. **Refine Responses**: Claude will analyze the tool results to formulate a response. Make sure the tool results are structured correctly so that Claude can generate the desired JSON output.\n",
      "\n",
      "6. **Iterate and Test**: Continuously refine your prompts and tool definitions based on the output you receive. Testing different prompts and tool configurations can help improve the accuracy of the JSON output.\n",
      "\n",
      "By following these steps, you can effectively set up tools and prompts to ensure Claude produces the required JSON output in the specified schema.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of the steps to set up tools and prompts for obtaining JSON output from Claude. It emphasizes defining tools clearly, using specific prompts, and monitoring tool use, which aligns with the key points in the correct answer. However, it does not explicitly mention the importance of providing a single tool and setting the tool_choice to instruct the model to use that tool, which is a critical piece of information from the correct answer. Therefore, while the generated answer contains relevant information, it is missing a key aspect, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [02:24<08:54,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 650, Query:\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?\n",
      "__Correct answer__:\n",
      "The Claude 3 Haiku model has vision capabilities, is faster, more performant, and more intelligent than the legacy Claude Instant 1.2 model. Claude 3 Haiku also has more up-to-date training data.\n",
      "__Generated answer__:\n",
      "The key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance are as follows:\n",
      "\n",
      "1. **Intelligence and Performance**: Claude 3 Haiku is part of the Claude 3 family, which is designed to be more intelligent and capable compared to legacy models like Claude Instant 1.2. The legacy model is noted to perform less well than the Claude 3 models.\n",
      "\n",
      "2. **Speed and Latency**: Claude 3 Haiku is described as the fastest and most compact model in the Claude 3 lineup, providing near-instant responsiveness. In contrast, Claude Instant 1.2, while fast, is generally slower than the Claude 3 models.\n",
      "\n",
      "3. **Multilingual Capabilities**: Both models support multilingual functionality, but Claude 3 Haiku offers better coverage, understanding, and skill in this area compared to Claude Instant 1.2.\n",
      "\n",
      "4. **Vision Capabilities**: Claude 3 Haiku includes vision capabilities, which are absent in Claude Instant 1.2.\n",
      "\n",
      "5. **Context Window and Output**: Both models have a maximum output of 4096 tokens and a context window of 100K tokens, but Claude 3 Haiku benefits from advancements in model architecture that enhance its overall performance.\n",
      "\n",
      "6. **Cost**: The cost structure differs significantly, with Claude 3 Haiku being more cost-effective at $0.25 / $1.25 per million tokens compared to Claude Instant 1.2, which is priced at $0.80 / $2.40.\n",
      "\n",
      "Overall, Claude 3 Haiku represents a significant upgrade over Claude Instant 1.2 in terms of intelligence, speed, multilingual capabilities, and additional features like vision.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model, including improvements in intelligence, speed, multilingual capabilities, and the inclusion of vision capabilities. It also mentions the cost differences, which adds valuable context. While the generated answer includes more detail than the correct answer, it does not contradict any of the information provided in the correct answer. Therefore, the generated answer is correct in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [02:27<07:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 297, Query:\n",
      "What is one key benefit of using examples when prompt engineering with Claude?\n",
      "__Correct answer__:\n",
      "One key benefit of using examples in prompts is that they reduce misinterpretation of instructions, leading to more accurate outputs from Claude.\n",
      "__Generated answer__:\n",
      "One key benefit of using examples when prompt engineering with Claude is that they reduce misinterpretation of instructions. This helps ensure that the prompts are understood correctly, leading to more accurate and effective responses from Claude.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key benefit of using examples in prompt engineering with Claude, stating that they reduce misinterpretation of instructions and lead to more accurate responses. It captures the essence of the correct answer without omitting any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [02:30<06:08,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 362, Query:\n",
      "According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\n",
      "__Correct answer__:\n",
      "Prompt engineering allows you to easily adapt AI models to new domains by providing domain-specific context directly in the prompts, without needing to retrain the model through fine-tuning.\n",
      "__Generated answer__:\n",
      "One key advantage of using prompt engineering instead of fine-tuning is that prompt engineering is far faster and more resource-efficient. It allows for nearly instantaneous results and requires only text input, while fine-tuning necessitates high-end GPUs, large memory, and can take hours or even days to implement. This makes prompt engineering a more agile approach for adapting AI models to new domains or tasks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that prompt engineering is faster and more resource-efficient compared to fine-tuning, which aligns with the essence of the correct answer. However, it does not explicitly mention the aspect of providing domain-specific context directly in the prompts, which is a key point in the correct answer. Therefore, while the generated answer captures some advantages of prompt engineering, it lacks a critical piece of information regarding the provision of context, making it incomplete.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [02:34<05:36,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 358, Query:\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?\n",
      "__Correct answer__:\n",
      "You can make a copy of Anthropic's provided Claude for Sheets workbook template to quickly get started using the extension with your own work.\n",
      "__Generated answer__:\n",
      "To quickly get started using the Claude for Sheets extension with a pre-made template, you can make a copy of the Claude for Sheets workbook template provided by Anthropic. This template serves as a starting point for integrating the Claude AI model into your spreadsheet applications and workflows. \n",
      "\n",
      "You can find the template and instructions on how to copy it in the Claude for Sheets documentation or the Google Workspace Marketplace overview page. Once you have the template copied, you can begin customizing it for your specific needs.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately conveys the main idea of the Correct Answer, stating that you can make a copy of the Claude for Sheets workbook template provided by Anthropic to get started. It also adds helpful context about the template serving as a starting point for integrating the Claude AI model into spreadsheet applications. Additionally, it mentions where to find the template and instructions, which is relevant information. Therefore, it does not miss any critical pieces of information and aligns well with the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [02:37<05:04,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 390, Query:\n",
      "How does the \"index\" field in the \"content_block_delta\" event relate to the text being streamed in a response?\n",
      "__Correct answer__:\n",
      "The \"index\" field in each \"content_block_delta\" event indicates which content block the text delta applies to. Multiple deltas with the same index consecutively stream the text for a single content block in the response.\n",
      "__Generated answer__:\n",
      "The \"index\" field in the \"content_block_delta\" event indicates the position of the content block within the overall message structure. Each content block is associated with an index that corresponds to its place in the final Message content array. The \"content_block_delta\" events are used to update the content of these blocks, and the \"index\" helps to identify which specific block is being modified. This allows for precise updates to the text being streamed in a response, ensuring that changes are applied to the correct part of the message.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains the role of the \"index\" field in the \"content_block_delta\" event, noting that it identifies which content block is being modified. It also mentions that the \"content_block_delta\" events are used to update the content of these blocks, which aligns with the correct answer's emphasis on multiple deltas streaming text for a single content block. Therefore, the generated answer captures the essential information and intent of the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [02:43<05:43,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 414, Query:\n",
      "How can you include an image as part of a Claude API request, and what image formats are currently supported?\n",
      "__Correct answer__:\n",
      "To include an image in a Claude API request, provide it as a base64-encoded image in an \"image\" content block within the \"messages\" array. The currently supported image formats are JPEG, PNG, GIF, and WebP.\n",
      "__Generated answer__:\n",
      "To include an image as part of a Claude API request, you need to upload the image file directly in the request. Claude supports the following image formats: JPEG, PNG, GIF, and WebP. Specifically, the supported MIME types are:\n",
      "\n",
      "- image/jpeg\n",
      "- image/png\n",
      "- image/gif\n",
      "- image/webp\n",
      "\n",
      "Keep in mind that Claude cannot read image URLs, so you must upload the image file itself rather than providing a link. Additionally, there are limits on the image file size: a maximum of 5MB per image for API requests. You can include up to 20 images in a single request.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a different method for including images in a Claude API request, stating that the image file must be uploaded directly, while the correct answer specifies that the image should be base64-encoded within an \"image\" content block in the \"messages\" array. Additionally, the generated answer includes information about MIME types, file size limits, and the number of images allowed in a request, which are not mentioned in the correct answer. However, the critical method of including the image is incorrect, leading to a contradiction. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [02:48<05:40,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 451, Query:\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?\n",
      "__Correct answer__:\n",
      "TTFT is a specific measure of latency that captures the time it takes for a language model to generate the first token of its response after receiving a prompt. It is an important component of a model's overall latency and responsiveness, especially for interactive applications.\n",
      "__Generated answer__:\n",
      "The relationship between Time to First Token (TTFT) and latency in evaluating a language model's performance is that both metrics measure aspects of the model's responsiveness. TTFT specifically measures the time taken to generate the first token of output after receiving a prompt, while latency refers to the overall time taken for the model to respond to the prompt.\n",
      "\n",
      "A lower TTFT indicates that the model can start generating a response more quickly, which contributes to lower latency. Therefore, TTFT can be seen as a component of overall latency, particularly important in interactive applications where quick initial feedback is essential. Both metrics are influenced by similar factors, including model size, hardware capabilities, network conditions, and the complexity of the prompt. In summary, TTFT is a specific measure of responsiveness that directly impacts the overall latency experienced by users.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the relationship between Time to First Token (TTFT) and latency, emphasizing that TTFT is a specific measure of latency that reflects the time taken to generate the first token after receiving a prompt. It also correctly notes that TTFT contributes to overall latency and is particularly important for interactive applications. The explanation of how both metrics are influenced by similar factors is also relevant. Therefore, the generated answer captures the essential points made in the correct answer without omitting any critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [02:56<06:48,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 620, Query:\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?\n",
      "__Correct answer__:\n",
      "Providing edge case examples to Claude in the prompt can meaningfully improve its performance in correctly routing support tickets in scenarios where it may otherwise misclassify them, such as implicit requests, emotional prioritization, ambiguous intent vs. routing, or issue prioritization.\n",
      "__Generated answer__:\n",
      "Providing Claude with examples of handling edge cases like implicit requests and emotional prioritization can significantly enhance its performance in routing support tickets. \n",
      "\n",
      "1. **Implicit Requests**: Customers often communicate their needs indirectly. By training Claude with examples of such implicit requests, it can learn to recognize phrases that indicate a customer's underlying needs, such as inquiries about order status or service updates. This understanding allows Claude to route tickets more accurately to the appropriate department or team, ensuring timely responses to customer concerns.\n",
      "\n",
      "2. **Emotional Prioritization**: Customers may express dissatisfaction or frustration, which can be critical to address. By providing Claude with guidelines on when to prioritize emotional responses over technical solutions, it can better assess the urgency of a ticket. This helps in routing tickets not just based on the issue at hand but also considering the emotional state of the customer, leading to improved customer satisfaction.\n",
      "\n",
      "3. **Intent vs. Routing**: Sometimes, Claude may identify a customer's intent correctly but route the ticket incorrectly. By clarifying the appropriate routes for certain intents through examples, Claude can improve its routing accuracy, ensuring that tickets reach the right teams without unnecessary delays.\n",
      "\n",
      "4. **Issue Prioritization**: When customers present multiple issues, Claude may struggle to identify the primary concern. Providing examples that clarify how to prioritize different issues can help Claude focus on the most critical problem first, leading to more effective resolutions.\n",
      "\n",
      "Regularly reviewing and refining these examples as the system evolves is essential to maintain and enhance Claude's accuracy and efficiency in ticket routing. This iterative process ensures that Claude adapts to changing customer needs and improves its overall performance in handling support tickets.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by discussing how providing examples of handling edge cases like implicit requests and emotional prioritization can enhance Claude's performance in routing support tickets. It elaborates on various aspects such as intent vs. routing and issue prioritization, which align with the points made in the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 30/100 questions. Current Accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [03:04<07:18,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 623, Query:\n",
      "How does the stop_reason of \"tool_use\" relate to the overall workflow of integrating external tools with Claude?\n",
      "__Correct answer__:\n",
      "When Claude determines that one of the user-provided tools can help answer the user's query, it constructs a tool use request. This causes the API response to have a stop_reason of \"tool_use\", signaling Claude's intent to use the tool. The user must then extract the tool input from Claude's request, run the actual tool code client-side, and continue the conversation by sending the tool results back to Claude.\n",
      "__Generated answer__:\n",
      "The stop_reason of \"tool_use\" indicates that Claude has determined a specific external tool is necessary to address the user's query effectively. This is a crucial part of the workflow for integrating external tools with Claude, as it signals that Claude has assessed the user prompt and identified that the available tools can provide a more accurate or relevant response than it could generate on its own.\n",
      "\n",
      "In the overall workflow, the integration process involves several steps:\n",
      "\n",
      "1. **Providing Tools and User Prompt**: Users must define the tools available to Claude, including their names, descriptions, and input schemas, alongside a user prompt that may require these tools.\n",
      "\n",
      "2. **Claude Decides to Use a Tool**: Upon receiving the user prompt, Claude evaluates whether any of the provided tools can assist. If it identifies a suitable tool, it constructs a tool use request, which results in an API response with the stop_reason of \"tool_use.\"\n",
      "\n",
      "3. **Extracting Tool Input and Running Code**: The user then extracts the tool name and input from Claude's request, executes the tool code on their end, and continues the conversation with the results.\n",
      "\n",
      "4. **Formulating a Response**: Finally, Claude uses the results from the tool to craft its response to the original user prompt.\n",
      "\n",
      "This workflow emphasizes the importance of the \"tool_use\" stop_reason as it signifies the transition from Claude's internal processing to utilizing external capabilities, ensuring that the response is informed by the most relevant data or functionality available through the tools provided by the user.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the role of the \"tool_use\" stop_reason in the workflow of integrating external tools with Claude. It explains that Claude determines the necessity of a tool based on the user prompt and constructs a tool use request, which results in the \"tool_use\" stop_reason. It also outlines the subsequent steps, including extracting tool input, running the tool, and formulating a response. All critical pieces of information from the Correct Answer are present in the Generated Answer, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [03:07<05:55,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 325, Query:\n",
      "According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\n",
      "__Correct answer__:\n",
      "During periods of high usage, an overloaded_error event may be sent in the event stream, which would normally correspond to an HTTP 529 error code in a non-streaming context.\n",
      "__Generated answer__:\n",
      "During periods of high usage for the Anthropic API when using streaming responses, an \"overloaded_error\" may be sent, which corresponds to an HTTP error code of 529 in a non-streaming context.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. Both answers mention that during periods of high usage, an \"overloaded_error\" may be sent, and both specify that this corresponds to an HTTP error code of 529 in a non-streaming context. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [03:11<05:34,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 314, Query:\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?\n",
      "__Correct answer__:\n",
      "The two types of deltas that can be contained in a content_block_delta event are text_delta and input_json_delta.\n",
      "__Generated answer__:\n",
      "The two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API are modifications that describe different ways the content block can be updated, such as inserting, deleting, or replacing text.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not explicitly mention the two specific types of deltas, which are text_delta and input_json_delta, as stated in the correct answer. Instead, it describes the types of modifications in a general sense without naming them. Therefore, it is missing critical information that is essential to the question.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [03:14<04:48,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 364, Query:\n",
      "On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\n",
      "__Correct answer__:\n",
      "Claude 3.5 Sonnet became generally available across those platforms on June 20th, 2024, while tool use became generally available on May 30th, 2024.\n",
      "__Generated answer__:\n",
      "Claude 3.5 Sonnet became generally available on June 20th, 2024, while tool use became generally available on May 30th, 2024. Therefore, both Claude 3.5 Sonnet and tool use became generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI on June 20th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states the dates for when Claude 3.5 Sonnet and tool use became generally available, but it incorrectly implies that both became available on the same date (June 20th, 2024). The correct answer specifies that tool use became available earlier on May 30th, 2024. This critical piece of information is missing from the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [03:17<04:25,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 308, Query:\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?\n",
      "__Correct answer__:\n",
      "Anthropic launched Claude.ai and the Claude iOS app in Europe in May 2024, and then launched them in Canada the following month in June 2024.\n",
      "__Generated answer__:\n",
      "Claude.ai and the Claude iOS app were launched in Europe on May 13th, 2024, followed by their launch in Canada on June 5th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states the order of the launches, indicating that Claude.ai and the Claude iOS app were launched in Europe first in May 2024, followed by their launch in Canada in June 2024. Although it provides specific dates, the essential information about the order and timing of the launches is consistent with the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [03:24<05:07,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 456, Query:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", what does this indicate and what should be done next to continue the conversation?\n",
      "__Correct answer__:\n",
      "A stop_reason of \"tool_use\" signals that Claude has decided to use a tool and has constructed a formatted tool use request. To continue the conversation, the tool name and input should be extracted from Claude's request, the actual tool code should be executed client-side, and then a new user message containing a tool_result content block should be sent to Claude.\n",
      "__Generated answer__:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", it indicates that Claude has determined that a tool is necessary to assist with the user's query. The next steps to continue the conversation are as follows:\n",
      "\n",
      "1. Extract the tool name and input from Claude's request.\n",
      "2. Execute the tool code on your end using the extracted input.\n",
      "3. Return the results of the tool execution in a new user message, formatted as a tool_result content block. This block should include the tool results so Claude can use them to formulate its final response.\n",
      "\n",
      "By following these steps, you can ensure that the conversation continues smoothly and that Claude has the necessary information to provide a complete answer.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer. It explains that a stop_reason of \"tool_use\" indicates that Claude has determined a tool is necessary and outlines the steps to continue the conversation, including extracting the tool name and input, executing the tool code, and returning the results in a formatted content block. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [03:36<07:20,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 421, Query:\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?\n",
      "__Correct answer__:\n",
      "The example code snippet for evaluating tone and style in a customer service chatbot uses the anthropic Python library to interact with the Claude AI model.\n",
      "__Generated answer__:\n",
      "The Python libraries used in the example code snippet for evaluating tone and style in a customer service chatbot include:\n",
      "\n",
      "1. `anthropic` - This library is used to interact with the Anthropic API for generating responses and evaluating the tone of customer service interactions.\n",
      "2. `numpy` - This library is used for numerical operations, particularly in the cosine similarity evaluation.\n",
      "3. `sentence_transformers` - This library is used to create sentence embeddings for evaluating the consistency of responses based on cosine similarity.\n",
      "4. `rouge` - This library is used for evaluating the quality of generated summaries in the context of relevance and coherence.\n",
      "\n",
      "These libraries facilitate various evaluation methods, including Likert scale assessments for tone and style, cosine similarity for consistency, and ROUGE-L for summarization quality.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes additional libraries (`numpy`, `sentence_transformers`, and `rouge`) that are not mentioned in the correct answer. While it does correctly identify the `anthropic` library, the inclusion of other libraries suggests a broader scope than what was specified in the correct answer. Since the correct answer focuses solely on the `anthropic` library for interacting with the Claude AI model, the generated answer introduces critical information that diverges from the original context. Therefore, it is deemed incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [03:40<06:24,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 393, Query:\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?\n",
      "__Correct answer__:\n",
      "The two main ways to authenticate are: 1) Directly providing the aws_access_key, aws_secret_key, and optionally aws_session_token, or 2) Using the default AWS credential providers, such as the ~/.aws/credentials file or the AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID environment variables.\n",
      "__Generated answer__:\n",
      "The two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock are:\n",
      "\n",
      "1. **Using API Keys**: Users need to create an account and obtain API keys, which are used for authentication when making requests to the Claude models.\n",
      "\n",
      "2. **Using AWS Credentials**: Since the SDK supports AWS services, users can also authenticate using AWS credentials, which can be managed through AWS Identity and Access Management (IAM).\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it introduces the concept of API keys as a method of authentication, which is not mentioned in the correct answer. The correct answer specifies two main ways to authenticate: providing AWS access keys directly or using default AWS credential providers. The generated answer fails to accurately reflect the authentication methods as outlined in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [03:45<05:51,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 405, Query:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\n",
      "__Correct answer__:\n",
      "When deciding to use leak-resistant prompt engineering, the potential reduction in prompt leaks should be balanced against the risk of degraded model performance due to the added complexity of the prompt.\n",
      "__Generated answer__:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, two key factors to consider and balance are:\n",
      "\n",
      "1. **Complexity vs. Performance**: Implementing leak-resistant techniques can add complexity to the prompt, which may degrade the overall performance of the model. It's essential to evaluate whether the added complexity will negatively impact the model's outputs and the quality of its performance.\n",
      "\n",
      "2. **Necessity vs. Alternatives**: Consider whether leak-resistant strategies are absolutely necessary for your use case. Before resorting to these techniques, it may be beneficial to explore simpler monitoring methods, such as output screening and post-processing, to address prompt leaks without complicating the prompt engineering process.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the need to balance complexity against performance, which aligns with the correct answer. However, it introduces an additional factor, \"Necessity vs. Alternatives,\" which is not mentioned in the correct answer. The correct answer focuses solely on the balance between the potential reduction in prompt leaks and the risk of degraded model performance due to added complexity. Since the generated answer introduces a critical piece of information that is not part of the correct answer, it diverges from the original intent and completeness of the response. Therefore, the generated answer is not correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [03:50<05:28,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 481, Query:\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?\n",
      "__Correct answer__:\n",
      "Choosing the right Claude model that best fits your needs in terms of speed and output quality is one of the most straightforward ways to reduce latency in your application. Anthropic offers a range of Claude models with different capabilities and performance characteristics to allow you to choose the optimal balance of intelligence, speed, and cost for your use case.\n",
      "__Generated answer__:\n",
      "Selecting the appropriate Claude model based on your specific requirements can significantly reduce latency in your application. Different models offered by Anthropic have varying capabilities and performance characteristics, which means that some models are optimized for speed while others may prioritize output quality or complex reasoning.\n",
      "\n",
      "For instance, if your application involves classification tasks that require quick responses, opting for a smaller model like Claude 3 Haiku is advisable due to its speed and efficiency. In contrast, if your tasks demand specialized knowledge or complex reasoning, models like Sonnet or Opus may be more suitable, albeit potentially at the cost of increased latency.\n",
      "\n",
      "By carefully choosing the model that aligns with your specific use case—balancing the need for speed against the complexity of the tasks—you can optimize performance and minimize latency. Evaluating the models based on their metrics will help ensure that you select the most appropriate one for your needs, ultimately leading to a more responsive application.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the essence of the correct answer by emphasizing the importance of selecting the appropriate Claude model to reduce latency based on specific requirements. It discusses the trade-offs between speed and output quality, which aligns with the correct answer's focus on choosing the optimal balance for the use case. Additionally, it provides examples of different models and their suitability for various tasks, which enhances the explanation without contradicting the original statement. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 40/100 questions. Current Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [03:56<05:37,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 500, Query:\n",
      "How can you stream responses from the Anthropic API using the Python SDK?\n",
      "__Correct answer__:\n",
      "You can stream responses from the Anthropic API using the Python SDK by using the client.messages.stream() method and iterating over the stream.text_stream attribute in a for loop.\n",
      "__Generated answer__:\n",
      "To stream responses from the Anthropic API using the Python SDK, you can utilize the `messages.stream` method provided by the SDK. Here’s a sample code snippet demonstrating how to do this:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "# Initialize the Anthropic client\n",
      "client = anthropic.Anthropic()\n",
      "\n",
      "# Stream messages\n",
      "with client.messages.stream(\n",
      "    max_tokens=1024,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      ") as stream:\n",
      "    for text in stream.text_stream:\n",
      "        print(text, end=\"\", flush=True)\n",
      "```\n",
      "\n",
      "In this example, the `max_tokens` parameter specifies the maximum number of tokens to generate, and the `messages` parameter contains the input message from the user. The model specified is \"claude-3-5-sonnet-20240620\". The `for` loop iterates over the streamed text responses, printing them in real-time. \n",
      "\n",
      "Make sure to set your API key either by exporting the `ANTHROPIC_API_KEY` environment variable or by supplying it directly when initializing the client.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to stream responses from the Anthropic API using the Python SDK by mentioning the `messages.stream` method and iterating over the `text_stream` attribute. It also provides a relevant code snippet that illustrates the process, including parameters like `max_tokens` and `messages`, and mentions the need to set the API key, which is important for functionality. Therefore, it aligns well with the correct answer and includes all necessary information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [04:00<04:56,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 390, Query:\n",
      "How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\n",
      "__Correct answer__:\n",
      "You can shape Claude's response by pre-filling part of it in the last position of the input messages list. To get a short response like a single multiple choice answer, you can set the \"max_tokens\" parameter to a small value like 1.\n",
      "__Generated answer__:\n",
      "You can guide Claude's response by pre-filling part of the response using either Text Completions or Messages. In Text Completions, you set the prompt to start with the assistant's response. In Messages, you achieve this by making the last input message have the assistant role, allowing the response to continue from that content.\n",
      "\n",
      "To generate a short response, you can use the API parameter `max_tokens` to limit the length of the response.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to guide Claude's response by pre-filling part of it, mentioning both Text Completions and Messages, which aligns with the correct answer's intent. It also correctly states that the `max_tokens` parameter can be used to generate a short response. However, it does not explicitly mention that the pre-filled content should be in the last position of the input messages list, which is a critical detail from the correct answer. Therefore, while the generated answer is mostly correct, it is missing a key piece of information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [04:04<04:33,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 410, Query:\n",
      "What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\n",
      "__Correct answer__:\n",
      "When building an eval set, it is better to prioritize having a larger volume of test cases with slightly lower signal automated grading over having fewer questions with high-quality human hand-grading.\n",
      "__Generated answer__:\n",
      "When building an eval set for an AI system, having a larger number of test cases with automated grading is generally more important than having fewer high-quality test cases graded by humans. The principles suggest prioritizing volume over quality, as more questions with slightly lower signal from automated grading can provide a broader assessment of the AI's capabilities. Automated grading is favored for its speed, reliability, and scalability, while human grading, although high-quality, is slow and expensive and should be avoided if possible. Therefore, a larger set of automated test cases is preferred to ensure comprehensive evaluation and coverage of various scenarios, including edge cases.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the essence of the correct answer by emphasizing the importance of having a larger number of test cases with automated grading over fewer high-quality human-graded cases. It also correctly highlights the advantages of automated grading, such as speed, reliability, and scalability, which aligns with the principles mentioned in the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [04:11<05:01,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 336, Query:\n",
      "What are the two required fields in a content_block_delta event for a text delta type?\n",
      "__Correct answer__:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\", where the \"delta\" field contains a \"type\" of \"text_delta\" and the \"text\" being added.\n",
      "__Generated answer__:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\". The \"delta\" field itself must contain a \"type\" (which should be \"text_delta\") and a \"text\" field that represents the updated text.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly identifies the two required fields \"index\" and \"delta\" in a content_block_delta event for a text delta type. It also accurately states that the \"delta\" field must contain a \"type\" of \"text_delta\" and a \"text\" field representing the updated text. Therefore, it conveys the same essential information as the Correct Answer without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [04:14<04:24,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 390, Query:\n",
      "What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\n",
      "__Correct answer__:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks demonstrating how to upload PDFs, generate embeddings, and more. The Developer Console offers a prompt generator tool for easier, more powerful prompting.\n",
      "__Generated answer__:\n",
      "Two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings, are:\n",
      "\n",
      "1. **Anthropic Cookbook**: This resource includes interactive Jupyter notebooks that demonstrate how to upload PDFs, generate embeddings, and more. It provides hands-on examples that allow users to experiment with Claude's functionalities in a practical setting.\n",
      "\n",
      "2. **Developer Console**: This tool offers a browser-based Workbench and prompt generator, enabling users to engage in easier and more powerful prompting. It allows for real-time experimentation and exploration of Claude's capabilities directly in the browser.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two interactive ways to learn about Claude's capabilities: the Anthropic Cookbook and the Developer Console. It provides similar details about the Cookbook's interactive Jupyter notebooks and the Developer Console's prompt generator tool. Although there are minor wording differences, the essential information is preserved, and no critical pieces of information are missing. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [04:20<04:43,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 562, Query:\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?\n",
      "__Correct answer__:\n",
      "Breaking a task into distinct subtasks for chained prompts improves Claude's accuracy because each subtask gets Claude's full attention, reducing errors compared to tackling the entire complex task at once.\n",
      "__Generated answer__:\n",
      "Breaking a task into distinct subtasks for chained prompts helps improve Claude's accuracy on the overall task for several reasons:\n",
      "\n",
      "1. **Focused Attention**: Each subtask allows Claude to concentrate fully on a specific aspect of the task, which reduces the likelihood of errors. When tasks are complex, dividing them into simpler components minimizes the cognitive load on the model, leading to more accurate outputs.\n",
      "\n",
      "2. **Clarity of Instructions**: Simpler subtasks result in clearer instructions and expectations. This clarity helps ensure that the model understands what is required at each step, further enhancing the accuracy of its responses.\n",
      "\n",
      "3. **Traceability and Debugging**: Chaining prompts makes it easier to trace the workflow and identify where issues may arise. If a particular subtask produces an error, it can be pinpointed and addressed without having to analyze the entire task at once.\n",
      "\n",
      "4. **Reduction of Inconsistency Errors**: By breaking down complex tasks, the chances of inconsistency errors are reduced. Each subtask can be handled in a consistent manner, which is particularly important in scaled workflows.\n",
      "\n",
      "5. **Structured Thinking**: Allowing Claude to think through problems in a structured way leads to more coherent and well-organized responses. This structured approach helps in reducing errors, especially in tasks that require logical reasoning or complex analysis.\n",
      "\n",
      "Overall, these factors contribute to improved accuracy and quality in Claude's outputs when tasks are divided into manageable subtasks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains that breaking a task into distinct subtasks allows Claude to focus on specific aspects, which reduces errors and enhances accuracy. It also mentions clarity of instructions, traceability, reduction of inconsistency errors, and structured thinking, all of which align with the reasoning provided in the correct answer. Therefore, the generated answer captures the essence of the correct answer and adds further detail without contradicting it.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [04:28<05:15,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 521, Query:\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?\n",
      "__Correct answer__:\n",
      "Messages streaming responses can contain multiple content blocks of varying types, making the streaming format more complex compared to Text Completions which only include completion, ping, and error server-sent-events.\n",
      "__Generated answer__:\n",
      "The streaming format for Messages responses differs from Text Completions streaming responses in several ways:\n",
      "\n",
      "1. **Complexity of Content**: The Messages streaming format is more complex because it can contain multiple content blocks of varying types, whereas the Text Completions streaming format primarily includes completion, ping, and error server-sent-events.\n",
      "\n",
      "2. **Event Structure**: In the Messages streaming format, the response consists of a series of events, including `message_start`, `content_block_start`, `content_block_delta`, `content_block_stop`, `message_delta`, and `message_stop`. There may also be ping events. In contrast, the Text Completions streaming format does not specify this detailed event structure.\n",
      "\n",
      "3. **Input Specification**: For Text Completions, inputs are provided as raw strings, while for Messages, inputs are specified as a list of messages, each with a role (user or assistant) and content.\n",
      "\n",
      "4. **Output Format**: The output from Text Completions is returned in the completion values, while the output from Messages is a list of content blocks, which may include various types of content.\n",
      "\n",
      "These differences highlight the more intricate nature of the Messages streaming format compared to the simpler Text Completions format.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the key differences between the Messages streaming format and the Text Completions streaming format, including the complexity of content, event structure, input specification, and output format. It aligns with the Correct Answer in emphasizing the complexity of Messages responses due to multiple content blocks. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [04:34<05:09,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 329, Query:\n",
      "What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\n",
      "__Correct answer__:\n",
      "According to the documentation, users can start experimenting with Claude by visiting claude.ai or using Anthropic's web Console.\n",
      "__Generated answer__:\n",
      "Two ways to start experimenting with Claude as a user, according to Anthropic's documentation, are:\n",
      "\n",
      "1. Check out the quickstart guide for step-by-step instructions on how to create an account, obtain API keys, and start interacting with the models.\n",
      "2. Use the web Console at claude.ai to start experimenting with Claude right away.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two ways to start experimenting with Claude: checking out the quickstart guide and using the web Console at claude.ai. While the mention of the quickstart guide is a useful addition, it does not replace the direct reference to visiting claude.ai as a way to start experimenting, which is included in the correct answer. Therefore, the generated answer is correct in substance as it aligns with the essence of the correct answer, even though it includes additional information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [04:39<04:50,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 455, Query:\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?\n",
      "__Correct answer__:\n",
      "Chain prompts break complex tasks into smaller subtasks, allowing Claude to give its full attention to each one. This reduces errors and inconsistencies that may occur when trying to handle a complex workflow all at once.\n",
      "__Generated answer__:\n",
      "Using chain prompts can significantly reduce errors and inconsistencies in complex tasks handled by Claude by breaking down these tasks into smaller, manageable subtasks. This approach allows Claude to focus on each individual step, which enhances accuracy and minimizes the likelihood of errors. \n",
      "\n",
      "By simplifying the tasks, clarity is improved, leading to clearer instructions and outputs. Additionally, the structure of chain prompts provides traceability, making it easier to identify and resolve issues within the prompt chain. If a specific step is missed or performed poorly, it can be isolated and fine-tuned without the need to redo the entire task, further enhancing the efficiency of the process.\n",
      "\n",
      "Moreover, techniques such as specifying the desired output format, pre-filling responses, constraining with examples, and using retrieval for contextual consistency contribute to reducing inconsistencies and hallucinations in Claude's responses. Overall, chaining prompts fosters a more organized and reliable workflow for complex tasks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how chain prompts break down complex tasks into smaller subtasks, allowing Claude to focus on each step, which reduces errors and inconsistencies. It also adds valuable details about improving clarity, traceability, and techniques that contribute to reducing inconsistencies and hallucinations, which are relevant to the topic. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [04:42<03:57,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 285, Query:\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?\n",
      "__Correct answer__:\n",
      "In a non-streaming context, an overloaded_error event would normally correspond to an HTTP 529 status code.\n",
      "__Generated answer__:\n",
      "An overloaded_error event corresponds to an HTTP status code 529 in a non-streaming context for the Anthropic API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately states that an overloaded_error event corresponds to an HTTP status code 529 in a non-streaming context for the Anthropic API, which is consistent with the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 50/100 questions. Current Accuracy: 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [04:48<04:10,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 351, Query:\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?\n",
      "__Correct answer__:\n",
      "When making a request to Voyage AI's embedding endpoint, you can either leave the encoding_format parameter unspecified to get the embeddings as lists of floating-point numbers, or set encoding_format to \"base64\" to get the embeddings compressed to Base64 encodings.\n",
      "__Generated answer__:\n",
      "The two ways to specify the format in which Voyage AI returns embeddings through its HTTP API are:\n",
      "\n",
      "1. By default (if not specified), the embeddings are represented as lists of floating-point numbers.\n",
      "2. By setting the `encoding_format` parameter to \"base64\", the embeddings are compressed to Base64 encodings.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two ways to specify the format for embeddings in Voyage AI's HTTP API. It states that if the `encoding_format` parameter is not specified, the embeddings are returned as lists of floating-point numbers, and it also mentions setting the `encoding_format` to \"base64\" for Base64 encodings. This aligns with the correct answer, and there are no critical pieces of information missing or contradictions present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [04:54<04:18,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 512, Query:\n",
      "When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\n",
      "__Correct answer__:\n",
      "When streaming requests with tool use, the input JSON deltas for tool_use content blocks are sent as partial JSON strings in multiple content_block_delta events. The client can accumulate these partial JSON strings and parse the complete JSON object once a content_block_stop event is received, using a library like Pydantic for partial JSON parsing or helpers provided in Anthropic's SDKs.\n",
      "__Generated answer__:\n",
      "Input JSON deltas for tool_use content blocks are sent as partial JSON strings through multiple `content_block_delta` events. Each delta corresponds to updates for the input field of the tool_use block. The final `tool_use.input` is always an object, while the deltas are designed to provide maximum granularity by allowing for incremental updates.\n",
      "\n",
      "Clients can accumulate these string deltas until they receive a `content_block_stop` event. At that point, they can parse the accumulated JSON using libraries like Pydantic or by utilizing the provided SDKs, which offer helpers for accessing parsed incremental values.\n",
      "\n",
      "The sequence of events in a streaming API response includes a `message_start` event, followed by one or more content blocks that contain `content_block_start`, multiple `content_block_delta` events, and a `content_block_stop` event. After processing the deltas, the client can construct the final input object from the accumulated partial JSON strings.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the process of sending input JSON deltas for tool_use content blocks as partial JSON strings through multiple `content_block_delta` events. It also correctly states that clients can accumulate these deltas until receiving a `content_block_stop` event and parse the accumulated JSON using libraries like Pydantic or SDK helpers. Additionally, it outlines the sequence of events in a streaming API response, which aligns with the correct answer. Therefore, the generated answer captures all critical information and is consistent with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [04:58<04:00,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 406, Query:\n",
      "What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\n",
      "__Correct answer__:\n",
      "Anthropic offers a GitHub prompting tutorial that covers prompt engineering concepts in-depth with examples, and a lighter-weight Google Sheets prompting tutorial that utilizes Claude for Sheets.\n",
      "__Generated answer__:\n",
      "Anthropic offers two interactive prompt engineering tutorials: \n",
      "\n",
      "1. **GitHub Prompting Tutorial**: This is an example-filled tutorial that covers the prompt engineering concepts found in their documentation. It is designed for users who prefer a more detailed and structured approach to learning about prompt engineering.\n",
      "\n",
      "2. **Google Sheets Prompting Tutorial**: This is a lighter-weight version of the prompt engineering tutorial that is delivered via an interactive spreadsheet. It provides a more accessible and simplified way to learn the concepts of prompt engineering.\n",
      "\n",
      "The main difference between the two tutorials lies in their format and depth: the GitHub tutorial is more comprehensive and example-driven, while the Google Sheets tutorial offers a more straightforward and interactive experience.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the two interactive prompt engineering tutorials offered by Anthropic, including the GitHub Prompting Tutorial and the Google Sheets Prompting Tutorial. It correctly identifies the GitHub tutorial as being more comprehensive and example-driven, while the Google Sheets tutorial is presented as a lighter-weight and more accessible option. Therefore, the substance of the answer aligns with the Correct Answer, and there are no critical pieces of information missing or contradictions present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [05:13<06:05,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 623, Query:\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?\n",
      "__Correct answer__:\n",
      "Claude offers a 200K token context window, tool use for integration into specialized applications, multimodal input capabilities for richer context, and is uniquely positioned to serve high-trust industries processing large volumes of sensitive data with enterprise-grade security and data handling.\n",
      "__Generated answer__:\n",
      "Claude is suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data due to several key capabilities:\n",
      "\n",
      "1. **Enterprise-Grade Security**: Claude is built with SOC II Type 2 certification and offers HIPAA compliance options, ensuring secure handling of sensitive data.\n",
      "\n",
      "2. **Trustworthiness**: It is designed to be resistant to jailbreaks and misuse, with continuous monitoring for harmful use cases, making it reliable for high-trust industries.\n",
      "\n",
      "3. **Scalability**: Claude can handle large volumes of data efficiently, with a 200K token context window that allows for processing extensive documents, and future support for a 1M token context window.\n",
      "\n",
      "4. **Multimodal Input Capabilities**: It can process and analyze visual inputs alongside text, enabling richer context and more complex use cases, such as extracting insights from images and generating code from diagrams.\n",
      "\n",
      "5. **Tool Use and Integration**: Claude supports function calling, allowing seamless integration into specialized applications and custom workflows, which is essential for enterprise environments.\n",
      "\n",
      "6. **Developer Tools**: It includes a Developer Console with a Workbench and prompt generation tools, facilitating easier and more powerful experimentation and development.\n",
      "\n",
      "7. **Low Hallucination Rates**: Claude demonstrates very low rates of hallucination, ensuring high accuracy and reliability, particularly important for critical enterprise applications.\n",
      "\n",
      "8. **Broad Language Support**: It is fluent in multiple languages, enhancing its utility in global enterprises and enabling features like automatic translation.\n",
      "\n",
      "These capabilities collectively make Claude a robust solution for enterprises looking to integrate AI into their operations while ensuring security, reliability, and scalability.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several key capabilities of Claude that make it suitable for enterprise use cases, including enterprise-grade security, trustworthiness, scalability with a 200K token context window, multimodal input capabilities, tool use for integration, and low hallucination rates. It also mentions the Developer Tools and broad language support, which, while not explicitly stated in the correct answer, do not contradict it. The generated answer captures the essence of the correct answer and adds additional relevant details, making it comprehensive. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [05:15<04:40,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 292, Query:\n",
      "As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\n",
      "__Correct answer__:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in the United States, Canada, and Europe.\n",
      "__Generated answer__:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in Canada and Europe.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is missing the information that the Claude.ai API and iOS app are also available in the United States, which is a critical piece of information compared to the correct answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [05:19<04:11,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 503, Query:\n",
      "What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\n",
      "__Correct answer__:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are push-based using webhooks, and pull-based. The push-based approach is more web-scalable but requires exposing a public endpoint which has IT security implications. The pull-based approach is easier to implement but makes unnecessary calls to the support ticket system.\n",
      "__Generated answer__:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are:\n",
      "\n",
      "1. **Push-Based Approach**: In this method, the support ticket system (e.g., Zendesk) triggers the integration by sending a webhook event to the routing service when a new ticket is created. This approach is more scalable because it allows for real-time processing of tickets as they are created, minimizing delays in ticket handling. However, it requires exposing a public endpoint, which may have IT security implications.\n",
      "\n",
      "2. **Pull-Based Approach**: This method involves the code periodically checking for new tickets at scheduled intervals. While this approach is easier to implement since it does not require setting up a public endpoint, it can lead to unnecessary calls to the support ticket system, making it less efficient and scalable compared to the push-based approach.\n",
      "\n",
      "In summary, the push-based approach offers better scalability but poses security challenges, while the pull-based approach is simpler to implement but less efficient due to potential redundant API calls.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the two main approaches for integrating Claude into a support ticket workflow, including the push-based and pull-based methods. It correctly highlights the scalability and implementation differences, noting that the push-based approach is more scalable but has security implications, while the pull-based approach is easier to implement but less efficient due to unnecessary calls. Therefore, the Generated Answer is consistent with the Correct Answer in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [05:22<03:24,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 293, Query:\n",
      "When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\n",
      "__Correct answer__:\n",
      "On May 10th, 2024, Anthropic released a prompt generator tool that is available through the Developer Console.\n",
      "__Generated answer__:\n",
      "Anthropic released the prompt generator tool on May 10th, 2024. It is available through the Developer Console.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating both the release date of the prompt generator tool and its availability through the Developer Console. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [05:25<03:04,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 376, Query:\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?\n",
      "__Correct answer__:\n",
      "The Claude 3 Sonnet model balances intelligence and speed, making it well-suited for high-throughput tasks like sales forecasting and targeted marketing.\n",
      "__Generated answer__:\n",
      "The Claude 3 model that provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing is the **claude-3-haiku-20240307**. This model is noted for being the fastest and most cost-effective option in the Claude 3 family, making it ideal for tasks that require quick responses and efficient processing. However, if the tasks require deep subject matter expertise or highly complex reasoning, the **claude-3-sonnet** model may be considered despite its higher cost.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly identifies the **claude-3-haiku-20240307** model as the best balance of intelligence and speed for high-throughput tasks, whereas the correct answer states that the **claude-3-sonnet** model is the one that provides this balance. This is a critical piece of information that is missing from the generated answer, and it directly contradicts the correct answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [05:32<03:25,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 391, Query:\n",
      "How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\n",
      "__Correct answer__:\n",
      "You can calculate the similarity between two Voyage embedding vectors using the dot product, which is equivalent to cosine similarity since Voyage embeddings are normalized to length 1.\n",
      "__Generated answer__:\n",
      "To calculate the similarity between two Voyage embedding vectors, you can use cosine similarity, which is a common choice for this purpose. Since Voyage embeddings are normalized to length 1, the cosine similarity is equivalent to the dot product of the two vectors. You can compute this in Python using the following code snippet:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "similarity = np.dot(embd1, embd2)\n",
      "```\n",
      "\n",
      "This approach leverages the fact that for normalized vectors, the cosine of the angle between them (cosine similarity) is the same as their dot product.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can calculate the similarity between two Voyage embedding vectors using cosine similarity and that this is equivalent to the dot product since the embeddings are normalized to length 1. It also provides a Python code snippet for calculating the dot product, which is relevant and helpful. Therefore, the generated answer conveys the same essential information as the correct answer without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [05:39<03:44,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 566, Query:\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?\n",
      "__Correct answer__:\n",
      "Well-chosen examples in prompts can boost Claude's ability to handle complex tasks by reducing misinterpretation of instructions, enforcing consistent structure and style, and serving as a guide for the desired output.\n",
      "__Generated answer__:\n",
      "Using examples in prompts can significantly enhance Claude's performance on complex tasks in several ways:\n",
      "\n",
      "1. **Reduction of Misinterpretation**: Examples clarify instructions, helping to minimize misunderstandings that could lead to incorrect outputs. By providing concrete instances of what is expected, Claude can better grasp the nuances of the task.\n",
      "\n",
      "2. **Enforcement of Consistency**: Including examples establishes a uniform structure and style for responses. This consistency aids Claude in maintaining a coherent approach across different tasks, which is particularly beneficial in complex scenarios where uniformity is crucial.\n",
      "\n",
      "3. **Boosting Performance on Complex Tasks**: Well-chosen examples can enhance Claude's ability to manage intricate tasks by providing context and illustrating the desired outcome. This is especially important for tasks that may involve multiple layers of interpretation or require nuanced understanding.\n",
      "\n",
      "4. **Adapting to Common Scenarios**: By incorporating examples that address common challenges, such as implicit requests or emotional prioritization, Claude can learn to navigate these situations more effectively. This targeted approach helps Claude to better identify and respond to customer needs.\n",
      "\n",
      "5. **Iterative Improvement**: Regularly refining prompts with targeted examples allows for continuous performance enhancement. Utilizing tools like vector databases to retrieve relevant examples can provide Claude with additional context, leading to significant improvements in accuracy, as evidenced by case studies showing increases in performance metrics.\n",
      "\n",
      "Overall, the strategic use of examples in prompts not only clarifies expectations but also equips Claude with the necessary context to excel in handling complex tasks.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by discussing how examples in prompts can reduce misinterpretation, enforce consistency, and provide context for complex tasks. It also expands on the benefits by mentioning adaptation to common scenarios and iterative improvement, which are relevant and enhance the understanding of the topic. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 60/100 questions. Current Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [05:43<03:15,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 409, Query:\n",
      "What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\n",
      "__Correct answer__:\n",
      "When streaming responses with tool use, the two types of content block deltas are text deltas and input JSON deltas. Text deltas contain a \"text\" field with a string of the incrementally generated text. Input JSON deltas contain a \"partial_json\" field with a string containing part of the JSON object specifying the tool's input.\n",
      "__Generated answer__:\n",
      "The two types of content block deltas that can be emitted when streaming responses with tool use are:\n",
      "\n",
      "1. **Insertion Delta**: This delta type contains information that indicates new content is being added to the content block at a specified index.\n",
      "\n",
      "2. **Deletion Delta**: This delta type contains information that indicates existing content is being removed from the content block at a specified index.\n",
      "\n",
      "Each content_block_delta event updates the content block at a given index, reflecting these modifications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it describes two types of deltas (Insertion Delta and Deletion Delta) that are not mentioned in the Correct Answer. The Correct Answer specifies the two types of content block deltas as text deltas and input JSON deltas, along with their specific contents. The Generated Answer does not align with this information and introduces entirely different terms and concepts.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [05:47<02:59,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 390, Query:\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?\n",
      "__Correct answer__:\n",
      "Claude's question answering and text analysis capabilities enable it to build intelligent, interactive systems like chatbots and personalize user experiences by understanding sentiment and preferences.\n",
      "__Generated answer__:\n",
      "Two key capabilities of Claude that enable it to build interactive systems and personalized user experiences are:\n",
      "\n",
      "1. **Text and Code Generation**: Claude can summarize text, answer questions, extract data, translate text, and generate code. This capability allows for the creation of intelligent, interactive systems such as customer support chatbots and educational AI tutors, which can engage users effectively and provide tailored responses.\n",
      "\n",
      "2. **Text Analysis and Recommendations**: Claude can understand sentiment, preferences, and patterns from user interactions, which helps in personalizing user experiences and offerings. This capability is crucial for creating systems that adapt to individual user needs and enhance engagement.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two key capabilities of Claude: text and code generation, and text analysis and recommendations. These capabilities align with the correct answer's mention of question answering and text analysis, as they both contribute to building interactive systems and personalizing user experiences. The generated answer expands on the functionalities but does not omit any critical information. Therefore, it is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [05:52<03:08,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 512, Query:\n",
      "What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\n",
      "__Correct answer__:\n",
      "A raw HTTP stream response includes a message_start event, followed by one or more content blocks (each with a content_block_start, content_block_delta events, and content_block_stop), a message_delta event, and a final message_stop event. Ping events may also be dispersed throughout.\n",
      "__Generated answer__:\n",
      "The key event types included in a raw HTTP stream response when using message streaming are:\n",
      "\n",
      "1. **message_start**: This event indicates the start of a message and contains a Message object with empty content.\n",
      "2. **content_block_start**: This event marks the beginning of a content block.\n",
      "3. **content_block_delta**: There can be multiple of these events within a content block, indicating changes or additions to the content.\n",
      "4. **content_block_stop**: This event signifies the end of a content block.\n",
      "5. **message_delta**: This event indicates top-level changes to the final Message object.\n",
      "6. **message_stop**: This event marks the end of the message.\n",
      "\n",
      "Additionally, there may be **ping** events dispersed throughout the response. \n",
      "\n",
      "The typical order of these events is as follows:\n",
      "- message_start\n",
      "- content_block_start\n",
      "- (multiple content_block_delta events)\n",
      "- content_block_stop\n",
      "- (one or more message_delta events)\n",
      "- message_stop\n",
      "\n",
      "This sequence outlines the flow of events in a streaming response.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key event types included in a raw HTTP stream response and their typical order, as outlined in the correct answer. It mentions all the necessary events: message_start, content_block_start, content_block_delta, content_block_stop, message_delta, and message_stop, along with the presence of ping events. The sequence of events is also correctly described. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [05:58<03:11,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 314, Query:\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?\n",
      "__Correct answer__:\n",
      "The Messages API allows including up to 20 images per request, while the claude.ai interface has a lower limit of up to 5 images per turn.\n",
      "__Generated answer__:\n",
      "The maximum number of images that can be included in a single request using the Anthropic API is 20, while the claude.ai interface allows for up to 5 images per turn.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating that the Anthropic API allows for up to 20 images per request and the claude.ai interface allows for up to 5 images per turn. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [06:02<02:43,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 346, Query:\n",
      "When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\n",
      "__Correct answer__:\n",
      "If Claude's response hits the max_tokens limit and has an incomplete tool use block, you should retry the request with a higher max_tokens value to get Claude's full response including the complete tool use.\n",
      "__Generated answer__:\n",
      "If Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, you should retry the request with a higher max_tokens value to get the full tool use. This will allow you to capture the complete tool use request that Claude intended to make.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the correct answer by stating that if Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, one should retry the request with a higher max_tokens value to obtain the full tool use. Both answers convey the same essential information without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [06:04<02:17,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 297, Query:\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?\n",
      "__Correct answer__:\n",
      "Before running a classification evaluation on Claude, you need to 1) develop your test cases, and 2) take a look at Anthropic's guide to developing test cases.\n",
      "__Generated answer__:\n",
      "The two steps needed before running a classification evaluation on Claude are:\n",
      "\n",
      "1. Build a strong input prompt.\n",
      "2. Develop your test cases.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it omits the critical step of taking a look at Anthropic's guide to developing test cases, which is mentioned in the correct answer. While it includes the step of developing test cases, it introduces a different step about building a strong input prompt that is not part of the correct answer. Therefore, it does not fully align with the requirements stated in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [06:10<02:30,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 455, Query:\n",
      "How can you use the content parameter in the messages list to influence Claude's response?\n",
      "__Correct answer__:\n",
      "You can provide content in the last position of the messages list, with the \"assistant\" role, to pre-fill part of Claude's response. This allows you to shape the assistant's output.\n",
      "__Generated answer__:\n",
      "You can use the content parameter in the messages list to influence Claude's response by pre-filling part of the response with specific text. By setting the last message in the conversation to have the assistant role and providing desired content, you can dictate how Claude continues the conversation. For example, if you want Claude to respond with a specific phrase or idea, you can structure the messages like this:\n",
      "\n",
      "```json\n",
      "messages = [\n",
      "  {\"role\": \"human\", \"content\": \"What do you think about the weather today?\"},\n",
      "  {\"role\": \"assistant\", \"content\": \"The weather is nice, and I think it would be great to go for a walk.\"}\n",
      "]\n",
      "```\n",
      "\n",
      "In this case, Claude will continue from the assistant's last message, allowing you to guide the conversation in a specific direction. This method is useful for simulating dialogues or ensuring that Claude's responses align with your intended context.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how to use the content parameter in the messages list to influence Claude's response by pre-filling part of the response with specific text. It mentions setting the last message to have the assistant role and providing desired content, which aligns with the correct answer. Both answers convey the same essential information about guiding Claude's responses. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [06:15<02:26,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 421, Query:\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?\n",
      "__Correct answer__:\n",
      "Compared to fine-tuning, prompt engineering is far more effective at helping models understand and utilize external content like retrieved documents. Prompt engineering also preserves the model's broad general knowledge, while fine-tuning risks catastrophic forgetting where the model loses its general capabilities.\n",
      "__Generated answer__:\n",
      "Two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation are:\n",
      "\n",
      "1. **Comprehension Improvements**: Prompt engineering is more effective at helping models better understand and utilize external content, such as retrieved documents. This allows for enhanced performance in tasks that require comprehension of specific information without altering the underlying model.\n",
      "\n",
      "2. **Preservation of General Knowledge**: Fine-tuning can lead to catastrophic forgetting, where the model loses its broad general knowledge due to the focus on specific tasks. In contrast, prompt engineering maintains the model's wide-ranging capabilities, ensuring that general knowledge is preserved while still allowing for task-specific adaptations.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the two key advantages of prompt engineering over fine-tuning as stated in the correct answer. It discusses the effectiveness of prompt engineering in helping models understand and utilize external content, as well as the preservation of general knowledge compared to the risks associated with fine-tuning. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [06:17<02:04,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 342, Query:\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?\n",
      "__Correct answer__:\n",
      "To get started making requests to Claude models on Anthropic's Bedrock API, you need to: 1) Install and configure the AWS CLI, and 2) Install an SDK for accessing Bedrock, such as the Python SDK shown in the example code.\n",
      "__Generated answer__:\n",
      "The two main steps to get started with making requests to Claude models on Anthropic's Bedrock API are:\n",
      "\n",
      "1. Follow the Quickstart guide to make your first API call.\n",
      "2. Explore the API Reference for detailed information on using the API.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not accurately reflect the two main steps outlined in the correct answer. While it mentions making an API call and exploring the API Reference, it omits the critical steps of installing and configuring the AWS CLI and installing an SDK for accessing Bedrock. Therefore, it lacks essential information necessary to get started with making requests to Claude models on Anthropic's Bedrock API.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 70/100 [06:21<01:55,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 395, Query:\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?\n",
      "__Correct answer__:\n",
      "You can list the available Claude models in a specific AWS region by running the command `aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`, replacing `<region>` with the desired AWS region such as `us-west-2`.\n",
      "__Generated answer__:\n",
      "To check which Claude models are available in a specific AWS region using the AWS CLI, you can use the following command:\n",
      "\n",
      "```bash\n",
      "aws bedrock list-foundation-models --region=<your-region> --by-provider=anthropic --query \"modelSummaries[*].modelId\"\n",
      "```\n",
      "\n",
      "Replace `<your-region>` with the desired AWS region, such as `us-west-2`. This command will list all the available Claude models provided by Anthropic in that region.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer correctly conveys the essential information needed to check which Claude models are available in a specific AWS region using the AWS CLI. It includes the necessary command and instructions to replace the placeholder with the desired region, which aligns with the Correct Answer. The minor differences in wording do not affect the substance of the answer. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 70/100 questions. Current Accuracy: 0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [06:24<01:49,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 340, Query:\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?\n",
      "__Correct answer__:\n",
      "The input_type argument can be passed with a value of \"query\" or \"document\" to specify the type of input text being embedded.\n",
      "__Generated answer__:\n",
      "The argument that can be passed to the `voyageai.Client.embed()` method or the Voyage HTTP API to specify whether the input text is a query or a document is `input_type`. This argument can be set to either `\"query\"` or `\"document\"`. If it is not specified, it defaults to `None`.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the argument `input_type` and states that it can be set to either `\"query\"` or `\"document\"`, which aligns with the correct answer. Additionally, it mentions the default value of `None` if not specified, which is an extra detail but does not contradict the correct answer. Therefore, the generated answer is correct in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [06:30<02:01,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 541, Query:\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?\n",
      "__Correct answer__:\n",
      "Tool_use content block deltas contain partial JSON strings for the input field, whereas text content block deltas directly contain the text delta. Tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time.\n",
      "__Generated answer__:\n",
      "The streaming API delta formats for tool_use content blocks and text content blocks differ primarily in their structure and purpose.\n",
      "\n",
      "1. **Tool_use Content Blocks**:\n",
      "   - The deltas for tool_use content blocks correspond to updates for the input field of the block. These deltas are represented as partial JSON strings, allowing for maximum granularity in updates. The final output for a tool_use content block is always an object.\n",
      "   - The format includes multiple `content_block_delta` events that contain a `partial_json` field, which is a string representing a portion of the JSON input. Clients can accumulate these string deltas and parse the JSON once they receive a `content_block_stop` event.\n",
      "\n",
      "2. **Text Content Blocks**:\n",
      "   - The streaming format for text completions includes events such as completion, ping, and error server-sent-events. The response can contain multiple content blocks of varying types, making the streaming format more complex.\n",
      "   - Unlike tool_use content blocks, the text content blocks do not specifically focus on partial JSON updates but rather on delivering text completions and related events.\n",
      "\n",
      "In summary, tool_use content blocks utilize partial JSON strings for input updates, while text content blocks focus on delivering text responses and events, leading to a more complex structure in the latter.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences between tool_use content blocks and text content blocks in terms of their delta formats. It mentions that tool_use content blocks use partial JSON strings and that the final output is an object, which aligns with the correct answer. It also correctly states that text content blocks focus on delivering text completions and related events, which reflects the essence of the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [06:33<01:50,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 326, Query:\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?\n",
      "__Correct answer__:\n",
      "When uploading images to Claude, the API has a maximum file size limit of 5MB per image, while on claude.ai the limit is 10MB per image.\n",
      "__Generated answer__:\n",
      "The image file size limits for uploading images to Claude are as follows:\n",
      "\n",
      "- When using the API: Maximum 5MB per image.\n",
      "- When using claude.ai: Maximum 10MB per image.\n",
      "\n",
      "Images larger than these limits will be rejected and return an error.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer regarding the image file size limits for uploading images to Claude via the API and on claude.ai. It states the same limits of 5MB for the API and 10MB for claude.ai, and it also includes the additional information that images larger than these limits will be rejected and return an error, which is consistent with the context. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [06:37<01:44,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 337, Query:\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?\n",
      "__Correct answer__:\n",
      "When selecting a Claude model for an enterprise use case that requires low latency, it's important to choose the model that best balances speed and output quality based on the specific requirements of the use case.\n",
      "__Generated answer__:\n",
      "One key consideration when selecting a Claude model for an enterprise use case that needs low latency is to choose a smaller model, such as Claude 3 Haiku, which is typically ideal due to its speed and efficiency. This allows for faster response times while still meeting the necessary performance requirements for the specific tasks at hand.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the need to choose a model that balances speed and efficiency for low latency use cases, which aligns with the correct answer's emphasis on balancing speed and output quality. However, it specifically mentions selecting a smaller model, Claude 3 Haiku, which may not universally apply to all use cases as the correct answer suggests a broader consideration based on specific requirements. This specificity could lead to a misunderstanding that only smaller models are suitable for low latency, which is not necessarily true. Therefore, while the generated answer captures part of the essence, it lacks the broader context provided in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [06:40<01:30,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 332, Query:\n",
      "What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\n",
      "__Correct answer__:\n",
      "For code retrieval, Voyage AI recommends using the voyage-code-2 embedding model, which they claim performs 17% better than alternatives and achieves state-of-the-art results on general-purpose corpora as well.\n",
      "__Generated answer__:\n",
      "Anthropic recommends the voyage-code-2 embedding model for code retrieval, which is optimized for this purpose and claims to be 17% better than alternatives. According to Voyage AI, this model also achieves state-of-the-art performance on general-purpose corpora.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer, stating that the voyage-code-2 embedding model is recommended for code retrieval and that it performs 17% better than alternatives while achieving state-of-the-art results on general-purpose corpora. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [06:46<01:46,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 355, Query:\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?\n",
      "__Correct answer__:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks that demonstrate how to upload PDFs and work with embeddings to help developers learn to use Anthropic's APIs.\n",
      "__Generated answer__:\n",
      "The Anthropic Cookbook can help developers learn to use Anthropic's APIs in two ways:\n",
      "\n",
      "1. It provides interactive Jupyter notebooks that demonstrate various functionalities, such as uploading PDFs and working with embeddings, allowing developers to see practical examples of how to use the APIs effectively.\n",
      "\n",
      "2. The Cookbook serves as a resource for exploring different use cases and techniques, helping developers understand how to optimize their interactions with the Claude AI model through hands-on learning and experimentation.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that the Anthropic Cookbook provides interactive Jupyter notebooks that demonstrate functionalities like uploading PDFs and working with embeddings. Additionally, it expands on the idea by mentioning that the Cookbook serves as a resource for exploring different use cases and techniques, which aligns with the purpose of helping developers learn to use the APIs. Therefore, the generated answer captures the essence of the correct answer and adds relevant information without contradicting it.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [06:52<01:50,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 486, Query:\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?\n",
      "__Correct answer__:\n",
      "The size of the context window determines how much retrieved information can be passed to the language model to augment its knowledge when generating a response using RAG. A larger context window allows more relevant retrieved information to be utilized by the model, improving the accuracy and groundedness of the generated text.\n",
      "__Generated answer__:\n",
      "The size of the context window significantly impacts a language model's ability to utilize retrieval augmented generation (RAG). A larger context window allows the model to incorporate more retrieved information from an external knowledge base along with the original query, enabling it to generate more accurate and relevant responses. This is because the model can reference a greater amount of context, which helps maintain coherence and relevance over longer interactions or more complex prompts.\n",
      "\n",
      "Conversely, a smaller context window may limit the amount of retrieved information that can be effectively utilized, potentially leading to less coherent outputs and a reduced ability to ground responses in the retrieved evidence. This limitation can hinder the model's performance, especially in tasks requiring up-to-date information or detailed domain-specific knowledge, as it may not be able to consider all relevant data when generating text. Thus, the effectiveness of RAG is closely tied to the context window size, as it determines how much information the model can process and integrate into its responses.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer regarding the impact of context window size on the effectiveness of retrieval augmented generation (RAG). Both answers emphasize that a larger context window allows for more retrieved information to be incorporated, leading to improved accuracy and coherence in responses. The generated answer also correctly notes the limitations of a smaller context window, which aligns with the correct answer's assertion about the importance of context window size in utilizing RAG effectively. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [07:04<02:31,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 532, Query:\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?\n",
      "__Correct answer__:\n",
      "The Evaluation tool helps identify edge cases where prompts might falter, allows rating individual results to determine prompt performance, ensures consistent performance across inputs, and enables prompt refinement for better reliability. Reviewing results across test cases helps spot patterns to make informed adjustments that lead to more robust AI applications.\n",
      "__Generated answer__:\n",
      "The Evaluation tool in Anthropic's Claude platform can significantly enhance prompt development and the robustness of AI applications in several ways:\n",
      "\n",
      "1. **Identifying Edge Cases**: The tool helps users pinpoint specific scenarios where prompts may not perform as expected, allowing for targeted improvements.\n",
      "\n",
      "2. **Rating Results**: Users can evaluate individual outputs from the model, which aids in understanding where prompts yield better or worse results. This feedback loop is crucial for refining prompts.\n",
      "\n",
      "3. **Ensuring Consistency**: By testing prompts across a variety of inputs, the Evaluation tool ensures that performance remains stable and reliable, which is essential for building trust in AI applications.\n",
      "\n",
      "4. **Refining Prompts**: The tool facilitates the iterative process of prompt refinement, enabling users to make informed adjustments based on observed patterns in the results.\n",
      "\n",
      "5. **Informed Adjustments**: By reviewing results across different test cases, users can spot trends and make data-driven decisions to enhance the effectiveness of their prompts.\n",
      "\n",
      "Overall, the Evaluation tool supports a systematic approach to prompt engineering, leading to more reliable and effective AI applications. Additionally, users are encouraged to provide feedback to improve the tool further, as it is currently in beta.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the key points from the Correct Answer regarding the Evaluation tool's functionality. It discusses identifying edge cases, rating results, ensuring consistency, refining prompts, and making informed adjustments, all of which align with the Correct Answer's content. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [07:07<02:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 276, Query:\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?\n",
      "__Correct answer__:\n",
      "The Claude 3 Haiku model has the fastest comparative latency\n",
      "__Generated answer__:\n",
      "The Claude model with the fastest comparative latency is the Claude 3 Haiku, which is described as the \"Fastest and most compact model for near-instant responsiveness.\"\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the Claude 3 Haiku model as having the fastest comparative latency, which aligns with the correct answer. It also includes additional context about the model being described as the \"Fastest and most compact model for near-instant responsiveness,\" which does not contradict the correct answer. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [07:14<02:03,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 652, Query:\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?\n",
      "__Correct answer__:\n",
      "To have a multi-turn conversation using the Anthropic Messages API in Python, send the full conversation history in the messages parameter each time, including any prior user and assistant messages. The API is stateless, so the entire context must be provided with each request.\n",
      "__Generated answer__:\n",
      "To build up a conversation with multiple turns using the Anthropic Messages API in Python, you need to maintain the full conversational history and send it with each request. The API is stateless, meaning you must include all previous messages in the `messages` list.\n",
      "\n",
      "Here’s an example of how to implement this in Python:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "# Initialize the client with your API key\n",
      "client = anthropic.Anthropic(api_key=\"my_api_key\")\n",
      "\n",
      "# Start with an initial conversation history\n",
      "conversation_history = [\n",
      "    {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n",
      "]\n",
      "\n",
      "# First message from the user\n",
      "response = client.messages.create(\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      "    max_tokens=1024,\n",
      "    messages=conversation_history\n",
      ")\n",
      "\n",
      "# Print the assistant's response\n",
      "print(\"Claude:\", response.content[0]['text'])\n",
      "\n",
      "# Add the assistant's response to the conversation history\n",
      "conversation_history.append({\"role\": \"assistant\", \"content\": response.content[0]['text']})\n",
      "\n",
      "# Next user message\n",
      "conversation_history.append({\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"})\n",
      "\n",
      "# Send the updated conversation history\n",
      "response = client.messages.create(\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      "    max_tokens=1024,\n",
      "    messages=conversation_history\n",
      ")\n",
      "\n",
      "# Print the assistant's response\n",
      "print(\"Claude:\", response.content[0]['text'])\n",
      "\n",
      "# Continue adding messages and responses as needed\n",
      "```\n",
      "\n",
      "In this example, the conversation starts with a greeting. After each interaction, the assistant's response is appended to the conversation history, which is then sent back to the API for the next user input. This way, you can build a multi-turn conversation.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by stating that the full conversational history must be maintained and sent with each request, emphasizing the stateless nature of the API. It also provides a practical example that aligns with the explanation given in the Correct Answer. Therefore, the Generated Answer is correct as it conveys the same critical information without omitting any essential details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 80/100 questions. Current Accuracy: 0.7625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [07:20<01:54,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 470, Query:\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?\n",
      "__Correct answer__:\n",
      "Providing Claude with a specific role, such as being the General Counsel of a company, using XML tags can help it catch critical legal issues and risks in a contract that it might miss without the role context, potentially saving the company millions of dollars.\n",
      "__Generated answer__:\n",
      "Using XML tags to provide a specific role or context can significantly enhance Claude's analysis of a legal contract compared to not using a role prompt. The structured nature of XML tags allows for clearer separation of different parts of the prompt, which improves clarity and reduces the likelihood of misinterpretation. This structured approach ensures that Claude can accurately identify and focus on the relevant sections of the contract, leading to more precise analysis.\n",
      "\n",
      "Additionally, role prompting enhances accuracy in complex scenarios like legal analysis by tailoring Claude's responses to the specific requirements of the task. By setting a clear role context, Claude can adjust its communication style and focus more effectively on the legal aspects of the contract, ensuring that the analysis is relevant and aligned with the user's needs.\n",
      "\n",
      "Overall, the combination of XML tags for structure and role prompting for context leads to improved clarity, accuracy, and focus in Claude's analysis of legal contracts.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the essence of the Correct Answer by explaining how using XML tags and role prompting can enhance Claude's analysis of legal contracts. It discusses the benefits of structured prompts for clarity and the importance of role context for accuracy, which aligns with the idea that providing a specific role can help identify critical legal issues. However, it does not explicitly mention the potential financial savings for the company, which is a critical piece of information in the Correct Answer. Therefore, while the Generated Answer is largely correct in its reasoning, it is missing a key aspect of the impact of using role prompts.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [07:25<01:42,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 475, Query:\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?\n",
      "__Correct answer__:\n",
      "When required parameters are missing, Claude 3 Opus is more likely to ask the user for the missing information, while Claude 3 Sonnet is more likely to try to infer reasonable values on its own to proceed with the tool call.\n",
      "__Generated answer__:\n",
      "Claude 3 Opus and Claude 3 Sonnet handle missing information differently when making tool calls. \n",
      "\n",
      "Claude 3 Opus is designed to manage complex tools and ambiguous queries effectively. It is capable of handling multiple tools simultaneously and actively seeks clarification when it encounters missing information. This means that if there are parameters or details that are not provided, Opus will attempt to ask for more information to ensure accurate tool usage.\n",
      "\n",
      "On the other hand, Claude 3 Sonnet is more suited for tasks that require a balance of intelligence and speed but may not handle ambiguity as effectively as Opus. While Sonnet can infer missing parameters, it does not actively seek clarification like Opus does. This can lead to situations where Sonnet might make assumptions about the missing information rather than confirming it.\n",
      "\n",
      "In summary, Opus is proactive in seeking clarification for missing information, while Sonnet may infer or assume missing parameters without seeking further input.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key differences between Claude 3 Opus and Claude 3 Sonnet regarding how they handle missing information when making tool calls. It correctly states that Opus actively seeks clarification for missing parameters, while Sonnet tends to infer reasonable values without asking for additional information. Therefore, the substance of the generated answer aligns with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [07:35<02:00,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 601, Query:\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?\n",
      "__Correct answer__:\n",
      "To ensure a reliable production deployment of Claude for ticket routing, key steps include implementing retry logic to handle errors, conducting thorough staging and load testing, setting up error handling and logging, using a gradual rollout process, providing documentation and training, and establishing monitoring and alerting.\n",
      "__Generated answer__:\n",
      "To ensure a reliable deployment of an automated ticket routing system using Claude into a production environment, follow these steps:\n",
      "\n",
      "1. **Implement Retry Logic**: Add try/except logic to handle cases where Claude may not return the expected output or is temporarily unavailable. Use back-off logic to retry after increasing intervals or adjust the temperature for output variations.\n",
      "\n",
      "2. **Thorough Staging Testing**: Conduct extensive testing in a staging environment that closely resembles your production setup to identify potential issues or incompatibilities before deployment.\n",
      "\n",
      "3. **Load Testing**: Perform load testing to verify that the system can handle the anticipated volume of tickets without performance degradation, ensuring responsiveness and efficiency under real-world conditions.\n",
      "\n",
      "4. **Error Handling and Logging**: Implement comprehensive error handling and logging mechanisms to facilitate debugging and monitoring in production, allowing for quick identification and resolution of issues.\n",
      "\n",
      "5. **Gradual Rollout**: Establish a phased rollout plan, starting with a small percentage of traffic and gradually increasing it while closely monitoring the system’s behavior to minimize risk.\n",
      "\n",
      "6. **Documentation and Training**: Prepare detailed documentation and provide training to relevant stakeholders on how to use and maintain the new system effectively, ensuring a smooth transition and promoting adoption.\n",
      "\n",
      "7. **Monitoring and Alerting**: Set up robust monitoring and alerting mechanisms to proactively detect and address any issues that may arise in production, enabling quick responses to minimize downtime.\n",
      "\n",
      "By following these steps, you can achieve a successful and reliable deployment of your automated ticket routing system using Claude, ensuring a seamless experience for users.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key steps outlined in the correct answer for ensuring a reliable deployment of an automated ticket routing system using Claude. It includes all essential components such as implementing retry logic, thorough staging and load testing, error handling and logging, gradual rollout, documentation and training, and monitoring and alerting. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [07:41<01:48,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 505, Query:\n",
      "How should you evaluate a model's performance on a ticket routing classifier?\n",
      "__Correct answer__:\n",
      "You should evaluate performance in terms of accuracy, cost, and speed.\n",
      "__Generated answer__:\n",
      "To evaluate a model's performance on a ticket routing classifier, consider the following key metrics and factors:\n",
      "\n",
      "1. **Accuracy**: Measure how often the model's predictions match the correct classifications. This is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects the model's performance on both false positives and false negatives.\n",
      "\n",
      "3. **Consistency**: Assess whether the model produces consistent outputs for similar inputs, ensuring reliability in its predictions.\n",
      "\n",
      "4. **Structure**: Ensure that the model's output follows the expected format, such as JSON, making it easy to parse and interpret.\n",
      "\n",
      "5. **Speed**: Evaluate the response time of the model to ensure it meets acceptable latency thresholds for ticket routing tasks.\n",
      "\n",
      "6. **Bias and Fairness**: If the model classifies data related to individuals, it is crucial to check for biases that could affect classification based on gender, ethnicity, or other characteristics.\n",
      "\n",
      "Additionally, before deploying the model, implement thorough staging testing, load testing, and robust error handling. Consider a gradual rollout to monitor performance and address any issues that arise. Documentation and training for stakeholders, along with monitoring and alerting mechanisms, are also essential for a successful deployment.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer includes accuracy, speed, and adds additional relevant metrics such as F1 Score, consistency, structure, and bias and fairness, which are important for evaluating a model's performance. While the correct answer mentions accuracy, cost, and speed, the generated answer does not explicitly mention cost. However, it does provide a more comprehensive evaluation approach, which can be seen as an enhancement rather than a contradiction. Therefore, the generated answer is correct in substance, even if it lacks the specific mention of cost.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [07:44<01:24,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 339, Query:\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?\n",
      "__Correct answer__:\n",
      "Anthropic recommends trying their interactive GitHub prompting tutorial and Google Sheets prompting tutorial to learn prompt engineering concepts before diving into the techniques in the documentation.\n",
      "__Generated answer__:\n",
      "Anthropic recommends two methods for learning how to prompt engineer with Claude before diving into the techniques: \n",
      "\n",
      "1. **Prompt Engineering Guide** - This guide helps optimize Claude’s performance through effective prompting.\n",
      "2. **Anthropic Cookbook** - This resource provides interactive Jupyter notebooks that demonstrate various functionalities, including uploading PDFs and embeddings, allowing users to learn through practical examples.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer does not correctly identify the two methods recommended by Anthropic for learning how to prompt engineer with Claude. Instead, it mentions a \"Prompt Engineering Guide\" and an \"Anthropic Cookbook,\" which are not the same as the interactive GitHub prompting tutorial and Google Sheets prompting tutorial mentioned in the correct answer. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [07:59<01:56,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 628, Query:\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?\n",
      "__Correct answer__:\n",
      "Pretrained large language models are trained on unlabeled text data to predict the next word given the previous context, but are not inherently good at answering questions or following instructions without prompt engineering. In contrast, Claude is a large language model that has been further fine-tuned and trained using RLHF to be more helpful, honest, and capable of performing a wider range of useful tasks.\n",
      "__Generated answer__:\n",
      "The key differences between a pretrained large language model (LLM) and Claude in terms of their training and capabilities are as follows:\n",
      "\n",
      "1. **Pretraining Process**: Both pretrained LLMs and Claude undergo an initial pretraining phase where they are trained on a large corpus of text to predict the next word based on previous context. However, Claude is specifically designed as a conversational assistant, which implies a more focused application of its capabilities.\n",
      "\n",
      "2. **Refinement Techniques**: Pretrained LLMs often require additional techniques such as fine-tuning and reinforcement learning from human feedback (RLHF) to enhance their performance in specific tasks. Claude has undergone these processes to improve its ability to assist users, making it more helpful, honest, and harmless compared to standard pretrained models that may not be as refined.\n",
      "\n",
      "3. **Capabilities**: While pretrained LLMs can generate text and perform various tasks, they may not be inherently good at answering questions or following instructions without significant prompt engineering. In contrast, Claude has been fine-tuned to excel in these areas, making it more effective as a conversational assistant.\n",
      "\n",
      "4. **Model Family**: Claude is part of a family of models that are designed to balance intelligence, speed, and cost, indicating a tailored approach to meet specific user needs, whereas pretrained LLMs may not have such a structured framework for application.\n",
      "\n",
      "In summary, Claude is a specialized conversational assistant built on a pretrained LLM foundation, enhanced through fine-tuning and RLHF, which allows it to perform better in user interactions compared to standard pretrained models.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key differences between a pretrained large language model and Claude, including the pretraining process, refinement techniques, capabilities, and the model family. It emphasizes Claude's enhancements through fine-tuning and RLHF, which aligns with the correct answer's points about Claude being more helpful, honest, and capable. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [08:06<01:46,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 653, Query:\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?\n",
      "__Correct answer__:\n",
      "Prompt engineering is typically faster, more cost-effective, requires less data and compute resources, and preserves the model's general knowledge compared to fine-tuning. It also allows for greater flexibility, rapid iteration, and transparency.\n",
      "__Generated answer__:\n",
      "Key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain include:\n",
      "\n",
      "1. **Resource Efficiency**: Prompt engineering requires only text input and does not need high-end GPUs or large memory, making it much more resource-friendly compared to fine-tuning.\n",
      "\n",
      "2. **Cost-Effectiveness**: Fine-tuning can incur significant costs, especially for cloud-based AI services. In contrast, prompt engineering typically utilizes the base model, which is cheaper.\n",
      "\n",
      "3. **Time-Saving**: Fine-tuning can take hours or even days, while prompt engineering provides nearly instantaneous results, allowing for quick problem-solving.\n",
      "\n",
      "4. **Minimal Data Needs**: Fine-tuning requires substantial task-specific, labeled data, which can be scarce or expensive. Prompt engineering can work with few-shot or even zero-shot learning, requiring much less data.\n",
      "\n",
      "5. **Flexibility and Rapid Iteration**: Prompt engineering allows for quick experimentation with various approaches and immediate results, making it easier to tweak prompts compared to the more rigid fine-tuning process.\n",
      "\n",
      "6. **Domain Adaptation**: It is easier to adapt models to new domains by providing domain-specific context in prompts without the need for retraining.\n",
      "\n",
      "7. **Comprehension Improvements**: Prompt engineering is more effective at helping models understand and utilize external content, such as retrieved documents.\n",
      "\n",
      "8. **Preservation of General Knowledge**: Fine-tuning risks catastrophic forgetting, where the model may lose its general knowledge. Prompt engineering helps maintain the model’s broad capabilities.\n",
      "\n",
      "9. **Transparency**: Prompts are human-readable, which aids in understanding and debugging the model's behavior, providing clarity on what information the model receives.\n",
      "\n",
      "These advantages make prompt engineering a preferred method for adapting language models quickly and efficiently without the drawbacks associated with fine-tuning.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key advantages of prompt engineering over fine-tuning, including resource efficiency, cost-effectiveness, time-saving, minimal data needs, flexibility, domain adaptation, preservation of general knowledge, and transparency. It also emphasizes the rapid iteration and experimentation capabilities of prompt engineering. All critical points from the correct answer are covered in the generated answer, making it a comprehensive and correct response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [08:09<01:18,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 322, Query:\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?\n",
      "__Correct answer__:\n",
      "Before running requests to access Claude models on Vertex AI, you may need to run `gcloud auth application-default login` to authenticate with GCP.\n",
      "__Generated answer__:\n",
      "To authenticate with Google Cloud Platform (GCP) before running requests to access Claude models on Vertex AI, you need to execute the command `gcloud auth application-default login`. This command will allow you to authenticate your application with GCP, enabling you to make requests to the Claude models.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately conveys the necessary command to authenticate with GCP before accessing Claude models on Vertex AI, which is the same information provided in the correct answer. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [08:13<01:02,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 512, Query:\n",
      "What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\n",
      "__Correct answer__:\n",
      "According to the information provided, on May 10th, 2024, Anthropic introduced a new \"Prompt Generator\" tool in the Developer Console. This tool is designed to help users guide Claude to generate high-quality prompts tailored to their specific tasks. The text states that the Prompt Generator \"makes it easy to guide Claude to generate a high-quality prompts tailored to your specific tasks.\" This indicates that the Prompt Generator feature provides users with the ability to create customized prompts for Claude, going beyond the standard prompting capabilities. By combining this information with the details about the Claude iOS app and the Claude Team plan released around the same time, we can infer that Anthropic was expanding its platform and tools to provide users with more advanced capabilities for interacting with and leveraging the Claude AI assistant for their specific needs and use cases.\n",
      "__Generated answer__:\n",
      "On May 10th, 2024, Anthropic introduced a new Prompt Generator tool in the Developer Console. This tool enables users to easily create high-quality prompts that are tailored to specific tasks. By guiding users in the prompt creation process, the Prompt Generator enhances the ability to interact effectively with Claude, Anthropic's AI model. This feature is part of Anthropic's broader suite of products and services related to Claude AI, facilitating more precise and effective use of the model for various applications.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key points from the Correct Answer regarding the introduction of the Prompt Generator tool and its purpose in helping users create tailored prompts for specific tasks. Both answers emphasize the tool's role in enhancing interaction with Claude and mention its integration into Anthropic's broader suite of products. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [08:16<00:49,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 299, Query:\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?\n",
      "__Correct answer__:\n",
      "Both Claude 3.5 Sonnet and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "__Generated answer__:\n",
      "Both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer matches the correct answer in substance, stating that both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 90/100 questions. Current Accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [08:18<00:37,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query tokens: 306, Query:\n",
      "When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\n",
      "__Correct answer__:\n",
      "You can use \"max_tokens\": 1 in the request to limit Claude's response to a single token when putting words in its mouth.\n",
      "__Generated answer__:\n",
      "To limit Claude's response to a single token, you can use the header `max_tokens` with a value of `1` in your request.\n",
      "__Response from judge LLM__:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that to limit Claude's response to a single token, you can use the header `max_tokens` with a value of `1`, which aligns with the correct answer. Both answers convey the same essential information without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "# level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "# level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the evaluations\n",
    "eval_data_range = eval_data[0:100]\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_similar_level_two, eval_data_range, level_two_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context_level_two, level_two_db, eval_data_range)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data_range],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_summary_enhanced.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a json file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_summary_enhanced.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Enhanced\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea28c0-4e1c-4398-ac76-0739a0a07553",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "from utils.plot_perf import plot_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec177621-9804-425e-a2c7-db0a00d6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualize our accuracy\n",
    "# plot_performance('evaluation/json_results', ['Basic RAG', 'Summary Enhanced'], colors=['skyblue', 'green'])\n",
    "plot_performance('evaluation/json_results', [ 'Summary Enhanced'], colors=['green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d19c2-a063-4bb9-ad29-beedbdf24e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
