{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (Basic w/Evals)\n",
    "\n",
    "LLMs excels at a wide range of tasks, but struggle with queries specific to your unique business context. This is where Retrieval Augmented Generation (RAG) becomes invaluable. RAG enables the LLM to leverage your internal knowledge bases or customer support documents, significantly enhancing its ability to answer domain-specific questions. Enterprises are increasingly building RAG applications to improve workflows in customer support, Q&A over internal company documents, financial & legal analysis, and much more.\n",
    "\n",
    "In this guide, we'll demonstrate how to build and optimize a RAG system using the Anthropic documentation as our knowledge base. We'll walk you through:\n",
    "\n",
    "1. Embeddings are from the `intfloat/multilingual-e5-large-instruct` model, where input is truncated to at most 512 tokens\n",
    "2. In-memory vector database class is from Anthropic\n",
    "3. Building a robust evaluation suite. We'll go beyond 'vibes' based evals and show you how to measure the retrieval pipeine & end to end performance independently\n",
    "4. Implementing advanced techniques to improve RAG including summary indexing and re-ranking with Claude.\n",
    "\n",
    "Through a series of targeted improvements, we achieved significant performance gains on the following metrics compared to a basic RAG pipeline (we'll explain what all these metrics *mean* in a bit)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1) Setup\n",
    "2) Level 1 - Basic RAG\n",
    "3) Building an Evaluation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need a few libraries and models:\n",
    "\n",
    "1. `intfloat/multilingual-e5-large-instruct` to generate high quality embeddings\n",
    "2. `openai`,  LLM for (1) generation (2) judge\n",
    "4. `pandas`, `numpy`, `matplotlib`, and `scikit-learn` for data manipulation and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "## silent setup (-q)\n",
    "!pip install openai -q\n",
    "!pip install --upgrade tiktoken -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "embedding_model = \"intfloat/multilingual-e5-large-instruct\"; generation_model = \"gpt-4o-mini\"; judge_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Embeddings model and run a quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf2423a05144d42a37c26c6b62b2b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8b560618cd4bbe866424deffa593ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b04564c2bb747f88381923dd29e7199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502493fc37a840fbb2cd7d7f459a16f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e75866d3ee84361b4e8d2cdd7b1c4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb21f4c110463aa62b7cda2a564b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd5d2c2fcb746b98d3862a65d2c6e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a32204a995408291077c0d8bc08d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ed4bafb8ac4bb38405a1ebbda9a1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ebb50fa4c044138818baed9cbc8cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91.92853546142578, 67.58030700683594], [70.38142395019531, 92.1330795288086]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, '南瓜的家常做法')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"\n",
    "]\n",
    "input_texts = queries + documents\n",
    "\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "embeddings = model.encode(input_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())\n",
    "# [[91.92853546142578, 67.5802993774414], [70.38143157958984, 92.13307189941406]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Vector DB Class\n",
    "\n",
    "In this example, we're using an in-memory vector DB, but for a production application, you may want to use a hosted solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_vec_db_in_memory(self, data):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_vec_db()\n",
    "            return\n",
    "\n",
    "        texts = [f\"Heading: {item['chunk_heading']}\\n\\n Chunk Text:{item['text']}\" for item in data]\n",
    "        self._embed_and_store(texts, data)\n",
    "        self.save_db()\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = model.encode(query)\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_vec_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_vec_in_memory to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 - Basic RAG\n",
    "\n",
    "To get started, we'll set up a basic RAG pipeline using a bare bones approach. This is sometimes called 'Naive RAG' by many in the industry. A basic RAG pipeline includes the following 3 steps:\n",
    "\n",
    "0) Pick a prompt (there's more than one to try out)\n",
    "\n",
    "1) Chunk documents by heading - containing only the content from each subheading\n",
    "\n",
    "2) Embed each document\n",
    "\n",
    "3) Use Cosine similarity to retrieve documents in order to answer query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0:\n",
      "You have been tasked with helping us to answer the following query: \n",
      "<query>\n",
      "{query}\n",
      "</query>\n",
      "You have access to the following documents which are meant to provide context as you answer the query:\n",
      "<documents>\n",
      "{context}\n",
      "</documents>\n",
      "Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
      "Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
      "\n",
      "\n",
      "Prompt 1:\n",
      "You have been tasked with helping us to answer the following query: \n",
      "<query>\n",
      "{query}\n",
      "</query>\n",
      "You have access to the following documents which are meant to provide context as you answer the query:\n",
      "<documents>\n",
      "{context}\n",
      "</documents>\n",
      "Please remain absolutely faithful to the underlying context, and do not deviate from it at all.\n",
      "If you do not find the answer, say, \"The context does not have the answer,\" \n",
      "Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick out a prompt\n",
    "import yaml\n",
    "\n",
    "def read_prompts(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "        \n",
    "    prompts = [entry[\"prompt\"] for entry in data]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "filename = \"./prompts/prompts.yaml\"\n",
    "prompts = read_prompts(filename)\n",
    "    \n",
    "for i, prompt in enumerate(prompts, start=0):\n",
    "    print(f\"Prompt {i}:\\n{prompt}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from disk.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a16559234e44be82d90df13d2ae7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test contexts:\n",
      "\n",
      "How to get embeddings with Anthropic\n",
      "\n",
      "\n",
      "Anthropic does not offer its own embedding model. One embeddings provider that has a wide variety of options and capabilities encompassing all of the above considerations is Voyage AI.\n",
      "Voyage AI makes state-of-the-art embedding models and offers customized models for specific industry domains such as finance and healthcare, or bespoke fine-tuned models for individual customers.\n",
      "The rest of this guide is for Voyage AI, but we encourage you to assess a variety of embeddings vendors to find the best fit for your specific use case.\n",
      "\n",
      "\n",
      "Before implementing embeddings\n",
      "\n",
      "\n",
      "When selecting an embeddings provider, there are several factors you can consider depending on your needs and preferences:\n",
      "Dataset size & domain specificity: size of the model training dataset and its relevance to the domain you want to embed. Larger or more domain-specific data generally produces better in-domain embeddings\n",
      "Inference performance: embedding lookup speed and end-to-end latency. This is a particularly important consideration for large scale production deployments\n",
      "Customization: options for continued training on private data, or specialization of models for very specific domains. This can improve performance on unique vocabularies\n",
      "\n",
      "\n",
      "Model options\n",
      "\n",
      "\n",
      "Enterprise use cases often mean complex needs and edge cases. Anthropic offers a range of models across the Claude 3 and Claude 3.5 families to allow you to choose the right balance of intelligence, speed, and cost.\n",
      "\n",
      "\n",
      "Test Answer:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models and provides customized models tailored for specific industry domains such as finance and healthcare, as well as bespoke fine-tuned models for individual customers.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "def retrieve_similar(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n{chunk['text']}\\n\"\n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):\n",
    "    # query = \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    # prompt = prompts[1]\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context(query, db):\n",
    "    _, context = retrieve_similar(query, db)  # k=3 similar\n",
    "    completion = client.chat.completions.create(\n",
    "        model=generation_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "logging.basicConfig(filename=\"log.log\",\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load the Anthropic documentation\n",
    "with open('data/anthropic_docs.json', 'r') as f:\n",
    "    anthropic_docs = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "db = VectorDB(\"anthropic_docs\")\n",
    "db.load_vec_db_in_memory(anthropic_docs)\n",
    "\n",
    "# test\n",
    "query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\n",
    "test_results, test_contexts = retrieve_similar(query, db)\n",
    "print(f'Test contexts:\\n{test_contexts}')\n",
    "print(f'Test Answer:\\n{answer_query_from_context(query, db)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Setup\n",
    "\n",
    "When evaluating RAG applications, it's critical to evaluate the performance of the retrieval system and end to end system separately.\n",
    "\n",
    "We synthetically generated an evaluation dataset consisting of 100 samples which include the following:\n",
    "- A question\n",
    "- Chunks from our docs which are relevant to that question. This is what we expect our retrieval system to retrieve when the question is asked\n",
    "- A correct answer to the question.\n",
    "\n",
    "This is a relatively challenging dataset. Some of our questions require synthesis between more than one chunk in order to be answered correctly, so it's important that our system can load in more than one chunk at a time. You can inspect the dataset by opening `evaluation/docs_evaluation_dataset.json`\n",
    "\n",
    "Run the next cell to see a preview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 4 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1d6210b8\",\n",
      "    \"question\": \"What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/claude-for-sheets#why-use-claude-for-sheets\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts#how-to-chain-prompts\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=4):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"For OpenAI models, returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db) # ??\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_tokens = num_tokens_from_string(comparision_prompt, \"o200k_base\")  # note, this encoding name for gpt-4o, gpt-4o-mini\n",
    "        print(f'Number of tokens: {nb_tokens}')\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'Query:\\n{query}')\n",
    "            print(f'Correct answer:\\n{correct_answer}')\n",
    "            print(f'Generated anser:\\n{generated_answer}')\n",
    "            print(f'Response_text from judge LLM:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:00<00:00, 3454.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.4333, Avg Recall: 0.7000, Avg MRR: 0.9000\n",
      "Processed 20/100 items. Current Avg Precision: 0.3333, Avg Recall: 0.5500, Avg MRR: 0.7000\n",
      "Processed 30/100 items. Current Avg Precision: 0.3778, Avg Recall: 0.6000, Avg MRR: 0.7667\n",
      "Processed 40/100 items. Current Avg Precision: 0.4083, Avg Recall: 0.6250, Avg MRR: 0.8000\n",
      "Processed 50/100 items. Current Avg Precision: 0.4067, Avg Recall: 0.6300, Avg MRR: 0.7800\n",
      "Processed 60/100 items. Current Avg Precision: 0.4056, Avg Recall: 0.6361, Avg MRR: 0.7833\n",
      "Processed 70/100 items. Current Avg Precision: 0.3952, Avg Recall: 0.6167, Avg MRR: 0.7548\n",
      "Processed 80/100 items. Current Avg Precision: 0.4208, Avg Recall: 0.6583, Avg MRR: 0.7792\n",
      "Processed 90/100 items. Current Avg Precision: 0.4185, Avg Recall: 0.6556, Avg MRR: 0.7704\n",
      "Processed 100/100 items. Current Avg Precision: 0.3933, Avg Recall: 0.6183, Avg MRR: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:04<07:44,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\n",
      "Correct answer:\n",
      "To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\n",
      "Generated anser:\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. Access the Evaluation screen where you will see a single row for your initial test case.\n",
      "2. Click the ‘Add Test Case’ button to create additional test cases.\n",
      "3. Fill in the required values for each variable in your prompt for each new test case.\n",
      "4. Repeat the process to create as many scenarios as needed.\n",
      "\n",
      "Ensure that your original prompt includes at least 1-2 dynamic variables using the double brace syntax (e.g., {{variable}}) to facilitate the creation of evaluation test sets. If you later update your original prompt text, you can re-run the entire evaluation suite against the new prompt to assess how the changes impact performance across all test cases.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the process of creating multiple test cases in the Anthropic Evaluation tool, including the steps to add test cases and fill in the required values. It also mentions the use of dynamic variables in the original prompt, which is relevant information for creating evaluation test sets. Therefore, it aligns well with the Correct Answer and does not omit any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:07<05:26,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\n",
      "Correct answer:\n",
      "Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\n",
      "Generated anser:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models and provides customized models tailored for specific industry domains such as finance and healthcare, as well as bespoke fine-tuned models for individual customers.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. It mentions that Anthropic recommends Voyage AI for embedding models and details the capabilities of Voyage AI, including customized models for specific industries like finance and healthcare, as well as bespoke fine-tuned models for individual customers. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:11<06:33,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\n",
      "Correct answer:\n",
      "When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\n",
      "Generated anser:\n",
      "Key success metrics to consider when evaluating Claude's performance on a classification task include:\n",
      "\n",
      "1. **Accuracy**: This measures the proportion of correct predictions made by the model compared to the total predictions. High accuracy indicates that the model is effectively classifying inputs.\n",
      "\n",
      "2. **F1 Score**: This metric balances precision (the accuracy of positive predictions) and recall (the ability to find all relevant instances). It is particularly useful in cases where class distribution is imbalanced.\n",
      "\n",
      "3. **Consistency**: This assesses whether the model provides similar outputs for similar inputs, ensuring reliability in its predictions.\n",
      "\n",
      "4. **Structure**: This evaluates whether the model's output adheres to expected formats, such as JSON, which is important for ease of interpretation and integration.\n",
      "\n",
      "5. **Speed**: This measures the response time of the model, which is critical for applications requiring low latency. A model that provides quick responses is often preferred, especially in real-time applications.\n",
      "\n",
      "6. **Bias and Fairness**: This metric examines whether the model's predictions are free from biases related to gender, ethnicity, or other characteristics, ensuring ethical and fair outcomes.\n",
      "\n",
      "When it comes to choosing the right model to reduce latency, selecting a smaller model like Claude 3 Haiku can be beneficial due to its speed and efficiency. However, for tasks requiring more complex reasoning or specialized knowledge, larger models like Sonnet or Opus may be more appropriate despite potentially higher latency. Therefore, understanding the trade-offs between model size, speed, and output quality is essential for optimizing performance in classification tasks.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the key success metrics for evaluating Claude's performance on a classification task, including accuracy, F1 score, consistency, structure, speed, and bias and fairness. It also discusses the importance of choosing the right model to reduce latency, mentioning the trade-offs between model size, speed, and output quality. While it introduces specific model names (Claude 3 Haiku, Sonnet, Opus) that are not mentioned in the correct answer, this does not detract from the overall correctness of the information provided. Therefore, the generated answer is consistent with the correct answer in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:14<05:42,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\n",
      "Correct answer:\n",
      "Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\n",
      "Generated anser:\n",
      "Claude for Sheets can improve prompt engineering workflows compared to using chained prompts in two key ways:\n",
      "\n",
      "1. **Parallel Testing of Prompts**: Claude for Sheets allows users to test multiple prompts across evaluation suites simultaneously. This capability enables more efficient experimentation and optimization of prompts, as users can quickly compare results and iterate on their designs without the sequential limitations of chained prompts.\n",
      "\n",
      "2. **Structured Example Prompts**: The Claude for Sheets prompting examples workbench provides a centralized location for example prompts and prompt engineering structures. This organized approach facilitates easier access to best practices and can streamline the process of developing and refining prompts, enhancing overall productivity compared to the more fragmented approach of using chained prompts.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts: parallel testing of prompts and the structured example prompts workbench. However, it misses the specific mention of office tasks like survey analysis and online data processing, which is a critical aspect of the correct answer. Therefore, while it captures some key points, it does not fully align with the correct answer due to the omission of important information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:17<04:53,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What happens if a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns?\n",
      "Correct answer:\n",
      "If a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error.\n",
      "Generated anser:\n",
      "If a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. The API requires these specific markers to properly identify the roles of the participants in the conversation. Without them, the API cannot process the prompt correctly.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns will result in an API error, which aligns with the correct answer. Additionally, it provides further clarification about the necessity of these markers for the API to identify roles in the conversation, which is consistent with the information provided in the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:21<05:21,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?\n",
      "Correct answer:\n",
      "Tool use requests in the Claude API are priced the same as regular API requests, based on the total input and output tokens. However, tool use requests have additional tokens beyond the regular input and output, including the tools parameter, tool use content blocks, tool result content blocks, and a special system prompt that enables tool use, which add to the total tokens and cost.\n",
      "Generated anser:\n",
      "The additional tokens required for tool use in Claude API requests impact pricing by increasing the total token count for each request. Tool use requests are priced the same as regular API requests, based on the total number of input tokens sent to the model (which includes tokens from the tools parameter) and the number of output tokens generated.\n",
      "\n",
      "The additional tokens come from several sources:\n",
      "1. The tools parameter, which includes tool names, descriptions, and schemas.\n",
      "2. Tool use content blocks in API requests and responses.\n",
      "3. Tool result content blocks in API requests.\n",
      "\n",
      "Moreover, a special system prompt for enabling tool use is automatically included, which adds a specific number of tokens depending on the model being used. For example, the Claude 3.5 Sonnet model requires an additional 294 tokens for tool choice and 261 tokens for tool use system prompts.\n",
      "\n",
      "These additional tokens are added to the normal input and output tokens to calculate the total cost of a request. Therefore, using tools will generally result in higher costs compared to regular API requests due to the increased token count.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer regarding the impact of additional tokens on pricing for tool use in Claude API requests. It mentions that tool use requests are priced the same as regular API requests based on total input and output tokens, and it details the sources of additional tokens, including the tools parameter, tool use content blocks, tool result content blocks, and the special system prompt. The explanation about the additional tokens leading to higher costs is also consistent with the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:22<04:25,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\n",
      "Correct answer:\n",
      "The new Usage, Cost, and Rate Limits tabs in the Anthropic Developer Console that show API usage, billing details, and current rate limits will be available on June 27th, 2024.\n",
      "Generated anser:\n",
      "The new Anthropic Developer Console features that show API usage, billing details, and rate limits will be available on June 27th, 2024.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that the new Anthropic Developer Console features will be available on June 27th, 2024, which matches the information provided in the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:26<04:47,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\n",
      "Correct answer:\n",
      "When deciding whether to use CoT, consider if the task requires in-depth thinking that a human would need to work through, and be aware that the increased output length from CoT may impact latency.\n",
      "Generated anser:\n",
      "When deciding whether to use chain-of-thought (CoT) for a task, two key factors to consider are:\n",
      "\n",
      "1. **Complexity of the Task**: CoT should be used for tasks that require in-depth thinking, such as complex math, multi-step analysis, or writing complex documents. If the task is straightforward and does not require extensive reasoning, using CoT may not be necessary and could lead to unnecessary latency.\n",
      "\n",
      "2. **Impact on Latency**: Increased output length from using CoT can impact latency. It is important to balance the need for thorough reasoning with the acceptable level of latency for the task at hand. If low latency is a priority, consider whether the depth of reasoning provided by CoT is essential for achieving the desired performance.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two key factors to consider when deciding whether to use chain-of-thought (CoT): the complexity of the task and the impact on latency. It emphasizes that CoT is suitable for tasks requiring in-depth thinking and notes the potential latency issues due to increased output length. This aligns well with the correct answer, which also highlights the need for in-depth thinking and the impact on latency. Therefore, the generated answer captures the essential points without omitting any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:30<05:16,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can I use Claude to more easily digest the content of long PDF documents?\n",
      "Correct answer:\n",
      "You can upload PDFs and have Claude summarize their content, making it easier to understand the key points of long documents without having to read through everything.\n",
      "Generated anser:\n",
      "To use Claude for more easily digesting the content of long PDF documents, you can follow these steps:\n",
      "\n",
      "1. **Extract Text from PDF**: First, you need to convert the PDF document into a text format. This can be done using various PDF extraction tools or software that can read and convert PDF content into plain text.\n",
      "\n",
      "2. **Summarization**: Once you have the text, you can input sections of it into Claude and ask for summaries. You can prompt Claude to provide concise summaries of specific sections or the entire document, making it easier to grasp the main ideas without reading everything in detail.\n",
      "\n",
      "3. **Question and Answer**: After summarizing, you can ask Claude specific questions about the content. This allows you to clarify any points of confusion or delve deeper into particular topics covered in the document.\n",
      "\n",
      "4. **Organizing Information**: You can also use Claude to help organize the information from the PDF. For instance, you can ask it to categorize content into themes or create bullet points of key information.\n",
      "\n",
      "5. **Iterative Refinement**: If the initial summaries or answers are not satisfactory, you can refine your prompts to get more targeted responses. This iterative process can help you extract the most relevant information efficiently.\n",
      "\n",
      "By leveraging these capabilities, Claude can significantly enhance your ability to digest and understand long PDF documents.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed process for using Claude to digest long PDF documents, including steps for text extraction, summarization, Q&A, organizing information, and iterative refinement. However, it introduces the need for text extraction from PDFs, which is not mentioned in the correct answer. The correct answer simply states that you can upload PDFs and have Claude summarize their content directly, which implies that Claude can handle PDFs without needing prior conversion. Therefore, the generated answer is incorrect as it adds unnecessary steps and deviates from the simplicity of the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:33<04:40,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\n",
      "Correct answer:\n",
      "You can view your organization's current API rate limits in the Rate Limits tab of the Developer Console.\n",
      "Generated anser:\n",
      "You can view your organization's current API rate limits in the Anthropic Console under the \"Plans and Billing\" section. Additionally, in the Developer Console, there is a new \"Rate Limits\" tab where you can also check your current API rate limits.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that the API rate limits can be viewed in the \"Plans and Billing\" section of the Anthropic Console, which is not mentioned in the correct answer. The correct answer specifies that the rate limits can be found in the \"Rate Limits\" tab of the Developer Console, and does not mention the \"Plans and Billing\" section at all. Therefore, there is a critical piece of information missing in the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 10/100 questions. Current Accuracy: 0.7000\n",
      "Number of tokens: 621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [00:37<05:25,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?\n",
      "Correct answer:\n",
      "In addition to accuracy, we can measure the 95th percentile response time and average cost per classification to assess the ticket classification system's performance and production-readiness.\n",
      "Generated anser:\n",
      "To measure the performance of the ticket classification system implemented using Claude beyond just accuracy, consider the following metrics and factors:\n",
      "\n",
      "1. **Precision, Recall, and F1 Score**: These metrics provide a deeper understanding of the model's performance, especially in multi-class classification scenarios. Precision measures the accuracy of the positive predictions, recall assesses the ability to find all relevant instances, and the F1 score balances both precision and recall.\n",
      "\n",
      "2. **Confusion Matrix**: Analyzing the confusion matrix helps identify specific classes where the model is performing well or poorly. This can highlight areas for improvement in the classification system.\n",
      "\n",
      "3. **Latency**: Measure the time it takes for the model to classify a ticket. This is crucial for ensuring that the system can handle real-time ticket routing efficiently.\n",
      "\n",
      "4. **Cost**: Evaluate the operational costs associated with running the classification system, including computational resources and any associated infrastructure costs.\n",
      "\n",
      "5. **Interpretable Reasoning**: Assess the quality of the explanations provided by Claude for its classification decisions. This can help build trust in the system and facilitate adjustments based on user feedback.\n",
      "\n",
      "6. **Adaptability**: Test how well the model adapts to changes in class definitions or the introduction of new classes. This can be evaluated by monitoring performance after updates to the classification schema.\n",
      "\n",
      "7. **User Feedback**: Gather qualitative feedback from users regarding the relevance and accuracy of the classifications. This can provide insights into the practical effectiveness of the system.\n",
      "\n",
      "8. **Error Analysis**: Conduct a thorough analysis of misclassified tickets to understand the underlying reasons for errors and to identify patterns that could inform future improvements.\n",
      "\n",
      "By utilizing these metrics and approaches, you can gain a comprehensive view of the performance of the ticket classification system beyond mere accuracy.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive list of metrics to evaluate the performance of the ticket classification system, including precision, recall, F1 score, latency, cost, and user feedback, among others. However, it does not specifically mention the 95th percentile response time, which is a critical piece of information included in the correct answer. Therefore, while the generated answer is informative and covers many relevant aspects, it is missing this key metric, making it incomplete in comparison to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [00:41<05:07,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?\n",
      "Correct answer:\n",
      "With the Text Completions API, the system prompt is added as text before the first \"\\n\\nHuman:\" turn. With the Messages API, the system prompt is specified using the separate \"system\" parameter when making the API request.\n",
      "Generated anser:\n",
      "To specify a system prompt using the Text Completions API, you include the system prompt text before the first `\\n\\nHuman:` in the prompt string. For example:\n",
      "\n",
      "```python\n",
      "prompt = \"Today is January 1, 2024.\\n\\nHuman: Hello, Claude\\n\\nAssistant:\"\n",
      "```\n",
      "\n",
      "In contrast, when using the Messages API, you specify the system prompt with the `system` parameter in the API call. For example:\n",
      "\n",
      "```python\n",
      "anthropic.Anthropic().messages.create(\n",
      "    model=\"claude-3-opus-20240229\",\n",
      "    max_tokens=1024,\n",
      "    system=\"Today is January 1, 2024.\",  # <-- system prompt\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
      "    ]\n",
      ")\n",
      "```\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to specify a system prompt using both the Text Completions API and the Messages API, aligning with the correct answer. It explains that for the Text Completions API, the system prompt is included before the first \"\\n\\nHuman:\" and provides an example. It also correctly states that for the Messages API, the system prompt is specified using the \"system\" parameter and gives an example. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  13%|█▎        | 13/100 [00:46<05:46,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?\n",
      "Correct answer:\n",
      "You can combine XML tags like <thinking> and <answer> with chain of thought reasoning, where Claude explains its step-by-step reasoning process, to create structured, high-performance prompts. For example, you can prompt Claude to show its reasoning by including \"Before answering, explain your reasoning step-by-step in <thinking> tags.\" in the user message or system prompt.\n",
      "Generated anser:\n",
      "To combine XML tags with chain of thought reasoning for creating high-performance prompts for Claude, follow these steps:\n",
      "\n",
      "1. **Identify Subtasks**: Break down your main task into smaller, manageable subtasks. Each subtask should focus on a specific aspect of the overall task.\n",
      "\n",
      "2. **Structure with XML**: Use XML tags to clearly delineate each part of your prompt. This enhances clarity and helps Claude understand the structure of the information being presented. For example:\n",
      "   ```xml\n",
      "   <task>\n",
      "       <subtask id=\"1\">Define the problem</subtask>\n",
      "       <subtask id=\"2\">Gather relevant data</subtask>\n",
      "       <subtask id=\"3\">Analyze the data</subtask>\n",
      "   </task>\n",
      "   ```\n",
      "\n",
      "3. **Single-Task Goal**: Ensure that each subtask has a single, clear objective. This prevents confusion and allows Claude to focus on one thing at a time.\n",
      "\n",
      "4. **Iterate and Refine**: After evaluating Claude’s responses, refine your prompts based on its performance. If certain subtasks are not yielding the desired results, adjust them for clarity or specificity.\n",
      "\n",
      "5. **Provide Targeted Examples**: Enhance the prompts by including relevant examples that illustrate the desired output. This can be done by retrieving examples from a vector database to provide context that improves Claude's accuracy.\n",
      "\n",
      "6. **Post-Processing with XML**: Design your prompts so that Claude's outputs are also structured in XML. This makes it easier to extract and manipulate specific parts of the response later on.\n",
      "\n",
      "By following these steps, you can effectively leverage XML tags and chain of thought reasoning to create structured, high-performance prompts for Claude.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed approach to combining XML tags with chain of thought reasoning, outlining steps such as identifying subtasks, structuring with XML, and refining prompts. However, it does not explicitly mention the use of specific XML tags like <thinking> and <answer> as highlighted in the correct answer. This omission is critical because it fails to address the specific mechanism of prompting Claude to explain its reasoning step-by-step using those tags. Therefore, while the generated answer offers useful information, it lacks a key element that is essential for creating high-performance prompts for Claude.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [00:48<04:59,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\n",
      "Correct answer:\n",
      "When evaluating the claude-3-haiku-20240307 model's performance on the 91 test samples, the three key metrics calculated are accuracy (89.01%), 95th percentile response time (1.61 seconds), and average cost per request routing ($0.0004).\n",
      "Generated anser:\n",
      "The three key metrics calculated when evaluating the Claude model's performance for ticket routing are intelligence, latency, and price. For the claude-3-haiku-20240307 model, specific results on the 91 test samples are not provided in the documents, so I cannot give the exact performance results for that model on those samples.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly identifies the three key metrics as intelligence, latency, and price, which do not match the correct metrics of accuracy, 95th percentile response time, and average cost per request routing. Additionally, it states that specific results are not provided, which contradicts the correct answer that provides specific performance results for the model. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [00:51<04:35,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\n",
      "Correct answer:\n",
      "Before prompt engineering, Anthropic highly recommends having a clear definition of success criteria for your use case, some ways to empirically test against those criteria, and a first draft prompt you want to improve.\n",
      "Generated anser:\n",
      "Before starting to engineer and improve a prompt in Claude, Anthropic recommends having the following key things in place:\n",
      "\n",
      "1. A clear definition of the success criteria for your use case.\n",
      "2. Some ways to empirically test against those criteria.\n",
      "3. A first draft prompt that you want to improve.\n",
      "\n",
      "If you do not have a first draft prompt, it is suggested to use the prompt generator available in the Anthropic Console.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key recommendations from the Correct Answer regarding the prerequisites for prompt engineering in Claude. It includes all the essential elements: a clear definition of success criteria, ways to empirically test those criteria, and a first draft prompt to improve. Additionally, it provides an extra suggestion about using the prompt generator if a first draft prompt is not available, which does not contradict the original answer. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [00:54<04:22,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?\n",
      "Correct answer:\n",
      "The Messages API allows you to continue a response by making the last input message have the \"assistant\" role, whereas the Text Completions API lets you pre-fill part of Claude's response directly in the prompt string.\n",
      "Generated anser:\n",
      "The Messages API handles mid-response prompting by allowing you to specify the last input message as having the assistant role, which enables the response to continue from that point. This is done by including the desired text in the assistant's message, and the model will generate text that follows from that content.\n",
      "\n",
      "In contrast, the Text Completions API requires you to pre-fill part of the response using a raw string format, where you alternate between \"Human:\" and \"Assistant:\" labels. The generated text is returned as a completion value.\n",
      "\n",
      "In summary, the Messages API uses structured message roles (user and assistant) and allows for seamless continuation of responses, while the Text Completions API relies on a simpler string format and returns completions based on the last prompt.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how the Messages API allows for mid-response prompting by specifying the last input message as having the assistant role, which aligns with the correct answer. It also correctly contrasts this with the Text Completions API, mentioning the need to pre-fill part of the response using a raw string format with \"Human:\" and \"Assistant:\" labels. Both answers convey the essential differences between the two APIs regarding how they handle mid-response prompting. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [00:57<04:27,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?\n",
      "Correct answer:\n",
      "When given the role of CFO through a system prompt, Claude provides a much more insightful, structured, and actionable financial analysis compared to not having a specific role. The role-based response breaks down key financial metrics, provides strategic commentary, and makes specific recommendations.\n",
      "Generated anser:\n",
      "Claude's response differs significantly when given a specific role through a system prompt compared to not having a defined role. When a role is assigned, such as \"a seasoned data scientist at a Fortune 500 company,\" Claude's analysis is likely to be more accurate, focused, and tailored to the expectations of that role. This means that the insights provided will be more relevant to financial analysis, reflecting the expertise and perspective of a data scientist.\n",
      "\n",
      "In contrast, without a specific role, Claude may provide a more generalized response that lacks the depth and precision required for complex financial analysis. The output may be less tailored in tone and content, potentially missing key insights that a specialized role would emphasize. Overall, role prompting enhances Claude's performance by improving accuracy, focus, and the appropriateness of the communication style for the task at hand.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the essence of the Correct Answer by explaining that Claude's response is more accurate and tailored when given a specific role, and that without a role, the response may be more generalized and lack depth. However, it does not explicitly mention the specific role of CFO or the structured, actionable nature of the analysis provided when a role is assigned, which are key elements in the Correct Answer. Therefore, while the Generated Answer is on the right track, it misses critical details that are present in the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:03<05:26,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\n",
      "Correct answer:\n",
      "Quantitative metrics for evaluating a sentiment analysis model include task-specific metrics like F1 score, as well as generic metrics like accuracy, precision, and recall. Specific targets should be based on industry benchmarks, prior experiments, AI research, or expert knowledge, and should represent an improvement over the current baseline.\n",
      "Generated anser:\n",
      "Some quantitative metrics to measure the success of a sentiment analysis model include:\n",
      "\n",
      "1. **Accuracy**: The ratio of correctly predicted sentiments to the total predictions made. This is calculated as (Number of correct predictions) / (Total number of predictions).\n",
      "\n",
      "2. **F1 Score**: This metric balances precision and recall, providing a single score that reflects both the model's ability to identify positive sentiments correctly and its ability to avoid false positives.\n",
      "\n",
      "3. **Precision**: The ratio of true positive predictions to the total predicted positives. It measures the accuracy of the positive predictions made by the model.\n",
      "\n",
      "4. **Recall**: The ratio of true positive predictions to the total actual positives. It indicates how well the model identifies all relevant instances.\n",
      "\n",
      "5. **Response Time**: The time taken by the model to produce a response, measured in milliseconds. This is crucial for applications requiring real-time feedback.\n",
      "\n",
      "6. **Uptime**: The percentage of time the model is operational and available for use, which is important for reliability.\n",
      "\n",
      "7. **Toxicity Rate**: The percentage of outputs flagged for toxicity, which can be critical for ensuring the model does not produce harmful content.\n",
      "\n",
      "To determine specific targets for these metrics, one can:\n",
      "\n",
      "- **Benchmark Against Industry Standards**: Research existing models and their performance metrics to set realistic and competitive targets.\n",
      "  \n",
      "- **Analyze Historical Data**: Review previous experiments or models to establish a baseline and aim for a percentage improvement (e.g., a 5% increase in F1 score).\n",
      "\n",
      "- **User Feedback and Requirements**: Gather input from end-users to understand their expectations and set targets that align with user needs.\n",
      "\n",
      "- **Define Clear and Specific Goals**: For instance, instead of a vague goal like \"good performance,\" specify a target such as \"achieve an F1 score of at least 0.85 on a diverse test set of 10,000 Twitter posts.\" \n",
      "\n",
      "By combining these approaches, one can set achievable, relevant, and measurable targets for the sentiment analysis model's performance.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several quantitative metrics for evaluating a sentiment analysis model, including accuracy, F1 score, precision, recall, and adds additional relevant metrics like response time, uptime, and toxicity rate. It also provides a comprehensive approach to determining specific targets for these metrics, aligning well with the correct answer's guidance on using industry benchmarks, historical data, user feedback, and clear goal definitions. Therefore, the generated answer captures the essence of the correct answer and provides additional valuable information without omitting any critical points.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  19%|█▉        | 19/100 [01:06<04:52,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?\n",
      "Correct answer:\n",
      "Combining XML tags with other prompt engineering techniques like multishot prompting (using <examples> tags) or chain of thought (using <thinking> and <answer> tags) to create super-structured, high-performance prompts.\n",
      "Generated anser:\n",
      "A power user tip mentioned in the documentation for creating high-performance prompts using XML tags is to combine XML tags with other techniques like multishot prompting (using <examples>) or chain of thought (using <thinking> and <answer>). This approach creates super-structured, high-performance prompts.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the substance of the correct answer by mentioning the combination of XML tags with multishot prompting and chain of thought techniques to create high-performance prompts. There are no critical pieces of information missing, and the essence of the advice is preserved. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [01:12<05:49,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?\n",
      "Correct answer:\n",
      "You can use an LLM like Claude to grade the outputs of other LLMs by providing it with the output to grade along with a detailed rubric. Instruct the LLM to think through its reasoning and then output a simple 'correct' or 'incorrect' result based on how well the output matches the criteria in the rubric.\n",
      "Generated anser:\n",
      "To use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric, follow these steps:\n",
      "\n",
      "1. **Define a Clear Rubric**: Create a detailed and clear rubric that specifies the criteria for grading. For example, you might specify that certain key phrases must be included in the answer or that the answer should meet specific content requirements.\n",
      "\n",
      "2. **Build a Grader Prompt**: Construct a prompt that instructs Claude to evaluate the answer based on the rubric. The prompt should include the rubric and the answer to be graded. For example:\n",
      "   ```python\n",
      "   def build_grader_prompt(answer, rubric):\n",
      "       return f\"\"\"Grade this answer based on the rubric:\n",
      "       <rubric>{rubric}</rubric>\n",
      "       <answer>{answer}</answer>\n",
      "       Think through your reasoning in <thinking> tags, then output 'correct' or 'incorrect' in <result> tags.\"\"\"\n",
      "   ```\n",
      "\n",
      "3. **Grade the Output**: Use the `grade_completion` function to send the answer and the rubric to Claude. This function will generate a response from Claude that indicates whether the answer is 'correct' or 'incorrect' based on the rubric:\n",
      "   ```python\n",
      "   def grade_completion(output, golden_answer):\n",
      "       grader_response = client.messages.create(\n",
      "           model=\"claude-3-opus-20240229\",\n",
      "           max_tokens=2048,\n",
      "           messages=[{\"role\": \"user\", \"content\": build_grader_prompt(output, golden_answer)}]\n",
      "       ).content[0].text\n",
      "       return \"correct\" if \"correct\" in grader_response.lower() else \"incorrect\"\n",
      "   ```\n",
      "\n",
      "4. **Collect and Evaluate Outputs**: Gather the outputs from the LLMs you want to grade, and for each output, use the grading function to assess it against the rubric. For example:\n",
      "   ```python\n",
      "   outputs = [get_completion(q[\"question\"]) for q in eval_data]\n",
      "   grades = [grade_completion(output, a[\"golden_answer\"]) for output, a in zip(outputs, eval_data)]\n",
      "   ```\n",
      "\n",
      "5. **Calculate Scores**: Finally, calculate the percentage of correct grades to evaluate the overall performance:\n",
      "   ```python\n",
      "   print(f\"Score: {grades.count('correct') / len(grades) * 100}%\")\n",
      "   ```\n",
      "\n",
      "By following these steps, you can effectively use Claude to automate the grading of LLM outputs based on a predefined rubric, leveraging its capabilities for nuanced judgment and reasoning.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive and detailed explanation of how to use an LLM like Claude to grade the outputs of other LLMs based on a rubric. It includes the necessary steps, such as defining a clear rubric, building a grader prompt, grading the output, collecting and evaluating outputs, and calculating scores. These steps align well with the essence of the correct answer, which emphasizes using Claude to evaluate outputs based on a rubric and providing a simple 'correct' or 'incorrect' result. Therefore, the generated answer captures the critical information and does not contradict the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 20/100 questions. Current Accuracy: 0.6500\n",
      "Number of tokens: 546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [01:17<05:57,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?\n",
      "Correct answer:\n",
      "To access Voyage embeddings on AWS, subscribe to the model package on AWS Marketplace, select the model to deploy, agree to the terms, and copy the Product ARN for your selected region. Then create a JupyterLab space in SageMaker Studio, upload Voyage's notebook, and follow the instructions to deploy the model package using the ARN.\n",
      "Generated anser:\n",
      "To access and deploy Voyage embeddings on AWS Marketplace, follow these steps:\n",
      "\n",
      "1. **Subscribe to the Model Package**:\n",
      "   - Navigate to the model package listing page on AWS Marketplace and select the Voyage model you wish to deploy.\n",
      "   - Click on the \"Continue to subscribe\" button.\n",
      "   - Review the details on the \"Subscribe to this software\" page. If you agree with the End-User License Agreement (EULA), pricing, and support terms, click on \"Accept Offer\".\n",
      "   - After selecting \"Continue to configuration\" and choosing a region, you will receive a Product ARN (Amazon Resource Name). This ARN is necessary for creating a deployable model using Boto3.\n",
      "\n",
      "2. **Deploy the Model Package**:\n",
      "   - Again, navigate to the model package listing page and select the model to deploy.\n",
      "   - Click on the \"Continue to subscribe\" button and accept the EULA and terms as before.\n",
      "   - After configuration, copy the Product ARN for your selected region.\n",
      "   - Create a JupyterLab space in SageMaker Studio, upload the Voyage notebook, and follow the instructions provided within the notebook to deploy the model.\n",
      "\n",
      "By following these steps, you will be able to access and deploy Voyage embeddings on AWS Marketplace.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed step-by-step process for accessing and deploying Voyage embeddings on AWS Marketplace, which aligns well with the correct answer. It includes subscribing to the model package, accepting the EULA, obtaining the Product ARN, and creating a JupyterLab space in SageMaker Studio to deploy the model. While there are some minor differences in wording and structure, the essential steps and information are present and accurately conveyed. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [01:22<06:25,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\n",
      "Correct answer:\n",
      "When using tools to get JSON output, you should provide a single tool, set the tool_choice to explicitly instruct the model to use that tool, and ensure the tool name and description are from the model's perspective since it will pass the input to the tool.\n",
      "Generated anser:\n",
      "To effectively set up tools for Claude to produce JSON output following a specific schema, you should focus on the following key aspects:\n",
      "\n",
      "1. **Define Tools Clearly**: Specify the tools in your API request with clear names, descriptions, and input schemas. Each tool should have:\n",
      "   - A unique name that adheres to the naming conventions (e.g., alphanumeric characters, underscores, hyphens).\n",
      "   - A detailed description explaining what the tool does and when it should be used.\n",
      "   - An input schema defined using JSON Schema that outlines the expected parameters, including required and optional fields.\n",
      "\n",
      "2. **Craft a Relevant User Prompt**: Include a user prompt that aligns with the functionality of the tools you have defined. This prompt should clearly indicate the information or action you want Claude to perform using the tools.\n",
      "\n",
      "3. **Monitor Tool Use Requests**: After providing the tools and user prompt, observe how Claude decides to use the tools. Ensure that the API response indicates a tool use request (with a `stop_reason` of `tool_use`), confirming that Claude recognizes the need for the tool.\n",
      "\n",
      "4. **Handle Tool Input and Execution**: On your end, extract the tool name and input from Claude's request. Execute the tool code client-side and prepare to send the results back to Claude.\n",
      "\n",
      "5. **Utilize Tool Results**: After executing the tool, send the results back to Claude in a new user message. Claude will analyze these results to formulate a final response that adheres to the expected JSON output schema.\n",
      "\n",
      "By following these steps, you can ensure that Claude produces the desired JSON output effectively.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a comprehensive overview of setting up tools for Claude to produce JSON output, covering aspects such as defining tools clearly, crafting a relevant user prompt, monitoring tool use requests, handling tool input and execution, and utilizing tool results. However, it does not explicitly mention the importance of providing a single tool and setting the tool_choice to instruct the model to use that tool, which are key points in the correct answer. Therefore, while the generated answer contains valuable information, it lacks critical details that are essential for the task. Thus, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [01:28<06:44,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?\n",
      "Correct answer:\n",
      "The Claude 3 Haiku model has vision capabilities, is faster, more performant, and more intelligent than the legacy Claude Instant 1.2 model. Claude 3 Haiku also has more up-to-date training data.\n",
      "Generated anser:\n",
      "The key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance are as follows:\n",
      "\n",
      "1. **Intelligence and Performance**: Claude 3 Haiku is part of the Claude 3 family, which is designed to be more intelligent and capable than the legacy models, including Claude Instant 1.2. The Claude 3 models offer higher levels of performance, fluency, and understanding.\n",
      "\n",
      "2. **Multilingual Capabilities**: Both models support multilingual functionality, but Claude 3 Haiku has better coverage, understanding, and skill compared to Claude Instant 1.2.\n",
      "\n",
      "3. **Vision Capabilities**: Claude Instant 1.2 does not have vision capabilities, whereas Claude 3 Haiku does, allowing it to process and understand visual inputs.\n",
      "\n",
      "4. **Latency**: Claude 3 Haiku is designed to be faster than Claude Instant 1.2, which is categorized as a slower model in comparison to the Claude 3 family.\n",
      "\n",
      "5. **Context Window**: Both models have a context window of 100K tokens, but Claude 3 Haiku benefits from the advancements in processing and understanding that come with the newer architecture.\n",
      "\n",
      "6. **Cost**: The cost structure is different, with Claude 3 Haiku being more cost-effective at $0.25 / $1.25 per million tokens compared to Claude Instant 1.2's $0.80 / $2.40.\n",
      "\n",
      "7. **Training Data Cut-off**: Claude Instant 1.2 has a training data cut-off in early 2023, while Claude 3 Haiku has a more recent cut-off in August 2023, which may contribute to its improved performance and relevance.\n",
      "\n",
      "Overall, Claude 3 Haiku represents a significant advancement over the legacy Claude Instant 1.2 model in terms of capabilities, performance, and cost-effectiveness.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model, including improvements in intelligence, performance, vision capabilities, latency, context window, cost, and training data cut-off. It aligns well with the correct answer, covering all the essential points without omitting critical information or introducing contradictions. Therefore, the generated answer is deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [01:30<05:20,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is one key benefit of using examples when prompt engineering with Claude?\n",
      "Correct answer:\n",
      "One key benefit of using examples in prompts is that they reduce misinterpretation of instructions, leading to more accurate outputs from Claude.\n",
      "Generated anser:\n",
      "One key benefit of using examples when prompt engineering with Claude is that they reduce misinterpretation of instructions, leading to increased accuracy in the responses generated.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer conveys the same essential idea as the correct answer, stating that using examples reduces misinterpretation of instructions and leads to increased accuracy in responses. The slight difference in wording does not change the meaning, so the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [01:33<04:34,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\n",
      "Correct answer:\n",
      "Prompt engineering allows you to easily adapt AI models to new domains by providing domain-specific context directly in the prompts, without needing to retrain the model through fine-tuning.\n",
      "Generated anser:\n",
      "One key advantage of using prompt engineering instead of fine-tuning when adapting an AI model to new domains or tasks is that prompt engineering is far faster and provides nearly instantaneous results, allowing for quick problem-solving. In contrast, fine-tuning can take hours or even days to implement.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies that prompt engineering is faster than fine-tuning, which aligns with the correct answer's emphasis on the ease and speed of adapting AI models through prompt engineering. However, it lacks the specific mention of providing domain-specific context directly in the prompts, which is a critical aspect of the correct answer. Therefore, while the generated answer captures some of the advantages, it misses a key piece of information regarding the method of adaptation.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [01:38<05:08,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?\n",
      "Correct answer:\n",
      "You can make a copy of Anthropic's provided Claude for Sheets workbook template to quickly get started using the extension with your own work.\n",
      "Generated anser:\n",
      "To quickly get started using the Claude for Sheets extension with a pre-made template, you should make a copy of the Claude for Sheets workbook template. This template will provide you with a structured starting point for your work. Once you have the template copied, you can begin using it to test prompts and perform various office tasks efficiently.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately conveys the essential information from the correct answer, stating that you should make a copy of the Claude for Sheets workbook template to get started. It also adds that this template provides a structured starting point and mentions using it for testing prompts and performing office tasks, which does not contradict the original answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [01:42<04:54,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does the \"index\" field in the \"content_block_delta\" event relate to the text being streamed in a response?\n",
      "Correct answer:\n",
      "The \"index\" field in each \"content_block_delta\" event indicates which content block the text delta applies to. Multiple deltas with the same index consecutively stream the text for a single content block in the response.\n",
      "Generated anser:\n",
      "The \"index\" field in the \"content_block_delta\" event indicates the position of the content block being updated within the overall message structure. Each \"content_block_delta\" event corresponds to a specific content block, and the \"index\" helps identify which block is being modified. \n",
      "\n",
      "In the context of the text being streamed in a response, the \"index\" allows the receiving system to correctly place the incoming text updates into the appropriate location in the overall message. For example, if multiple content blocks are being streamed, each with its own deltas, the \"index\" ensures that the updates are applied to the correct block, maintaining the integrity and order of the streamed content.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes the role of the \"index\" field in the \"content_block_delta\" event, indicating which content block is being updated and how it relates to the overall message structure. It emphasizes the importance of the \"index\" in ensuring that text updates are applied to the correct block, which aligns with the correct answer's explanation. Both answers convey the same essential information about the function of the \"index\" in streaming text responses. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [01:45<04:35,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you include an image as part of a Claude API request, and what image formats are currently supported?\n",
      "Correct answer:\n",
      "To include an image in a Claude API request, provide it as a base64-encoded image in an \"image\" content block within the \"messages\" array. The currently supported image formats are JPEG, PNG, GIF, and WebP.\n",
      "Generated anser:\n",
      "To include an image as part of a Claude API request, you can upload the image directly in the request. The API supports the following image formats: JPEG, PNG, GIF, and WebP, specifically identified as image/jpeg, image/png, image/gif, and image/webp.\n",
      "\n",
      "There are some limitations to keep in mind:\n",
      "- The maximum file size for images uploaded via the API is 5MB.\n",
      "- You can include up to 20 images in a single request.\n",
      "\n",
      "It's important to note that Claude cannot read image URLs, and images must be uploaded directly rather than linked. Additionally, Claude does not retain uploaded images after processing, as they are ephemeral and not stored.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides a different method for including images in a Claude API request, stating that images can be uploaded directly, while the Correct Answer specifies that images should be provided as base64-encoded data in an \"image\" content block. This is a critical difference in how images are included in the request. Additionally, the Generated Answer does not mention the requirement of base64 encoding, which is essential information. Therefore, the Generated Answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [01:49<04:43,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?\n",
      "Correct answer:\n",
      "TTFT is a specific measure of latency that captures the time it takes for a language model to generate the first token of its response after receiving a prompt. It is an important component of a model's overall latency and responsiveness, especially for interactive applications.\n",
      "Generated anser:\n",
      "The relationship between time to first token (TTFT) and latency when evaluating a language model's performance is that TTFT is a specific component of latency. TTFT measures the time it takes for a model to generate the first token of its output after receiving a prompt, while latency refers to the overall time taken for the model to respond to the prompt, which includes TTFT as well as any additional processing time for subsequent tokens.\n",
      "\n",
      "A lower TTFT contributes to lower overall latency, enhancing the model's responsiveness, especially in interactive applications where users expect quick feedback. Factors influencing both TTFT and latency include model size, hardware capabilities, network conditions, and the complexity of the prompt. Therefore, optimizing TTFT can lead to improved latency, resulting in a better user experience.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the relationship between time to first token (TTFT) and latency, stating that TTFT is a specific component of latency and explaining how it contributes to overall responsiveness. It also mentions factors influencing both TTFT and latency, which adds depth to the explanation. Therefore, the generated answer is correct as it conveys the same essential information as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [01:56<05:36,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?\n",
      "Correct answer:\n",
      "Providing edge case examples to Claude in the prompt can meaningfully improve its performance in correctly routing support tickets in scenarios where it may otherwise misclassify them, such as implicit requests, emotional prioritization, ambiguous intent vs. routing, or issue prioritization.\n",
      "Generated anser:\n",
      "Providing Claude with examples of handling edge cases like implicit requests and emotional prioritization can significantly enhance its performance in routing support tickets. \n",
      "\n",
      "1. **Implicit Requests**: Customers often communicate their needs indirectly. By including examples of such scenarios in the training data, Claude can learn to recognize phrases that imply a request, such as \"I've been waiting for my package for over two weeks now.\" This understanding allows Claude to accurately identify the customer's intent and route the ticket to the appropriate team for order status inquiries, rather than misclassifying it as a general complaint.\n",
      "\n",
      "2. **Emotional Prioritization**: Customers frequently express emotions related to their issues, such as frustration or dissatisfaction. Providing Claude with guidelines on when to prioritize addressing these emotions can help it better manage tickets where customer sentiment is critical. For instance, if a customer expresses anger about a service failure, Claude can be trained to prioritize empathetic responses or escalate the ticket to a team that specializes in handling sensitive situations, ensuring that the emotional aspect is addressed alongside the technical issue.\n",
      "\n",
      "3. **Intent vs. Routing**: There are instances where Claude may correctly identify a customer's intent but fail to route the ticket appropriately. By clarifying the correct routing for various intents, especially those that are ambiguous, Claude can improve its accuracy in directing tickets to the right teams. This reduces the chances of misrouting and enhances overall efficiency.\n",
      "\n",
      "4. **Issue Prioritization**: Customers may present multiple issues in a single interaction, which can confuse Claude. By providing examples that clarify how to prioritize these issues, Claude can learn to identify the primary concern more effectively. This ensures that the most pressing issues are addressed first, improving customer satisfaction and support team efficiency.\n",
      "\n",
      "Regularly reviewing and refining the prompts with these edge case examples will help maintain Claude's effectiveness as the system evolves, ensuring it adapts to changing customer needs and improves its routing accuracy over time.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by discussing how providing examples of edge cases like implicit requests and emotional prioritization can enhance Claude's performance in routing support tickets. It elaborates on various aspects such as intent vs. routing and issue prioritization, which are also mentioned in the correct answer. While the generated answer is more detailed, it does not omit any critical information and aligns well with the main points of the correct answer. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 30/100 questions. Current Accuracy: 0.6667\n",
      "Number of tokens: 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [02:01<05:32,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does the stop_reason of \"tool_use\" relate to the overall workflow of integrating external tools with Claude?\n",
      "Correct answer:\n",
      "When Claude determines that one of the user-provided tools can help answer the user's query, it constructs a tool use request. This causes the API response to have a stop_reason of \"tool_use\", signaling Claude's intent to use the tool. The user must then extract the tool input from Claude's request, run the actual tool code client-side, and continue the conversation by sending the tool results back to Claude.\n",
      "Generated anser:\n",
      "The stop_reason of \"tool_use\" indicates that Claude has determined that using an external tool is necessary to effectively respond to the user's query. This is a critical part of the workflow for integrating external tools with Claude, as it signifies the transition from generating a response based solely on its internal knowledge to leveraging external capabilities for more accurate or specific information.\n",
      "\n",
      "In the overall workflow, the process begins with providing Claude with the necessary tools and a user prompt. Once the tools are defined, Claude evaluates whether any of them can assist in answering the user's question. If it identifies a relevant tool, it constructs a tool use request, which is marked by the stop_reason of \"tool_use.\" This signals that Claude is shifting to an external tool to gather information.\n",
      "\n",
      "Subsequently, the user or client-side system extracts the tool name and input from Claude's request, executes the tool, and returns the results. Finally, Claude uses the results from the tool to formulate a comprehensive response to the original user prompt. This workflow emphasizes the importance of the \"tool_use\" stop_reason as a key indicator of when external resources are being utilized to enhance the interaction and improve the quality of responses.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the role of the stop_reason \"tool_use\" in the workflow of integrating external tools with Claude. It explains that this stop_reason indicates Claude's decision to use an external tool, which aligns with the Correct Answer. Both answers detail the process of extracting tool input, executing the tool, and returning results to Claude. The Generated Answer includes all critical pieces of information found in the Correct Answer and does not contradict it. Therefore, it is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [02:03<04:39,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\n",
      "Correct answer:\n",
      "During periods of high usage, an overloaded_error event may be sent in the event stream, which would normally correspond to an HTTP 529 error code in a non-streaming context.\n",
      "Generated anser:\n",
      "During periods of high usage for the Anthropic API when using streaming responses, you may receive an `overloaded_error`, which corresponds to an HTTP error code of 529.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. It mentions the `overloaded_error` event and the corresponding HTTP error code of 529 during periods of high usage for the Anthropic API when using streaming responses. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [02:06<04:03,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?\n",
      "Correct answer:\n",
      "The two types of deltas that can be contained in a content_block_delta event are text_delta and input_json_delta.\n",
      "Generated anser:\n",
      "The two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API are:\n",
      "\n",
      "1. **input_json_delta** - This type corresponds to updates for the input field of the block and consists of partial JSON strings.\n",
      "\n",
      "2. **message_delta** - While not explicitly detailed in the provided context, it is mentioned as part of the streaming response structure, indicating that it may also be a type of delta associated with content blocks.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly identifies \"message_delta\" as a type of delta, which is not mentioned in the correct answer. The correct answer specifies that the two types of deltas are \"text_delta\" and \"input_json_delta,\" and the generated answer fails to include \"text_delta\" while introducing an unverified type. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [02:09<03:39,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\n",
      "Correct answer:\n",
      "Claude 3.5 Sonnet became generally available across those platforms on June 20th, 2024, while tool use became generally available on May 30th, 2024.\n",
      "Generated anser:\n",
      "Claude 3.5 Sonnet and tool use both became generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI on June 20th, 2024.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer incorrectly states that both Claude 3.5 Sonnet and tool use became generally available on June 20th, 2024. However, the correct answer specifies that Claude 3.5 Sonnet became available on June 20th, 2024, while tool use became available earlier on May 30th, 2024. This critical piece of information about the different dates is missing from the generated answer, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [02:11<03:23,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?\n",
      "Correct answer:\n",
      "Anthropic launched Claude.ai and the Claude iOS app in Europe in May 2024, and then launched them in Canada the following month in June 2024.\n",
      "Generated anser:\n",
      "Claude.ai and the Claude iOS app were launched in Canada on June 5th, 2024, and in Europe on May 13th, 2024. Therefore, the order of launch was Europe first, followed by Canada.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that Claude.ai and the Claude iOS app were launched in Canada on June 5th, 2024, and in Europe on May 13th, 2024. This contradicts the correct answer, which states that the launch in Europe occurred in May 2024 and the launch in Canada followed in June 2024. The specific dates provided in the generated answer are not aligned with the correct answer, leading to a direct contradiction regarding the order and timing of the launches.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [02:16<03:44,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", what does this indicate and what should be done next to continue the conversation?\n",
      "Correct answer:\n",
      "A stop_reason of \"tool_use\" signals that Claude has decided to use a tool and has constructed a formatted tool use request. To continue the conversation, the tool name and input should be extracted from Claude's request, the actual tool code should be executed client-side, and then a new user message containing a tool_result content block should be sent to Claude.\n",
      "Generated anser:\n",
      "When the API response from Claude has a stop_reason of \"tool_use\", it indicates that Claude has determined that a tool is necessary to assist with the user's query. This means Claude is signaling its intent to use one of the external tools you have provided.\n",
      "\n",
      "To continue the conversation, you should:\n",
      "\n",
      "1. **Extract the tool information**: Identify the tool name, the unique identifier (id), and the input parameters from the tool_use block in the response.\n",
      "\n",
      "2. **Execute the tool**: Run the corresponding tool in your codebase, passing in the extracted input parameters.\n",
      "\n",
      "3. **Prepare the tool result**: After executing the tool, format the result into a new message with the role of \"user\". This message should include a content block of type \"tool_result\" that contains the tool_use_id and the result of the tool execution.\n",
      "\n",
      "4. **Send the result back to Claude**: This allows Claude to use the tool's output to generate a final response to the original user prompt.\n",
      "\n",
      "By following these steps, you can effectively manage the tool use and keep the conversation flowing.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the process to follow when the API response from Claude has a stop_reason of \"tool_use\". It correctly identifies the need to extract tool information, execute the tool, prepare the result, and send it back to Claude. The steps outlined in the generated answer align well with the correct answer's content. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [02:18<03:11,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?\n",
      "Correct answer:\n",
      "The example code snippet for evaluating tone and style in a customer service chatbot uses the anthropic Python library to interact with the Claude AI model.\n",
      "Generated anser:\n",
      "The Python library used in the example code snippet for evaluating tone and style in a customer service chatbot is `anthropic`.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the `anthropic` Python library as the one used in the example code snippet for evaluating tone and style in a customer service chatbot. It captures the essential information from the correct answer, which states that the library is used to interact with the Claude AI model. Although the generated answer does not explicitly mention the interaction with the Claude AI model, it still conveys the main point about the library being used for the chatbot's tone and style evaluation. Therefore, it can be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [02:20<02:54,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?\n",
      "Correct answer:\n",
      "The two main ways to authenticate are: 1) Directly providing the aws_access_key, aws_secret_key, and optionally aws_session_token, or 2) Using the default AWS credential providers, such as the ~/.aws/credentials file or the AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID environment variables.\n",
      "Generated anser:\n",
      "The two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock are:\n",
      "\n",
      "1. By providing AWS access keys directly in the code using `aws_access_key` and `aws_secret_key`.\n",
      "2. By using the default AWS credential providers, such as the credentials stored in `~/.aws/credentials` or the environment variables `AWS_SECRET_ACCESS_KEY` and `AWS_ACCESS_KEY_ID`.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock. It mentions providing AWS access keys directly and using default AWS credential providers, which aligns with the Correct Answer. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [02:23<03:06,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\n",
      "Correct answer:\n",
      "When deciding to use leak-resistant prompt engineering, the potential reduction in prompt leaks should be balanced against the risk of degraded model performance due to the added complexity of the prompt.\n",
      "Generated anser:\n",
      "When deciding whether to implement leak-resistant prompt engineering strategies, two key factors to consider and balance are:\n",
      "\n",
      "1. **Complexity of the Prompt**: Implementing leak-resistant strategies can add complexity to the prompt, which may negatively impact the model's performance and the quality of its outputs. It's important to ensure that the added complexity does not hinder the effectiveness of the model in performing its tasks.\n",
      "\n",
      "2. **Risk of Information Leakage**: The necessity to protect sensitive or proprietary information from being leaked is crucial. If the risk of leakage is significant, it may warrant the implementation of leak-resistant strategies, but this must be weighed against the potential degradation in performance due to increased complexity.\n",
      "\n",
      "Balancing these two factors—complexity and risk of leakage—is essential to achieve optimal performance while safeguarding sensitive information.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two key factors to consider when implementing leak-resistant prompt engineering strategies: the complexity of the prompt and the risk of information leakage. It also emphasizes the need to balance these factors, which aligns with the correct answer's essence. Therefore, the generated answer is correct as it conveys the same fundamental ideas without omitting critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [02:28<03:21,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?\n",
      "Correct answer:\n",
      "Choosing the right Claude model that best fits your needs in terms of speed and output quality is one of the most straightforward ways to reduce latency in your application. Anthropic offers a range of Claude models with different capabilities and performance characteristics to allow you to choose the optimal balance of intelligence, speed, and cost for your use case.\n",
      "Generated anser:\n",
      "Selecting the appropriate Claude model based on your specific requirements can significantly reduce latency in your application by ensuring that you choose a model that aligns with the complexity of the tasks you need to perform. Different models within the Claude family have varying capabilities and performance characteristics, which means that some models may be optimized for speed while still providing adequate output quality for simpler tasks.\n",
      "\n",
      "For instance, if your application requires quick responses for straightforward queries, opting for a model designed for speed can minimize processing time and reduce latency. Conversely, if the task involves complex reasoning or multi-step analysis, a more advanced model may be necessary, but it could also lead to increased latency due to the additional processing required.\n",
      "\n",
      "Additionally, being mindful of output length and the use of techniques like Chain of Thought (CoT) can further help manage latency. CoT should be used judiciously, as it is beneficial for tasks that require deep thinking but may slow down response times if applied unnecessarily. By carefully selecting the model and approach based on the specific needs of your application, you can achieve a better balance between performance and latency.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how selecting the appropriate Claude model can help reduce latency by aligning the model's capabilities with the complexity of the tasks. It emphasizes the importance of choosing a model optimized for speed for simpler tasks and acknowledges that more advanced models may increase latency. Additionally, it mentions the consideration of output length and techniques like Chain of Thought, which aligns with the correct answer's focus on balancing intelligence, speed, and cost. Overall, the generated answer captures the essence of the correct answer without omitting critical information or introducing contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 40/100 questions. Current Accuracy: 0.6750\n",
      "Number of tokens: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [02:31<03:26,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you stream responses from the Anthropic API using the Python SDK?\n",
      "Correct answer:\n",
      "You can stream responses from the Anthropic API using the Python SDK by using the client.messages.stream() method and iterating over the stream.text_stream attribute in a for loop.\n",
      "Generated anser:\n",
      "To stream responses from the Anthropic API using the Python SDK, you can use the following code snippet:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "client = anthropic.Anthropic()\n",
      "\n",
      "with client.messages.stream(\n",
      "    max_tokens=1024,\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      ") as stream:\n",
      "    for text in stream.text_stream:\n",
      "        print(text, end=\"\", flush=True)\n",
      "```\n",
      "\n",
      "This code initializes the Anthropic client, sets up a streaming request with specified parameters (like `max_tokens`, `messages`, and `model`), and then iterates over the streamed text responses, printing them in real-time.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how to stream responses from the Anthropic API using the Python SDK by mentioning the use of the `client.messages.stream()` method and iterating over `stream.text_stream`. It provides a code snippet that illustrates the process, which aligns with the correct answer's essence. Therefore, it captures the necessary information and is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [02:34<03:07,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\n",
      "Correct answer:\n",
      "You can shape Claude's response by pre-filling part of it in the last position of the input messages list. To get a short response like a single multiple choice answer, you can set the \"max_tokens\" parameter to a small value like 1.\n",
      "Generated anser:\n",
      "To guide Claude's response by pre-filling part of the response, you can include the desired initial text in the Assistant message. This way, Claude will continue from where the Assistant message leaves off. \n",
      "\n",
      "The API parameter used to generate a short response in this case is `max_tokens`, which you can set to limit the length of the response. For example, setting `max_tokens=1024` allows for a longer response, while a smaller value would yield a shorter response.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can guide Claude's response by pre-filling part of the response in the Assistant message, which aligns with the correct answer's idea of shaping the response. However, it does not explicitly mention that the pre-filled text should be in the last position of the input messages list, which is a critical detail. Additionally, while it discusses the `max_tokens` parameter, it does not specify that a small value like 1 is recommended for generating a short response, which is also a key point from the correct answer. Therefore, the generated answer is missing critical information and is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [02:37<02:58,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\n",
      "Correct answer:\n",
      "When building an eval set, it is better to prioritize having a larger volume of test cases with slightly lower signal automated grading over having fewer questions with high-quality human hand-grading.\n",
      "Generated anser:\n",
      "When building an eval set for an AI system, having a larger number of test cases with automated grading is generally more important than having fewer high-quality test cases graded by humans. The principles suggest prioritizing volume over quality, as more questions with slightly lower signal from automated grading can provide better coverage of the task distribution and edge cases. Automated grading methods are faster, more reliable, and scalable, making them preferable for evaluating performance across a wider range of scenarios. Human grading, while high quality, is slow and expensive, and should be avoided if possible.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the essence of the correct answer by emphasizing the importance of having a larger number of test cases with automated grading over fewer high-quality test cases graded by humans. It discusses the benefits of volume and automated grading, which aligns with the correct answer's message. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [02:39<02:42,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two required fields in a content_block_delta event for a text delta type?\n",
      "Correct answer:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\", where the \"delta\" field contains a \"type\" of \"text_delta\" and the \"text\" being added.\n",
      "Generated anser:\n",
      "The two required fields in a content_block_delta event for a text delta type are \"index\" and \"delta\". The \"delta\" field must contain a \"type\" field with the value \"text_delta\" and a \"text\" field that holds the actual text content.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two required fields \"index\" and \"delta\" for a content_block_delta event of type text delta. It also accurately states that the \"delta\" field must contain a \"type\" field with the value \"text_delta\" and a \"text\" field that holds the actual text content. Therefore, it conveys the same essential information as the correct answer without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [02:43<02:50,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\n",
      "Correct answer:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks demonstrating how to upload PDFs, generate embeddings, and more. The Developer Console offers a prompt generator tool for easier, more powerful prompting.\n",
      "Generated anser:\n",
      "Two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings, are:\n",
      "\n",
      "1. **Prompt Engineering Interactive Tutorial**: This tutorial utilizes Claude for Sheets and provides an in-depth, hands-on approach to learning prompt engineering techniques. Users can interact with Claude directly, which allows for practical experience in using its capabilities. An API key is required to access this tutorial.\n",
      "\n",
      "2. **Using the Console Workbench**: By selecting a Claude model that accepts images, users can interactively upload files, including PDFs, and explore how Claude processes and generates embeddings from the uploaded content. This method allows for direct experimentation with Claude's features in a user-friendly environment.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two interactive ways to learn about Claude's capabilities, but it does not mention the Anthropic Cookbook, which is a key part of the correct answer. Instead, it focuses on a Prompt Engineering Interactive Tutorial and the Console Workbench. While these may be valid methods, they do not align with the specific examples given in the correct answer. Therefore, the generated answer is missing critical information and does not fully address the question as posed.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [02:50<03:51,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?\n",
      "Correct answer:\n",
      "Breaking a task into distinct subtasks for chained prompts improves Claude's accuracy because each subtask gets Claude's full attention, reducing errors compared to tackling the entire complex task at once.\n",
      "Generated anser:\n",
      "Breaking a task into distinct subtasks for chained prompts helps improve Claude's accuracy on the overall task for several reasons:\n",
      "\n",
      "1. **Focused Attention**: Each subtask allows Claude to concentrate fully on a specific part of the task, which reduces the likelihood of errors that can occur when handling complex or multifaceted instructions all at once.\n",
      "\n",
      "2. **Clarity of Instructions**: Simpler subtasks lead to clearer instructions and outputs. When tasks are broken down, the expectations for each step are more straightforward, making it easier for Claude to understand and execute them correctly.\n",
      "\n",
      "3. **Traceability**: By structuring tasks into subtasks, it becomes easier to identify and address any issues that arise. If a mistake occurs, it can be traced back to a specific subtask, allowing for targeted adjustments rather than having to revise the entire task.\n",
      "\n",
      "4. **Structured Thinking**: Allowing Claude to think through problems step-by-step enhances coherence and organization in responses. This structured approach is particularly beneficial for complex tasks that require logical reasoning or analysis.\n",
      "\n",
      "5. **Iterative Refinement**: The process of breaking tasks down into subtasks enables iterative refinement based on Claude’s performance. If a particular subtask is not yielding the desired results, it can be adjusted without affecting the entire task flow.\n",
      "\n",
      "Overall, these factors contribute to a more accurate and efficient completion of tasks when using chained prompts.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly captures the essence of the correct answer by explaining that breaking a task into subtasks allows Claude to focus on each part, which reduces errors. It also elaborates on additional benefits such as clarity of instructions, traceability, structured thinking, and iterative refinement, all of which support the idea that subtasks improve accuracy. Since the generated answer aligns with the main point of the correct answer and expands on it without contradicting or omitting critical information, it is deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [02:55<03:55,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?\n",
      "Correct answer:\n",
      "Messages streaming responses can contain multiple content blocks of varying types, making the streaming format more complex compared to Text Completions which only include completion, ping, and error server-sent-events.\n",
      "Generated anser:\n",
      "The streaming format for Messages responses differs from Text Completions streaming responses in several key ways:\n",
      "\n",
      "1. **Event Structure**: \n",
      "   - In Text Completions, the streaming response includes events such as completion, ping, and error server-sent-events. \n",
      "   - For Messages, the streaming format is more complex and includes a `message_start` event, followed by potentially multiple content blocks, each containing:\n",
      "     - A `content_block_start` event\n",
      "     - Multiple `content_block_delta` events\n",
      "     - A `content_block_stop` event\n",
      "   - Additionally, there are `message_delta` and `message_stop` events in the Messages streaming format.\n",
      "\n",
      "2. **Content Blocks**: \n",
      "   - Messages can contain multiple content blocks of varying types, which allows for more structured and varied responses compared to the simpler completion format of Text Completions.\n",
      "\n",
      "3. **Input Specification**: \n",
      "   - Text Completions uses a raw string format for inputs, alternating between \"Human\" and \"Assistant\" prompts.\n",
      "   - Messages require a list of input messages, each with a specified role (\"user\" or \"assistant\") and content, allowing for a more organized representation of dialogue.\n",
      "\n",
      "4. **Response Structure**: \n",
      "   - In Text Completions, the generated text is returned in the `completion` value of the response.\n",
      "   - In Messages, the response is structured as a list of content blocks, with the content being accessed through the `content` value, which contains a list of objects detailing the type and text of each message.\n",
      "\n",
      "These differences reflect the more complex nature of handling conversational data in the Messages format compared to the simpler, linear format of Text Completions.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately describes the differences between Messages streaming responses and Text Completions streaming responses. It highlights the complexity of the Messages format, including the event structure and the presence of multiple content blocks, which aligns with the Correct Answer. Additionally, it discusses the input specification and response structure, providing a comprehensive overview that is consistent with the essence of the Correct Answer. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [03:00<04:11,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\n",
      "Correct answer:\n",
      "According to the documentation, users can start experimenting with Claude by visiting claude.ai or using Anthropic's web Console.\n",
      "Generated anser:\n",
      "Two ways to start experimenting with Claude as a user, according to Anthropic's documentation, are:\n",
      "\n",
      "1. Follow the Quickstart to make your first API call.\n",
      "2. Experiment and start building with the Workbench.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer provides two different methods for experimenting with Claude: following the Quickstart to make an API call and using the Workbench. However, these methods do not match the two methods mentioned in the Correct Answer, which are visiting claude.ai and using Anthropic's web Console. Since the Generated Answer does not include the correct methods and instead presents alternative methods, it is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [03:04<03:49,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?\n",
      "Correct answer:\n",
      "Chain prompts break complex tasks into smaller subtasks, allowing Claude to give its full attention to each one. This reduces errors and inconsistencies that may occur when trying to handle a complex workflow all at once.\n",
      "Generated anser:\n",
      "Using chain prompts can significantly reduce errors and inconsistencies in complex tasks handled by Claude by breaking down the overall task into smaller, manageable subtasks. This approach allows each subtask to receive Claude's full attention, which minimizes the likelihood of errors that can arise from handling overly complex instructions all at once.\n",
      "\n",
      "By structuring tasks into simpler subtasks, clarity is enhanced, leading to clearer instructions and outputs. This clarity helps ensure that each step is understood and executed correctly, reducing the chances of misinterpretation or oversight.\n",
      "\n",
      "Additionally, chain prompts improve traceability. When tasks are divided into distinct steps, it becomes easier to identify where issues may have occurred in the process. If an error arises, it can be pinpointed to a specific subtask, allowing for targeted corrections rather than having to troubleshoot the entire task.\n",
      "\n",
      "Overall, the structured approach of chain prompts not only enhances accuracy and clarity but also facilitates better management of complex workflows, leading to more consistent and reliable outcomes.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the main points of the correct answer. It explains how chain prompts break down complex tasks into smaller subtasks, allowing Claude to focus on each one, which reduces errors and inconsistencies. Additionally, it discusses the benefits of clarity and traceability, which are implied in the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [03:06<03:12,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?\n",
      "Correct answer:\n",
      "In a non-streaming context, an overloaded_error event would normally correspond to an HTTP 529 status code.\n",
      "Generated anser:\n",
      "An overloaded_error event corresponds to an HTTP status code of 529 in a non-streaming context for the Anthropic API.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that an overloaded_error event corresponds to an HTTP status code of 529 in a non-streaming context for the Anthropic API, which matches the correct answer provided. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 50/100 questions. Current Accuracy: 0.6800\n",
      "Number of tokens: 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [03:09<02:50,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?\n",
      "Correct answer:\n",
      "When making a request to Voyage AI's embedding endpoint, you can either leave the encoding_format parameter unspecified to get the embeddings as lists of floating-point numbers, or set encoding_format to \"base64\" to get the embeddings compressed to Base64 encodings.\n",
      "Generated anser:\n",
      "The two ways to specify the format in which Voyage AI returns embeddings through its HTTP API are:\n",
      "\n",
      "1. By default, the embeddings are represented as lists of floating-point numbers.\n",
      "2. By setting the `encoding_format` parameter to \"base64\", the embeddings are compressed to Base64 encodings.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two ways to specify the format for embeddings in Voyage AI's HTTP API: it mentions the default representation as lists of floating-point numbers and the option to set the `encoding_format` parameter to \"base64\". However, it does not explicitly mention that the `encoding_format` parameter can be left unspecified to achieve the default behavior. This omission is a critical piece of information, as it does not fully capture the essence of the correct answer. Therefore, the generated answer is not entirely correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [03:14<03:06,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\n",
      "Correct answer:\n",
      "When streaming requests with tool use, the input JSON deltas for tool_use content blocks are sent as partial JSON strings in multiple content_block_delta events. The client can accumulate these partial JSON strings and parse the complete JSON object once a content_block_stop event is received, using a library like Pydantic for partial JSON parsing or helpers provided in Anthropic's SDKs.\n",
      "Generated anser:\n",
      "When streaming API requests that use tools, the input JSON deltas for `tool_use` content blocks are sent as partial JSON strings. Each delta corresponds to updates for the input field of the block, allowing for maximum granularity. The final `tool_use.input` is always an object.\n",
      "\n",
      "The deltas are accumulated by the client until a `content_block_stop` event is received. At this point, the client can parse the accumulated string deltas into a complete JSON object. This can be achieved using libraries like Pydantic for partial JSON parsing or by utilizing SDKs that provide helpers for accessing parsed incremental values.\n",
      "\n",
      "For example, a `tool_use` content block delta may look like this:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"event\": \"content_block_delta\",\n",
      "  \"data\": {\n",
      "    \"type\": \"content_block_delta\",\n",
      "    \"index\": 1,\n",
      "    \"delta\": {\n",
      "      \"type\": \"input_json_delta\",\n",
      "      \"partial_json\": \"{\\\"location\\\": \\\"San Fra\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Multiple such events may occur, each providing a `partial_json` string that contributes to the final input object. Once the `content_block_stop` event is received, the client can combine these partial strings to form the complete JSON input for the tool.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly describes how the input JSON deltas for `tool_use` content blocks are sent as partial JSON strings and that they are accumulated until a `content_block_stop` event is received. It also mentions the use of libraries like Pydantic for parsing, which aligns with the correct answer. Both answers convey the same essential information regarding the streaming API requests and the handling of JSON deltas, without any critical omissions or contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [03:17<02:46,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\n",
      "Correct answer:\n",
      "Anthropic offers a GitHub prompting tutorial that covers prompt engineering concepts in-depth with examples, and a lighter-weight Google Sheets prompting tutorial that utilizes Claude for Sheets.\n",
      "Generated anser:\n",
      "Anthropic offers two interactive prompt engineering tutorials: one that utilizes Claude for Sheets and another that includes a prompt generator in the Anthropic Console. \n",
      "\n",
      "The tutorial using Claude for Sheets focuses on teaching and refining prompt engineering techniques through direct interaction with the Claude model. Users need an API key to access this tutorial.\n",
      "\n",
      "The prompt generator, on the other hand, is designed for those who do not have a first draft prompt. It assists users in creating initial prompts that can then be improved upon using the techniques learned in the Claude for Sheets tutorial. \n",
      "\n",
      "In summary, the Claude for Sheets tutorial is more hands-on and interactive, while the prompt generator serves as a tool for generating initial prompts.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies one of the tutorials as utilizing Claude for Sheets, which aligns with the correct answer. However, it introduces a new tutorial involving a prompt generator in the Anthropic Console, which is not mentioned in the correct answer. The correct answer specifies a GitHub prompting tutorial instead. This discrepancy indicates that the generated answer includes critical information that is not present in the correct answer, leading to a contradiction. Therefore, the generated answer is incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [03:23<03:27,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?\n",
      "Correct answer:\n",
      "Claude offers a 200K token context window, tool use for integration into specialized applications, multimodal input capabilities for richer context, and is uniquely positioned to serve high-trust industries processing large volumes of sensitive data with enterprise-grade security and data handling.\n",
      "Generated anser:\n",
      "Claude is suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data due to several key capabilities:\n",
      "\n",
      "1. **Enterprise-Grade Security**: Claude offers SOC II Type 2 certification and HIPAA compliance options, ensuring robust security and data handling for sensitive information.\n",
      "\n",
      "2. **Trustworthiness**: The model is designed to be resistant to jailbreaks and misuse, with continuous monitoring of prompts and outputs to prevent harmful use cases. It also provides copyright indemnity protections for paid commercial services.\n",
      "\n",
      "3. **High Capacity Context Window**: With a 200K token context window, and future support for up to 1M tokens, Claude can handle extensive documents and complex use cases effectively.\n",
      "\n",
      "4. **Tool Use and Function Calling**: This feature allows seamless integration of Claude into specialized applications and custom workflows, enabling it to interact with external tools and execute actions through structured outputs.\n",
      "\n",
      "5. **Multimodal Input Capabilities**: Claude can process and analyze visual inputs alongside text, which is beneficial for tasks that require richer context, such as extracting insights from images or generating code from diagrams.\n",
      "\n",
      "6. **Developer Support**: The Developer Console, Workbench, and prompt generation tools facilitate easier and more powerful experimentation and development, enhancing integration capabilities.\n",
      "\n",
      "7. **Low Hallucination Rates and High Accuracy**: Claude demonstrates very low hallucination rates and maintains accuracy over long documents, making it reliable for critical enterprise applications.\n",
      "\n",
      "8. **Global Language Support**: Claude is fluent in multiple languages, enabling translation services and broader global utility, which is essential for enterprises operating in diverse markets.\n",
      "\n",
      "9. **Cost-Effective Model Options**: The availability of different models within the Claude family allows enterprises to choose the right balance of intelligence, speed, and cost according to their specific needs. \n",
      "\n",
      "These capabilities collectively make Claude a strong candidate for enterprises that need to integrate with specialized applications and manage sensitive data efficiently.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key capabilities of Claude that make it suitable for enterprise use cases, including the 200K token context window, tool use for integration, multimodal input capabilities, and enterprise-grade security. It also includes additional relevant details such as trustworthiness, developer support, low hallucination rates, global language support, and cost-effective model options, which enhance the overall understanding of Claude's capabilities. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [03:25<02:42,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\n",
      "Correct answer:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in the United States, Canada, and Europe.\n",
      "Generated anser:\n",
      "As of June 2024, Anthropic's Claude.ai API and iOS app are available in Canada and Europe.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is missing the information that the Claude.ai API and iOS app are also available in the United States, which is a critical piece of information present in the correct answer. Therefore, the generated answer is incomplete and should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [03:29<02:47,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\n",
      "Correct answer:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are push-based using webhooks, and pull-based. The push-based approach is more web-scalable but requires exposing a public endpoint which has IT security implications. The pull-based approach is easier to implement but makes unnecessary calls to the support ticket system.\n",
      "Generated anser:\n",
      "The two main approaches for integrating Claude into a support ticket workflow are the push-based approach and the pull-based approach.\n",
      "\n",
      "1. **Push-Based Approach**: In this method, the support ticket system (like Zendesk) triggers the integration by sending a webhook event whenever a new ticket is created. This event prompts the routing service to classify the intent of the ticket and route it accordingly. This approach is more scalable because it allows for real-time processing of tickets as they are created, reducing latency and ensuring that tickets are handled promptly. However, it requires exposing a public endpoint, which can raise IT security concerns.\n",
      "\n",
      "2. **Pull-Based Approach**: This approach involves the integration periodically polling the support ticket system for new tickets at set intervals. The integration then processes these tickets and routes them. While this method is easier to implement since it does not require setting up a public endpoint, it is less scalable. It can lead to unnecessary API calls to the support ticket system, especially if there are few or no new tickets during a polling interval, which can waste resources.\n",
      "\n",
      "In summary, the push-based approach offers better scalability but comes with security implications, while the pull-based approach is easier to implement but less efficient in terms of resource use.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the two main approaches (push-based and pull-based) for integrating Claude into a support ticket workflow and provides a detailed explanation of each approach. It accurately describes the push-based approach as more scalable but with security implications, and the pull-based approach as easier to implement but less efficient due to unnecessary API calls. Therefore, the generated answer captures the essential information and differences outlined in the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [03:31<02:22,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\n",
      "Correct answer:\n",
      "On May 10th, 2024, Anthropic released a prompt generator tool that is available through the Developer Console.\n",
      "Generated anser:\n",
      "Anthropic released a prompt generator tool to help guide Claude in generating high-quality prompts on June 20th, 2024. This tool is available through the Developer Console, specifically in the Workbench interface.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it states that the prompt generator tool was released on June 20th, 2024, while the correct answer states it was released on May 10th, 2024. This is a critical piece of information that directly contradicts the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [03:34<02:14,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?\n",
      "Correct answer:\n",
      "The Claude 3 Sonnet model balances intelligence and speed, making it well-suited for high-throughput tasks like sales forecasting and targeted marketing.\n",
      "Generated anser:\n",
      "The Claude 3 Sonnet model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing. It is designed for strong utility and is balanced for scaled deployments, offering quick and accurate performance while maintaining a high level of intelligence and capability.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the Claude 3 Sonnet model as providing the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing. It elaborates on the model's design for strong utility and its suitability for scaled deployments, which aligns with the essence of the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [03:39<02:29,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\n",
      "Correct answer:\n",
      "You can calculate the similarity between two Voyage embedding vectors using the dot product, which is equivalent to cosine similarity since Voyage embeddings are normalized to length 1.\n",
      "Generated anser:\n",
      "To calculate the similarity between two Voyage embedding vectors, you can use the dot product of the two vectors. Since Voyage embeddings are normalized to length 1, the cosine similarity is equivalent to the dot product. You can compute it using the following code snippet:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "similarity = np.dot(embd1, embd2)\n",
      "```\n",
      "\n",
      "This will give you the cosine similarity between the two embedding vectors, which ranges from -1 to 1, where 1 indicates that the vectors are identical in direction, 0 indicates orthogonality, and -1 indicates opposite directions.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that the similarity between two Voyage embedding vectors can be calculated using the dot product and that this is equivalent to cosine similarity since the embeddings are normalized to length 1. It also provides a code snippet for calculating the dot product, which is relevant and helpful. The explanation of the range of cosine similarity values is also accurate. Therefore, the generated answer is essentially conveying the same information as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [03:44<02:46,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?\n",
      "Correct answer:\n",
      "Well-chosen examples in prompts can boost Claude's ability to handle complex tasks by reducing misinterpretation of instructions, enforcing consistent structure and style, and serving as a guide for the desired output.\n",
      "Generated anser:\n",
      "Using examples in prompts can significantly enhance Claude's performance on complex tasks in several ways:\n",
      "\n",
      "1. **Accuracy**: Examples help clarify instructions, reducing the likelihood of misinterpretation. When Claude has concrete instances to reference, it can better understand the expected output and context.\n",
      "\n",
      "2. **Consistency**: By providing examples, you establish a uniform structure and style for Claude to follow. This consistency aids in generating responses that align with the desired format and tone.\n",
      "\n",
      "3. **Performance Boost**: Well-chosen examples can enhance Claude's ability to tackle complex tasks. They serve as a guide, allowing the model to draw parallels and apply learned patterns to new situations effectively.\n",
      "\n",
      "4. **Targeted Examples**: Utilizing targeted examples, especially through methods like vector databases for similarity searches, can provide Claude with relevant context that improves accuracy. This approach has been shown to significantly increase performance metrics, as evidenced by improvements from 71% to 93% accuracy in specific tasks.\n",
      "\n",
      "5. **Structured Thinking**: Examples can help Claude step through problems methodically, leading to more coherent and organized responses. This structured approach is particularly beneficial in complex scenarios, such as math or logic problems.\n",
      "\n",
      "Overall, incorporating examples into prompts not only clarifies expectations but also enhances the model's ability to generate accurate and relevant outputs.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the essence of the correct answer by explaining how examples in prompts can improve Claude's performance on complex tasks. It discusses the reduction of misinterpretation, the establishment of consistency, and the guidance provided by examples, which aligns with the points made in the correct answer. Additionally, it introduces the concept of targeted examples and structured thinking, which further enriches the explanation without contradicting the original points. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 60/100 questions. Current Accuracy: 0.6667\n",
      "Number of tokens: 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [03:49<02:52,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\n",
      "Correct answer:\n",
      "When streaming responses with tool use, the two types of content block deltas are text deltas and input JSON deltas. Text deltas contain a \"text\" field with a string of the incrementally generated text. Input JSON deltas contain a \"partial_json\" field with a string containing part of the JSON object specifying the tool's input.\n",
      "Generated anser:\n",
      "The two types of content block deltas that can be emitted when streaming responses with tool use are:\n",
      "\n",
      "1. **Input JSON delta**: This delta type corresponds to updates for the input field of the content block. It contains a partial JSON string that represents a portion of the input data. The final input is always an object, and the deltas are emitted as multiple events to allow for finer granularity. An example of this delta type is:\n",
      "   ```json\n",
      "   {\n",
      "       \"type\": \"content_block_delta\",\n",
      "       \"index\": 1,\n",
      "       \"delta\": {\n",
      "           \"type\": \"input_json_delta\",\n",
      "           \"partial_json\": \"{\\\"location\\\": \\\"San Fra\\\"}\"\n",
      "       }\n",
      "   }\n",
      "   ```\n",
      "\n",
      "2. **Message delta**: While the specifics of the message delta are not detailed in the provided context, it is part of the overall streaming response structure. It typically indicates changes or updates to the message itself, separate from the content block deltas.\n",
      "\n",
      "In summary, the Input JSON delta contains partial JSON strings for input updates, while the Message delta pertains to updates related to the overall message.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it introduces a third type of delta, the \"Message delta,\" which is not mentioned in the correct answer. The correct answer specifies only two types of deltas: text deltas and input JSON deltas. The generated answer fails to include the text delta and does not accurately represent the types of deltas as described in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [03:53<02:35,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?\n",
      "Correct answer:\n",
      "Claude's question answering and text analysis capabilities enable it to build intelligent, interactive systems like chatbots and personalize user experiences by understanding sentiment and preferences.\n",
      "Generated anser:\n",
      "Two key capabilities of Claude that enable it to build interactive systems and personalized user experiences are:\n",
      "\n",
      "1. **Text and Code Generation**: Claude can summarize text, answer questions, extract data, translate text, and generate code. This capability allows for the creation of tailored interactions, such as chatbots that adhere to a specific brand voice and can engage users in a personalized manner.\n",
      "\n",
      "2. **Vision Processing**: Claude can process and analyze visual input, generating text and code from images. This ability enhances user experiences by enabling features like image description for users with low vision and extracting insights from visual data, making interactions more dynamic and accessible.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two capabilities of Claude: text and code generation, and vision processing. While these capabilities are relevant, they do not directly align with the key capabilities mentioned in the correct answer, which are specifically question answering and text analysis. The generated answer lacks the critical mention of understanding sentiment and preferences, which is essential for personalizing user experiences. Therefore, the generated answer is not correct as it misses key information and does not address the specific capabilities highlighted in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [03:56<02:25,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\n",
      "Correct answer:\n",
      "A raw HTTP stream response includes a message_start event, followed by one or more content blocks (each with a content_block_start, content_block_delta events, and content_block_stop), a message_delta event, and a final message_stop event. Ping events may also be dispersed throughout.\n",
      "Generated anser:\n",
      "The key event types included in a raw HTTP stream response when using message streaming, in typical order, are:\n",
      "\n",
      "1. **message_start**: This event indicates the beginning of a message and contains a Message object with empty content.\n",
      "2. **content_block_start**: This event marks the start of a content block.\n",
      "3. **content_block_delta**: There can be multiple of these events, which indicate changes or additions to the content block.\n",
      "4. **content_block_stop**: This event signifies the end of the content block.\n",
      "5. **message_delta**: This event indicates top-level changes to the final Message object.\n",
      "6. **message_stop**: This event marks the end of the message.\n",
      "\n",
      "Additionally, there may be **ping events** dispersed throughout the response.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly lists the key event types and their typical order as described in the correct answer. It includes all the necessary events: message_start, content_block_start, content_block_delta, content_block_stop, message_delta, and message_stop, along with the mention of ping events being dispersed throughout. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [03:59<02:08,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?\n",
      "Correct answer:\n",
      "The Messages API allows including up to 20 images per request, while the claude.ai interface has a lower limit of up to 5 images per turn.\n",
      "Generated anser:\n",
      "The maximum number of images that can be included in a single request using the Anthropic API is up to 20 images per request, while the claude.ai interface allows for up to 5 images per turn.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer, stating that the Anthropic API allows up to 20 images per request and the claude.ai interface allows for up to 5 images per turn. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [04:01<01:49,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\n",
      "Correct answer:\n",
      "If Claude's response hits the max_tokens limit and has an incomplete tool use block, you should retry the request with a higher max_tokens value to get Claude's full response including the complete tool use.\n",
      "Generated anser:\n",
      "If Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, you should retry the request with a higher max_tokens value to obtain the full tool use.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the correct answer's guidance on what to do when Claude's response is cut off due to the max_tokens limit. Both answers suggest retrying the request with a higher max_tokens value to obtain the full tool use, which means they convey the same essential information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [04:03<01:34,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?\n",
      "Correct answer:\n",
      "Before running a classification evaluation on Claude, you need to 1) develop your test cases, and 2) take a look at Anthropic's guide to developing test cases.\n",
      "Generated anser:\n",
      "The two steps needed before running a classification evaluation on Claude are: 1. Build a strong input prompt, and 2. Develop your test cases.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it omits the critical step of looking at Anthropic's guide to developing test cases, which is explicitly mentioned in the correct answer. While it includes the step of developing test cases, it incorrectly states the first step as \"Build a strong input prompt,\" which is not part of the correct answer. Therefore, the generated answer does not align with the correct answer in substance.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [04:06<01:36,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you use the content parameter in the messages list to influence Claude's response?\n",
      "Correct answer:\n",
      "You can provide content in the last position of the messages list, with the \"assistant\" role, to pre-fill part of Claude's response. This allows you to shape the assistant's output.\n",
      "Generated anser:\n",
      "You can use the content parameter in the messages list to influence Claude's response by pre-filling part of the assistant's reply. By including a message with the role of \"assistant\" and providing specific content, you set the stage for how Claude will continue the conversation. For example, if you want Claude to respond to a question about favorite colors, you can include a message like:\n",
      "\n",
      "```python\n",
      "{\"role\": \"assistant\", \"content\": \"As an AI assistant, I don't have a favorite color, but if I had to pick, it would be green because\"}\n",
      "```\n",
      "\n",
      "This way, Claude's response will build upon the provided content, allowing you to guide the direction and tone of the answer. The assistant will then continue from where the pre-filled content leaves off, creating a more controlled and contextually relevant response.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes how to use the content parameter in the messages list to influence Claude's response. It explains that by including a message with the role of \"assistant\" and specific content, you can guide the direction and tone of Claude's response, which aligns with the correct answer. Both answers convey the same essential information about pre-filling part of the response to shape the output. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [04:10<01:37,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?\n",
      "Correct answer:\n",
      "Compared to fine-tuning, prompt engineering is far more effective at helping models understand and utilize external content like retrieved documents. Prompt engineering also preserves the model's broad general knowledge, while fine-tuning risks catastrophic forgetting where the model loses its general capabilities.\n",
      "Generated anser:\n",
      "Two key advantages of prompt engineering over fine-tuning regarding model comprehension and general knowledge preservation are:\n",
      "\n",
      "1. **Comprehension Improvements**: Prompt engineering is more effective at helping models understand and utilize external content, such as retrieved documents. This allows for better contextual comprehension without altering the underlying model.\n",
      "\n",
      "2. **Preservation of General Knowledge**: Fine-tuning can lead to catastrophic forgetting, where the model loses its broad general knowledge due to the focus on specific tasks. In contrast, prompt engineering maintains the model's broad capabilities, ensuring that general knowledge is preserved while still allowing for task-specific adaptations.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the two key advantages of prompt engineering over fine-tuning as stated in the correct answer. It discusses both the effectiveness of prompt engineering in helping models understand external content and the preservation of general knowledge, while also mentioning the risk of catastrophic forgetting associated with fine-tuning. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [04:15<01:57,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?\n",
      "Correct answer:\n",
      "To get started making requests to Claude models on Anthropic's Bedrock API, you need to: 1) Install and configure the AWS CLI, and 2) Install an SDK for accessing Bedrock, such as the Python SDK shown in the example code.\n",
      "Generated anser:\n",
      "1. Subscribe to Anthropic models by going to the AWS Console, navigating to Bedrock, and requesting access to the Anthropic models. Availability may vary by region.\n",
      "\n",
      "2. Install and configure the AWS CLI or an SDK for accessing Bedrock, and set your API key to make requests to the Claude models.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the need to install and configure the AWS CLI or an SDK for accessing Bedrock, which aligns with the second step in the correct answer. However, it introduces an additional step about subscribing to Anthropic models, which is not mentioned in the correct answer. While this step may be relevant, it is not one of the two main steps outlined in the correct answer. Therefore, the generated answer is not fully aligned with the correct answer as it adds information that is not part of the original two steps.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 70/100 [04:18<01:40,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?\n",
      "Correct answer:\n",
      "You can list the available Claude models in a specific AWS region by running the command `aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`, replacing `<region>` with the desired AWS region such as `us-west-2`.\n",
      "Generated anser:\n",
      "To check which Claude models are available in a specific AWS region using the AWS CLI, you can use the following command:\n",
      "\n",
      "```bash\n",
      "aws bedrock list-foundation-models --region=<your-region> --by-provider=anthropic --query \"modelSummaries[*].modelId\"\n",
      "```\n",
      "\n",
      "Replace `<your-region>` with the desired AWS region (e.g., `us-west-2`). This command will list all the available Claude models provided by Anthropic in that region.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is correct as it provides the same command and instructions as the correct answer. It accurately describes how to check for available Claude models in a specific AWS region using the AWS CLI, including the necessary parameters and the example region. There are no critical pieces of information missing, and there are no contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 70/100 questions. Current Accuracy: 0.6571\n",
      "Number of tokens: 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [04:20<01:29,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?\n",
      "Correct answer:\n",
      "The input_type argument can be passed with a value of \"query\" or \"document\" to specify the type of input text being embedded.\n",
      "Generated anser:\n",
      "The argument that can be passed to the `voyageai.Client.embed()` method or the Voyage HTTP API to specify whether the input text is a query or a document is `input_type`. This argument can take the values \"query\" or \"document\". If not specified, it defaults to None.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the argument `input_type` and states that it can take the values \"query\" or \"document\", which aligns with the correct answer. Additionally, it mentions the default value of None if not specified, which is an extra detail but does not contradict the correct answer. Therefore, the generated answer is correct in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [04:27<01:57,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?\n",
      "Correct answer:\n",
      "Tool_use content block deltas contain partial JSON strings for the input field, whereas text content block deltas directly contain the text delta. Tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time.\n",
      "Generated anser:\n",
      "The streaming API delta formats for tool_use content blocks and text content blocks differ primarily in their structure and the type of data they handle.\n",
      "\n",
      "1. **Tool_use Content Blocks**:\n",
      "   - The deltas for tool_use content blocks are designed to update the input field of the block. They consist of partial JSON strings, allowing for maximum granularity in updates.\n",
      "   - Each delta event includes a `partial_json` field that contains a JSON string representing a portion of the input. The final input is always an object.\n",
      "   - Example format:\n",
      "     ```json\n",
      "     {\n",
      "       \"type\": \"content_block_delta\",\n",
      "       \"index\": 1,\n",
      "       \"delta\": {\n",
      "         \"type\": \"input_json_delta\",\n",
      "         \"partial_json\": \"{\\\"location\\\": \\\"San Fra\\\"}\"\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "   - This format allows for the accumulation of string deltas, which can be parsed into a complete JSON object once a `content_block_stop` event is received.\n",
      "\n",
      "2. **Text Content Blocks**:\n",
      "   - The deltas for text content blocks are simpler and consist of plain text updates. Each delta event contains a `text` field that represents a portion of the text being updated.\n",
      "   - Example format:\n",
      "     ```json\n",
      "     {\n",
      "       \"type\": \"content_block_delta\",\n",
      "       \"index\": 0,\n",
      "       \"delta\": {\n",
      "         \"type\": \"text_delta\",\n",
      "         \"text\": \"ello frien\"\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "   - Unlike tool_use content blocks, text content blocks do not involve JSON parsing and are straightforward text updates.\n",
      "\n",
      "In summary, tool_use content blocks utilize partial JSON strings for more complex structured data updates, while text content blocks use simple text strings for direct text updates.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the differences between tool_use content blocks and text content blocks in terms of their delta formats. It correctly states that tool_use content blocks contain partial JSON strings for the input field, while text content blocks consist of plain text updates. Additionally, it mentions the structure of the deltas and provides examples that align with the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [04:30<01:43,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?\n",
      "Correct answer:\n",
      "When uploading images to Claude, the API has a maximum file size limit of 5MB per image, while on claude.ai the limit is 10MB per image.\n",
      "Generated anser:\n",
      "The image file size limits for uploading images to Claude are as follows:\n",
      "\n",
      "- **API**: Maximum 5MB per image\n",
      "- **claude.ai**: Maximum 10MB per image\n",
      "\n",
      "Images larger than these limits will be rejected and return an error when using the respective interface.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer regarding the image file size limits for both the API and claude.ai. It states the same maximum file sizes and includes the detail about images larger than these limits being rejected, which is consistent with the correct answer. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [04:32<01:26,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?\n",
      "Correct answer:\n",
      "When selecting a Claude model for an enterprise use case that requires low latency, it's important to choose the model that best balances speed and output quality based on the specific requirements of the use case.\n",
      "Generated anser:\n",
      "One key consideration when selecting a Claude model for an enterprise use case that needs low latency is to choose the appropriate model that balances speed and output quality. Selecting a model with lower complexity can help reduce latency, as increased output length and complexity may negatively impact response times.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of the correct answer by emphasizing the importance of balancing speed and output quality when selecting a Claude model for low latency use cases. It also adds the consideration of model complexity affecting latency, which is relevant and does not contradict the correct answer. Therefore, the generated answer is correct as it conveys the same key considerations.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [04:35<01:18,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\n",
      "Correct answer:\n",
      "For code retrieval, Voyage AI recommends using the voyage-code-2 embedding model, which they claim performs 17% better than alternatives and achieves state-of-the-art results on general-purpose corpora as well.\n",
      "Generated anser:\n",
      "Anthropic does not offer its own embedding model but recommends using Voyage AI's embedding models for code retrieval. Specifically, Voyage AI suggests the \"voyage-code-2\" model, which is optimized for code retrieval and reportedly performs 17% better than alternatives. Additionally, it is state-of-the-art on general-purpose corpora.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. It mentions that Voyage AI recommends the \"voyage-code-2\" embedding model for code retrieval, states that it performs 17% better than alternatives, and notes that it achieves state-of-the-art results on general-purpose corpora. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [04:38<01:16,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?\n",
      "Correct answer:\n",
      "The Anthropic Cookbook provides interactive Jupyter notebooks that demonstrate how to upload PDFs and work with embeddings to help developers learn to use Anthropic's APIs.\n",
      "Generated anser:\n",
      "The Anthropic Cookbook can help developers learn to use Anthropic's APIs in two ways:\n",
      "\n",
      "1. **Interactive Jupyter Notebooks**: The Cookbook provides interactive Jupyter notebooks that demonstrate various functionalities, such as uploading PDFs and working with embeddings. This hands-on approach allows developers to experiment and learn through practical examples.\n",
      "\n",
      "2. **Demonstration of Use Cases**: By showcasing specific use cases and examples, the Cookbook helps developers understand how to implement and scale their applications using the Anthropic API and SDKs effectively.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer is correct as it accurately reflects the information provided in the correct answer. Both answers mention the use of interactive Jupyter notebooks for demonstrating functionalities like uploading PDFs and working with embeddings. Additionally, the generated answer introduces the concept of demonstrating use cases, which complements the learning process for developers. There are no critical pieces of information missing, and there are no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [04:42<01:17,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?\n",
      "Correct answer:\n",
      "The size of the context window determines how much retrieved information can be passed to the language model to augment its knowledge when generating a response using RAG. A larger context window allows more relevant retrieved information to be utilized by the model, improving the accuracy and groundedness of the generated text.\n",
      "Generated anser:\n",
      "The size of the context window significantly impacts a language model's ability to utilize retrieval augmented generation (RAG) in several ways. A larger context window allows the model to incorporate more retrieved information from the external knowledge base alongside the original query. This enhances the model's ability to generate responses that are not only relevant but also grounded in the specific evidence retrieved.\n",
      "\n",
      "With a larger context window, the model can maintain coherence over longer prompts and conversations, effectively integrating multiple pieces of information retrieved from the knowledge base. This is particularly important for complex queries that require the model to synthesize information from various sources or to provide detailed and accurate responses.\n",
      "\n",
      "Conversely, a smaller context window may limit the model's ability to effectively utilize the retrieved information, as it can only reference a limited amount of text at a time. This can lead to challenges in maintaining coherence and relevance, especially in tasks that require the integration of extensive or nuanced information.\n",
      "\n",
      "Overall, the effectiveness of RAG is enhanced with a larger context window, as it allows for a more comprehensive understanding of the input and the retrieved data, ultimately improving the quality and accuracy of the generated text.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points made in the correct answer regarding the impact of context window size on retrieval augmented generation (RAG). It explains that a larger context window allows for more retrieved information to be incorporated, enhancing the model's ability to generate relevant and grounded responses. Additionally, it discusses the limitations of a smaller context window, which aligns with the correct answer's emphasis on the importance of context window size for accuracy and groundedness. Therefore, the generated answer is correct and captures the essential substance of the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [04:47<01:28,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?\n",
      "Correct answer:\n",
      "The Evaluation tool helps identify edge cases where prompts might falter, allows rating individual results to determine prompt performance, ensures consistent performance across inputs, and enables prompt refinement for better reliability. Reviewing results across test cases helps spot patterns to make informed adjustments that lead to more robust AI applications.\n",
      "Generated anser:\n",
      "The Evaluation tool in Anthropic's Claude platform can significantly enhance prompts and contribute to building more robust AI applications in several ways:\n",
      "\n",
      "1. **Identifying Edge Cases**: The tool helps users pinpoint specific scenarios where prompts may not perform as expected, allowing for targeted improvements.\n",
      "\n",
      "2. **Rating Results**: By enabling users to evaluate individual outputs, the tool facilitates the identification of instances where prompt performance varies, helping to understand strengths and weaknesses.\n",
      "\n",
      "3. **Ensuring Consistency**: It allows for the assessment of prompt performance across a diverse range of inputs, ensuring that the AI behaves reliably under different conditions.\n",
      "\n",
      "4. **Refining Prompts**: Users can refine their prompts based on insights gained from evaluating results, leading to more reliable and effective interactions with the AI.\n",
      "\n",
      "5. **Spotting Patterns**: By reviewing results across various test cases, users can identify patterns that inform adjustments to prompts, ultimately enhancing their effectiveness.\n",
      "\n",
      "Overall, the Evaluation tool serves as a critical resource for developers looking to optimize their prompts and improve the overall performance of AI applications built on the Claude platform.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately reflects the key points from the Correct Answer regarding the Evaluation tool's capabilities. It discusses identifying edge cases, rating results, ensuring consistency, refining prompts, and spotting patterns, all of which are mentioned in the Correct Answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [04:49<01:10,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?\n",
      "Correct answer:\n",
      "The Claude 3 Haiku model has the fastest comparative latency\n",
      "Generated anser:\n",
      "The Claude model with the fastest comparative latency is the Claude 3 Haiku, which is described as the \"fastest and most compact model for near-instant responsiveness.\"\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies the Claude 3 Haiku model as having the fastest comparative latency, which aligns with the correct answer. Additionally, it provides extra context by describing it as the \"fastest and most compact model for near-instant responsiveness,\" which does not contradict the correct answer. Therefore, the generated answer is essentially saying the same thing as the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [04:56<01:26,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?\n",
      "Correct answer:\n",
      "To have a multi-turn conversation using the Anthropic Messages API in Python, send the full conversation history in the messages parameter each time, including any prior user and assistant messages. The API is stateless, so the entire context must be provided with each request.\n",
      "Generated anser:\n",
      "To build up a conversation with multiple turns using the Anthropic Messages API in Python, you need to maintain the full conversational history and send it with each request. Here's a step-by-step example of how to do this:\n",
      "\n",
      "1. **Initialize the API Client**: Set up the Anthropic client with your API key.\n",
      "2. **Create a List for Messages**: Start with an empty list to hold the conversation messages.\n",
      "3. **Add Messages to the List**: Append user and assistant messages to the list as the conversation progresses.\n",
      "4. **Send the Full History**: Each time you want to get a response, send the entire list of messages to the API.\n",
      "\n",
      "Here’s a sample implementation:\n",
      "\n",
      "```python\n",
      "import anthropic\n",
      "\n",
      "# Initialize the client with your API key\n",
      "client = anthropic.Anthropic(api_key=\"your_api_key\")\n",
      "\n",
      "# Initialize the conversation history\n",
      "messages = []\n",
      "\n",
      "# Add the first user message\n",
      "messages.append({\"role\": \"user\", \"content\": \"Hello, Claude\"})\n",
      "\n",
      "# Send the first message to the API\n",
      "response = client.messages.create(\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      "    max_tokens=1024,\n",
      "    messages=messages\n",
      ")\n",
      "\n",
      "# Print the assistant's response\n",
      "print(\"Claude:\", response['content'])\n",
      "\n",
      "# Add the assistant's response to the conversation history\n",
      "messages.append({\"role\": \"assistant\", \"content\": response['content']})\n",
      "\n",
      "# Add another user message\n",
      "messages.append({\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"})\n",
      "\n",
      "# Send the updated conversation history to the API\n",
      "response = client.messages.create(\n",
      "    model=\"claude-3-5-sonnet-20240620\",\n",
      "    max_tokens=1024,\n",
      "    messages=messages\n",
      ")\n",
      "\n",
      "# Print the assistant's response\n",
      "print(\"Claude:\", response['content'])\n",
      "\n",
      "# Add the assistant's response to the conversation history\n",
      "messages.append({\"role\": \"assistant\", \"content\": response['content']})\n",
      "\n",
      "# Continue the conversation as needed...\n",
      "```\n",
      "\n",
      "In this example, you maintain a list of messages that includes both user and assistant messages. Each time you call the API, you send the entire list, allowing the assistant to respond in the context of the previous messages.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the process of maintaining a conversation with multiple turns using the Anthropic Messages API in Python. It emphasizes the need to keep the full conversation history and send it with each request, which aligns with the correct answer. The step-by-step example provided further clarifies the implementation without omitting any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 80/100 questions. Current Accuracy: 0.7000\n",
      "Number of tokens: 604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [05:00<01:20,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?\n",
      "Correct answer:\n",
      "Providing Claude with a specific role, such as being the General Counsel of a company, using XML tags can help it catch critical legal issues and risks in a contract that it might miss without the role context, potentially saving the company millions of dollars.\n",
      "Generated anser:\n",
      "Using XML tags to provide a specific role or context can significantly enhance Claude's analysis of a legal contract compared to not using a role prompt in several ways:\n",
      "\n",
      "1. **Clarity**: XML tags help clearly separate different components of the prompt, making it easier for Claude to understand the structure and intent of the request. This clarity can lead to more accurate interpretations of the legal contract's elements.\n",
      "\n",
      "2. **Accuracy**: By using XML tags, the risk of misinterpretation is reduced. Claude can more accurately identify which parts of the contract to focus on, leading to a more precise analysis.\n",
      "\n",
      "3. **Flexibility**: XML tags allow for easy modification of the prompt. If specific aspects of the legal contract need to be emphasized or changed, tags can be adjusted without rewriting the entire prompt, facilitating a more tailored analysis.\n",
      "\n",
      "4. **Parseability**: When Claude outputs its analysis using XML tags, it becomes easier to extract and process specific parts of the response. This is particularly useful in legal contexts where certain clauses or terms may need to be highlighted or referenced later.\n",
      "\n",
      "5. **Enhanced Role Context**: By combining XML tags with role prompting, Claude can adopt a specific perspective (e.g., a legal expert) that aligns with the analysis required. This enhances the accuracy and relevance of the insights provided, as Claude can focus on the nuances that a legal professional would prioritize.\n",
      "\n",
      "Overall, using XML tags in conjunction with role prompting creates a structured and context-rich environment that improves the quality and precision of Claude's analysis of legal contracts.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly explains how using XML tags to provide a specific role or context can enhance Claude's analysis of a legal contract. It emphasizes clarity, accuracy, flexibility, parseability, and enhanced role context, which align with the idea of improving the analysis by providing a specific role. While the generated answer does not explicitly mention the potential financial savings for the company, it captures the essence of the correct answer regarding the benefits of role context in legal analysis. Therefore, it can be considered correct overall.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [05:03<01:10,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?\n",
      "Correct answer:\n",
      "When required parameters are missing, Claude 3 Opus is more likely to ask the user for the missing information, while Claude 3 Sonnet is more likely to try to infer reasonable values on its own to proceed with the tool call.\n",
      "Generated anser:\n",
      "Claude 3 Opus and Claude 3 Sonnet handle missing information during tool calls differently based on their design and intended use cases. \n",
      "\n",
      "Claude 3 Opus is optimized for complex tasks and ambiguous queries. It is designed to handle multiple tools effectively and actively seeks clarification when it encounters missing information. This means that when Opus identifies that certain parameters are missing, it is more likely to ask follow-up questions or request additional details to ensure it can make an informed tool call.\n",
      "\n",
      "In contrast, Claude 3 Sonnet, while also intelligent, is more balanced in terms of speed and utility. It may infer missing parameters based on context rather than explicitly seeking clarification. This can lead to quicker responses, but it may not always address the ambiguity as thoroughly as Opus, potentially resulting in less accurate tool calls if the inferred information is incorrect.\n",
      "\n",
      "In summary, Opus prioritizes clarification and handling complexity, while Sonnet leans towards inference and speed, which can affect how each model deals with missing information in tool calls.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key differences between Claude 3 Opus and Claude 3 Sonnet in handling missing information during tool calls. It states that Opus is more likely to ask for clarification when parameters are missing, while Sonnet tends to infer values based on context. This aligns with the correct answer, which emphasizes Opus's focus on seeking additional information and Sonnet's inclination to make inferences. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [05:07<01:09,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?\n",
      "Correct answer:\n",
      "To ensure a reliable production deployment of Claude for ticket routing, key steps include implementing retry logic to handle errors, conducting thorough staging and load testing, setting up error handling and logging, using a gradual rollout process, providing documentation and training, and establishing monitoring and alerting.\n",
      "Generated anser:\n",
      "To ensure a reliable deployment of an automated ticket routing system using Claude into a production environment, follow these steps:\n",
      "\n",
      "1. **Implement Retry Logic**: Add try/except logic to handle cases where Claude may not return the expected output or is temporarily unavailable. Incorporate back-off logic to retry after increasing intervals or adjust the temperature for output variations.\n",
      "\n",
      "2. **Conduct Thorough Staging Testing**: Test the system extensively in a staging environment that closely resembles your production setup. This helps identify potential issues or incompatibilities before the actual deployment.\n",
      "\n",
      "3. **Perform Load Testing**: Verify that the system can handle the anticipated volume of tickets without performance degradation. This ensures responsiveness and efficiency under real-world conditions.\n",
      "\n",
      "4. **Establish Error Handling and Logging**: Implement comprehensive error handling and logging mechanisms to facilitate debugging and monitoring in production. This allows for quick identification and resolution of any issues.\n",
      "\n",
      "5. **Plan a Gradual Rollout**: Start with a small percentage of traffic and gradually increase it while closely monitoring the system’s behavior. This minimizes risk and allows for a controlled deployment.\n",
      "\n",
      "6. **Prepare Documentation and Training**: Create detailed documentation and provide training for relevant stakeholders on how to use and maintain the new system effectively. This promotes a smooth transition and user adoption.\n",
      "\n",
      "7. **Set Up Monitoring and Alerting**: Implement robust monitoring and alerting mechanisms to proactively detect and address any issues that may arise in production. This enables quick responses and minimizes downtime.\n",
      "\n",
      "By following these steps, you can ensure a successful and reliable deployment of your automated ticket routing system using Claude.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer includes all the key steps outlined in the Correct Answer, such as implementing retry logic, conducting thorough staging and load testing, establishing error handling and logging, planning a gradual rollout, preparing documentation and training, and setting up monitoring and alerting. The details provided in the Generated Answer expand on the points made in the Correct Answer but do not omit any critical information. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [05:12<01:09,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How should you evaluate a model's performance on a ticket routing classifier?\n",
      "Correct answer:\n",
      "You should evaluate performance in terms of accuracy, cost, and speed.\n",
      "Generated anser:\n",
      "To evaluate a model's performance on a ticket routing classifier, you should consider the following factors:\n",
      "\n",
      "1. **Accuracy**: Measure how often the model correctly classifies tickets into the appropriate categories. This can be done using metrics such as precision, recall, F1-score, and overall accuracy. A confusion matrix can also help visualize the performance across different classes.\n",
      "\n",
      "2. **Cost**: Analyze the cost-effectiveness of the model in terms of computational resources and operational expenses. This includes evaluating the cost per classification and any associated infrastructure costs.\n",
      "\n",
      "3. **Speed**: Assess the response time of the model in classifying tickets. This is crucial for ensuring that the ticket routing process is efficient and does not introduce delays in handling customer inquiries.\n",
      "\n",
      "Additionally, it's important to understand the existing ticketing system and how tickets are currently routed. Investigate the criteria used for routing, existing automated rules, and how edge cases are managed. This understanding will help you better evaluate the model's performance in the context of real-world scenarios.\n",
      "\n",
      "Choosing the right model is also essential. For instance, the claude-3-haiku-20240307 model is noted for its excellent results, speed, and cost-effectiveness, making it a suitable choice for many ticket routing tasks. However, if the classification requires deep expertise or complex reasoning, a more advanced model like Sonnet may be necessary despite its higher cost. \n",
      "\n",
      "Overall, a comprehensive evaluation should incorporate these aspects to ensure the model is ready for deployment and meets the needs of both technical and business stakeholders.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies accuracy, cost, and speed as key factors for evaluating a model's performance on a ticket routing classifier, which aligns with the correct answer. It also elaborates on each factor, providing additional context and considerations that enhance the understanding of the evaluation process. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [05:14<00:55,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?\n",
      "Correct answer:\n",
      "Anthropic recommends trying their interactive GitHub prompting tutorial and Google Sheets prompting tutorial to learn prompt engineering concepts before diving into the techniques in the documentation.\n",
      "Generated anser:\n",
      "Anthropic recommends two methods for learning how to prompt engineer with Claude before diving into the techniques: \n",
      "\n",
      "1. Utilizing the in-depth prompt engineering interactive tutorial that employs Claude for Sheets.\n",
      "2. Exploring the Prompt Library for example prompts to understand effective prompting strategies.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer mentions utilizing an interactive tutorial for Claude for Sheets and exploring a Prompt Library, but it does not mention the specific interactive GitHub prompting tutorial or the Google Sheets prompting tutorial as recommended by Anthropic. This omission of the specific tutorials makes the generated answer incomplete and therefore incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [05:18<00:51,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?\n",
      "Correct answer:\n",
      "Pretrained large language models are trained on unlabeled text data to predict the next word given the previous context, but are not inherently good at answering questions or following instructions without prompt engineering. In contrast, Claude is a large language model that has been further fine-tuned and trained using RLHF to be more helpful, honest, and capable of performing a wider range of useful tasks.\n",
      "Generated anser:\n",
      "The key differences between a pretrained large language model and Claude lie in their training processes and capabilities. \n",
      "\n",
      "1. **Training Process**: \n",
      "   - Pretrained large language models undergo an initial phase called pretraining, where they are trained on a vast corpus of unlabeled text to predict the next word in a sequence. This phase focuses on learning language patterns but does not specifically optimize for tasks like answering questions or following instructions.\n",
      "   - Claude, on the other hand, is based on a pretrained autoregressive language model but has undergone additional fine-tuning using Reinforcement Learning from Human Feedback (RLHF). This process refines the model to be more helpful, honest, and harmless, enhancing its ability to perform specific tasks effectively.\n",
      "\n",
      "2. **Capabilities**:\n",
      "   - Pretrained large language models may require significant prompt engineering to elicit desired behaviors and are not inherently optimized for conversational tasks.\n",
      "   - Claude is specifically designed as a conversational assistant, making it more adept at generating human-like responses, answering questions, and summarizing information due to its fine-tuning and training with RLHF.\n",
      "\n",
      "In summary, while both types of models start with a similar pretraining phase, Claude's additional fine-tuning and focus on conversational capabilities set it apart from standard pretrained large language models.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key differences between pretrained large language models and Claude, including the training processes and capabilities. It explains that pretrained models focus on predicting the next word without optimization for specific tasks, while Claude has been fine-tuned with RLHF to enhance its helpfulness and conversational abilities. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [05:22<00:49,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?\n",
      "Correct answer:\n",
      "Prompt engineering is typically faster, more cost-effective, requires less data and compute resources, and preserves the model's general knowledge compared to fine-tuning. It also allows for greater flexibility, rapid iteration, and transparency.\n",
      "Generated anser:\n",
      "Key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain include:\n",
      "\n",
      "1. **Resource Efficiency**: Prompt engineering requires only text input, making it much more resource-friendly compared to fine-tuning, which necessitates high-end GPUs and large memory.\n",
      "\n",
      "2. **Cost-Effectiveness**: Fine-tuning can incur significant costs, especially with cloud-based AI services. In contrast, prompt engineering typically utilizes the base model, which is cheaper.\n",
      "\n",
      "3. **Maintaining Model Updates**: Fine-tuned models may require retraining when providers update their models, while prompts usually remain effective across different model versions without changes.\n",
      "\n",
      "4. **Time-Saving**: Fine-tuning can take hours or even days, whereas prompt engineering allows for nearly instantaneous results, facilitating quick problem-solving.\n",
      "\n",
      "5. **Minimal Data Needs**: Fine-tuning often requires substantial task-specific labeled data, which can be scarce or expensive. Prompt engineering can effectively operate with few-shot or even zero-shot learning.\n",
      "\n",
      "6. **Flexibility and Rapid Iteration**: Prompt engineering enables quick experimentation with various approaches and immediate results, which is more challenging with fine-tuning.\n",
      "\n",
      "7. **Domain Adaptation**: It is easier to adapt models to new domains by providing domain-specific context in prompts without the need for retraining.\n",
      "\n",
      "8. **Comprehension Improvements**: Prompt engineering is more effective at helping models understand and utilize external content, such as retrieved documents.\n",
      "\n",
      "9. **Preservation of General Knowledge**: Fine-tuning risks catastrophic forgetting, where the model may lose its general knowledge. Prompt engineering helps maintain the model’s broad capabilities.\n",
      "\n",
      "10. **Transparency**: Prompts are human-readable, making it clear what information the model receives, which aids in understanding and debugging the model's behavior.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately captures the key advantages of prompt engineering over fine-tuning as outlined in the correct answer. It discusses resource efficiency, cost-effectiveness, the need for less data, time-saving aspects, flexibility, and the preservation of general knowledge. Additionally, it mentions the ease of domain adaptation and the transparency of prompts. All critical points from the correct answer are present in the generated answer, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [05:25<00:41,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?\n",
      "Correct answer:\n",
      "Before running requests to access Claude models on Vertex AI, you may need to run `gcloud auth application-default login` to authenticate with GCP.\n",
      "Generated anser:\n",
      "To authenticate with Google Cloud Platform (GCP) before running requests to access Claude models on Vertex AI, you need to run the following command in your terminal:\n",
      "\n",
      "```\n",
      "gcloud auth application-default login\n",
      "```\n",
      "\n",
      "This command will allow you to authenticate your application with GCP, enabling you to make requests to access the Claude models. After authenticating, you can proceed to use the client SDKs to interact with the models as shown in the provided examples.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states the command `gcloud auth application-default login` needed to authenticate with GCP before accessing Claude models on Vertex AI. It also explains that this command allows authentication for making requests, which aligns with the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [05:27<00:34,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\n",
      "Correct answer:\n",
      "According to the information provided, on May 10th, 2024, Anthropic introduced a new \"Prompt Generator\" tool in the Developer Console. This tool is designed to help users guide Claude to generate high-quality prompts tailored to their specific tasks. The text states that the Prompt Generator \"makes it easy to guide Claude to generate a high-quality prompts tailored to your specific tasks.\" This indicates that the Prompt Generator feature provides users with the ability to create customized prompts for Claude, going beyond the standard prompting capabilities. By combining this information with the details about the Claude iOS app and the Claude Team plan released around the same time, we can infer that Anthropic was expanding its platform and tools to provide users with more advanced capabilities for interacting with and leveraging the Claude AI assistant for their specific needs and use cases.\n",
      "Generated anser:\n",
      "On May 10th, 2024, Anthropic introduced a new prompt generator tool in the Developer Console. This tool is designed to assist users in creating high-quality prompts that are tailored to specific tasks. By using the prompt generator, users can guide Claude more effectively, ensuring that the generated outputs align closely with their requirements. This capability enhances the customization of interactions with the AI, allowing for more precise and relevant responses based on the user's needs.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the key points from the correct answer regarding the introduction of the prompt generator tool and its purpose in assisting users to create tailored prompts for specific tasks. Both answers emphasize the tool's role in enhancing user interaction with Claude and improving the quality of generated outputs. There are no critical pieces of information missing, and there are no contradictions between the two answers. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [05:29<00:28,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?\n",
      "Correct answer:\n",
      "Both Claude 3.5 Sonnet and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "Generated anser:\n",
      "Both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer states that both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai became available on June 20th, 2024, which matches the correct answer exactly. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 90/100 questions. Current Accuracy: 0.7222\n",
      "Number of tokens: 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [05:31<00:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\n",
      "Correct answer:\n",
      "You can use \"max_tokens\": 1 in the request to limit Claude's response to a single token when putting words in its mouth.\n",
      "Generated anser:\n",
      "To limit Claude's response to a single token, you can use the header `\"max_tokens\": 1` in your request.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that you can use the header \"max_tokens\": 1 to limit Claude's response to a single token, which is consistent with the correct answer. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [05:34<00:19,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What does the temperature parameter do when working with large language models?\n",
      "Correct answer:\n",
      "Temperature is a parameter that controls the randomness of the model during generation\n",
      "Generated anser:\n",
      "The temperature parameter controls the randomness of a language model's predictions during text generation. A higher temperature results in more creative and diverse outputs, allowing for variations in phrasing and responses. Conversely, a lower temperature leads to more conservative and deterministic outputs, favoring the most probable choices. By adjusting the temperature, users can encourage the model to explore less common or surprising word choices and sequences, rather than sticking solely to the most likely predictions.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately describes the function of the temperature parameter in controlling the randomness of a language model's predictions during text generation. It elaborates on how higher temperatures lead to more creative outputs and lower temperatures result in more deterministic outputs, which aligns with the essence of the correct answer. Therefore, the generated answer is correct as it conveys the same fundamental information without any critical omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [05:37<00:18,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two ways to specify API parameters when calling the Claude API using Claude for Sheets?\n",
      "Correct answer:\n",
      "When calling the Claude API using Claude for Sheets, you can specify API parameters in two ways: 1) As additional arguments after the prompt and model in the CLAUDE() function, like =CLAUDE(prompt, model, \"max_tokens\", 3). 2) By passing in an API key to be used just for a specific cell, like \"api_key\", \"sk-ant-api03-j1W...\"\n",
      "Generated anser:\n",
      "Two ways to specify API parameters when calling the Claude API using Claude for Sheets are:\n",
      "\n",
      "1. **Simple Prompt**: You can call the API with just a prompt by using the formula `=CLAUDE(\"Your prompt here\")`. For example, `=CLAUDE(\"Claude, in one sentence, what's good about the color blue?\")`.\n",
      "\n",
      "2. **Adding Parameters**: You can specify additional parameters by using the formula `=CLAUDE(prompt, model, params...)`. For example, `=CLAUDE(\"Hi, Claude!\", \"claude-3-haiku-20240307\", \"max_tokens\", 3)`, where you can set various API parameters including the model and other options.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides two ways to specify API parameters when calling the Claude API using Claude for Sheets. The first method described is a simple prompt, which is not relevant to the question about specifying API parameters. The second method mentions adding parameters, which aligns with the correct answer but lacks the specific mention of passing an API key for a specific cell. Therefore, the generated answer is missing a critical piece of information regarding the API key, making it incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [05:40<00:17,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How does prefilling the response with an opening curly brace ({ ) affect Claude's output when extracting structured data from text?\n",
      "Correct answer:\n",
      "Prefilling Claude's response with { causes it to skip the preamble explanation and directly output the extracted data as a JSON object, resulting in a more concise response that is easier for programs to parse without additional processing.\n",
      "Generated anser:\n",
      "Prefilling the response with an opening curly brace ({) can influence Claude's output by signaling that the expected response should be structured data, such as a JSON object. This can guide Claude to format its output accordingly, making it more likely to produce a structured response that adheres to the expected syntax. For instance, if the prefill indicates the start of a JSON object, Claude may continue to generate content that fits within that structure, ensuring that the output is valid and correctly formatted for structured data extraction. This technique can be particularly useful when the user requires specific data formats or structured responses from Claude.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer captures the essence of how prefilling with an opening curly brace influences Claude's output by indicating that a structured response is expected. It mentions that this can guide Claude to format its output as a JSON object, which aligns with the correct answer's emphasis on skipping the preamble and producing a concise response. However, the generated answer does not explicitly mention the benefit of producing a more concise response that is easier for programs to parse, which is a critical aspect of the correct answer. Therefore, while the generated answer is largely correct, it is missing a key piece of information regarding the conciseness and ease of parsing. Thus, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [05:45<00:16,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are some helpful resources provided by Anthropic to dive deeper into building with images using Claude?\n",
      "Correct answer:\n",
      "Anthropic provides a multimodal cookbook with tips on getting started with images and best practices, as well as API reference documentation for the Messages API that includes example API calls involving images.\n",
      "Generated anser:\n",
      "To dive deeper into building with images using Claude, you can utilize the following resources provided by Anthropic:\n",
      "\n",
      "1. **How to Use Vision**: This section explains how to leverage Claude's vision capabilities. You can upload images via the Claude website (claude.ai) by dragging and dropping images into the chat window or using the Console Workbench. For models that accept images (Claude 3 models only), a button to add images will appear in the chat interface.\n",
      "\n",
      "2. **API Reference**: This resource allows you to explore, implement, and scale your applications using the Anthropic API and SDKs, which includes handling image inputs.\n",
      "\n",
      "3. **Anthropic Cookbook**: This includes interactive Jupyter notebooks that provide working code examples, including demonstrations of uploading images and other functionalities.\n",
      "\n",
      "4. **Quickstart Guide**: Follow the Quickstart to make your first API call, which can include image processing if you are using the appropriate model.\n",
      "\n",
      "5. **Prompt Library**: Explore example prompts that can help you understand how to effectively use Claude with images.\n",
      "\n",
      "These resources will help you get started and deepen your understanding of building applications with image capabilities in Claude.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies several resources provided by Anthropic for building with images using Claude, including the multimodal cookbook, API reference, and quickstart guide. It also mentions how to use vision capabilities and provides practical steps for image uploads. While the generated answer includes additional details and resources, it does not omit any critical information present in the correct answer. Therefore, it is accurate and aligns well with the correct answer's substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [05:47<00:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How do you specify the API key when creating a new Anthropic client in the Python and TypeScript SDK examples?\n",
      "Correct answer:\n",
      "In both the Python and TypeScript examples, you can specify the API key as a string parameter when creating a new Anthropic client object. If no API key is provided, it defaults to using the ANTHROPIC_API_KEY environment variable.\n",
      "Generated anser:\n",
      "To specify the API key when creating a new Anthropic client in the Python and TypeScript SDKs, you can either set the API key in an environmental variable or supply it directly during client initialization.\n",
      "\n",
      "For Python, you can set the API key in the environment variable like this:\n",
      "\n",
      "```bash\n",
      "export ANTHROPIC_API_KEY='your-api-key-here'\n",
      "```\n",
      "\n",
      "Alternatively, you can pass the API key directly when initializing the client:\n",
      "\n",
      "```python\n",
      "from anthropic import Anthropic\n",
      "\n",
      "client = Anthropic(api_key='your-api-key-here')\n",
      "```\n",
      "\n",
      "For TypeScript, you can also set the API key in the environment variable:\n",
      "\n",
      "```bash\n",
      "export ANTHROPIC_API_KEY='your-api-key-here'\n",
      "```\n",
      "\n",
      "Or you can provide it directly when creating the client:\n",
      "\n",
      "```typescript\n",
      "import { Anthropic } from 'anthropic-sdk';\n",
      "\n",
      "const client = new Anthropic({ apiKey: 'your-api-key-here' });\n",
      "```\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that the API key can be specified either through an environment variable or directly during client initialization, which aligns with the correct answer. It provides examples for both Python and TypeScript, demonstrating how to set the API key in both ways. Therefore, it captures the essential information and is consistent with the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [05:50<00:09,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application?\n",
      "Correct answer:\n",
      "The Evaluation tool helps identify edge cases where the prompt might falter, and ensures consistent performance across a range of test case inputs. This allows you to refine the prompt for better reliability in the AI classification application.\n",
      "Generated anser:\n",
      "Two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application are:\n",
      "\n",
      "1. **Identification of Edge Cases**: The Evaluation tool helps developers identify edge cases where the prompt might not perform as expected. This allows for targeted improvements to the prompt, ensuring that it can handle a wider variety of inputs effectively.\n",
      "\n",
      "2. **Performance Consistency and Reliability**: The tool enables developers to rate individual results and assess the performance of prompts across a range of inputs. This ensures consistent performance and helps refine prompts for better reliability, ultimately leading to a more robust AI application.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer accurately captures the two key benefits of using the Anthropic Evaluation tool as stated in the Correct Answer. It mentions the identification of edge cases and the importance of performance consistency and reliability, both of which align with the points made in the Correct Answer. Therefore, the Generated Answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [05:55<00:07,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are the key differences between a pretrained language model like Claude's underlying model, and the final version of Claude available through Anthropic's API?\n",
      "Correct answer:\n",
      "The pretrained language model that forms Claude's foundation is not inherently good at answering questions or following instructions. To create the helpful, honest and safe Claude assistant available through the API, the pretrained model underwent fine-tuning and reinforcement learning from human feedback (RLHF).\n",
      "Generated anser:\n",
      "The key differences between a pretrained language model like Claude's underlying model and the final version of Claude available through Anthropic's API include:\n",
      "\n",
      "1. **Training and Fine-tuning**: The pretrained model serves as a foundational architecture that has been trained on a large corpus of text. The final version, such as Claude 3.5 Sonnet, undergoes additional fine-tuning and optimization to enhance its performance on specific tasks, improve accuracy, and ensure better alignment with user expectations.\n",
      "\n",
      "2. **Model Versioning**: The pretrained model may refer to an earlier or generic version of the architecture, while the final version available through the API (e.g., \"claude-3-5-sonnet-20240620\") is a specific iteration that includes updates, improvements, and possibly new features that have been implemented after the initial training phase.\n",
      "\n",
      "3. **API Integration**: The final version is designed for integration with the Anthropic API, allowing users to interact with it through defined endpoints, manage input and output tokens, and utilize specific parameters (like max_tokens) to control the model's responses.\n",
      "\n",
      "4. **Performance Enhancements**: The final version typically includes optimizations that enhance its ability to generate coherent, contextually relevant, and accurate responses, which may not be present in the pretrained model.\n",
      "\n",
      "5. **Availability and Access**: The pretrained model may not be directly accessible for public use, whereas the final version is made available through specific platforms like the Anthropic API, Amazon Bedrock, and Google Vertex AI, ensuring broader accessibility and usability for developers and businesses.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer provides a detailed explanation of the differences between the pretrained language model and the final version of Claude available through the API. However, it lacks the critical information about the fine-tuning and reinforcement learning from human feedback (RLHF) that is essential for transforming the pretrained model into the helpful assistant Claude. This aspect is a key part of the correct answer and is missing from the generated answer. Therefore, the generated answer is not fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [05:57<00:03,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is the IPv6 address range used by Anthropic?\n",
      "Correct answer:\n",
      "The IPv6 address range used by Anthropic is 2607:6bc0::/48.\n",
      "Generated anser:\n",
      "The IPv6 address range used by Anthropic is 2607:6bc0::/48.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer matches the correct answer exactly, providing the same IPv6 address range used by Anthropic. There are no missing pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [05:59<00:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "When using the Python SDK to create a message with Claude, what are two ways you can specify your API key?\n",
      "Correct answer:\n",
      "When using the Python SDK, you can specify your API key either by passing it as the api_key parameter when initializing the Anthropic client, or by setting it as an environment variable named ANTHROPIC_API_KEY which the client will use by default.\n",
      "Generated anser:\n",
      "You can specify your API key in two ways when using the Python SDK to create a message with Claude:\n",
      "\n",
      "1. Set the API key as an environment variable named `ANTHROPIC_API_KEY`. This allows you to omit the API key from your code.\n",
      "\n",
      "2. Supply the API key directly when initializing the Anthropic client, like so: `client = anthropic.Anthropic(api_key='your-api-key-here')`.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies both ways to specify the API key when using the Python SDK: setting it as an environment variable and supplying it directly when initializing the client. It captures the essence of the correct answer without omitting any critical information. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Processed 100/100 questions. Current Accuracy: 0.7200\n",
      "Detailed results saved to evaluation/csvs/evaluation_results_detailed.csv\n",
      "Average Precision: 0.3933\n",
      "Average Recall: 0.6183\n",
      "Average MRR: 0.7333\n",
      "Average F1: 0.4808\n",
      "End-to-End Accuracy: 0.7200\n",
      "Evaluation complete. Results saved to evaluation/json_results/evaluation_results_one.json, evaluation/csvs/evaluation_results_detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs = evaluate_retrieval(retrieve_similar, eval_data, db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context, db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_detailed.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_one.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(json_dir / result_file_name, 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Basic RAG\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Basic RAG\",\n",
      "  \"average_precision\": 0.3933333333333335,\n",
      "  \"average_recall\": 0.6183333333333334,\n",
      "  \"average_f1\": 0.48081274025260856,\n",
      "  \"average_mrr\": 0.7333333333333334,\n",
      "  \"end_to_end_accuracy\": 0.78\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/json_results/evaluation_results_one.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,retrieval_precision,retrieval_recall,retrieval_mrr,e2e_correct\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?,0.3333333333333333,0.5,1.0,True\n",
      "\"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",0.6666666666666666,1.0,1.0,True\n",
      "\"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",0.6666666666666666,1.0,1.0,True\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?,0.3333333333333333,0.5,1.0,False\n",
      "\"What happens if a prompt for the Text Completions API is missing the \"\"\\n\\nHuman:\"\" and \"\"\\n\\nAssistant:\"\" turns?\",0.6666666666666666,1.0,1.0,True\n",
      "How do the additional tokens required for tool use in Claude API requests impact pricing compared to regular API requests?,0.3333333333333333,0.5,1.0,True\n",
      "\"When will the new Anthropic Developer Console features that show API usage, billing details, and rate limits be available?\",0.3333333333333333,1.0,1.0,True\n",
      "\"When deciding whether to use chain-of-thought (CoT) for a task, what are two key factors to consider in order to strike the right balance between performance and latency?\",0.3333333333333333,0.5,1.0,True\n",
      "How can I use Claude to more easily digest the content of long PDF documents?,0.0,0.0,0.0,True\n",
      "\"According to the documentation, where can you view your organization's current API rate limits in the Anthropic Console?\",0.6666666666666666,1.0,1.0,True\n",
      "How can we measure the performance of the ticket classification system implemented using Claude beyond just accuracy?,0.0,0.0,0.0,False\n",
      "How can you specify a system prompt using the Text Completions API versus the Messages API?,0.3333333333333333,0.5,1.0,True\n",
      "How can you combine XML tags with chain of thought reasoning to create high-performance prompts for Claude?,0.0,0.0,0.0,False\n",
      "\"When evaluating the Claude model's performance for ticket routing, what three key metrics are calculated and what are the results for the claude-3-haiku-20240307 model on the 91 test samples?\",0.0,0.0,0.0,False\n",
      "\"Before starting to engineer and improve a prompt in Claude, what key things does Anthropic recommend you have in place first?\",0.3333333333333333,0.5,1.0,True\n",
      "How does the Messages API handle mid-response prompting compared to the Text Completions API?,0.6666666666666666,1.0,1.0,True\n",
      "How does Claude's response differ when given a role through a system prompt compared to not having a specific role in the financial analysis example?,0.0,0.0,0.0,True\n",
      "\"What are some quantitative metrics that can be used to measure the success of a sentiment analysis model, and how might specific targets for those metrics be determined?\",0.3333333333333333,1.0,0.5,True\n",
      "What is a power user tip mentioned in the documentation for creating high-performance prompts using XML tags?,0.3333333333333333,0.5,1.0,True\n",
      "How can you use an LLM like Claude to automatically grade the outputs of other LLMs based on a rubric?,0.3333333333333333,0.5,0.5,True\n",
      "How can you access and deploy Voyage embeddings on AWS Marketplace?,0.3333333333333333,1.0,1.0,True\n",
      "\"When using tools just to get Claude to produce JSON output following a particular schema, what key things should you do in terms of tool setup and prompting?\",0.3333333333333333,0.5,1.0,False\n",
      "What are the key differences between the legacy Claude Instant 1.2 model and the Claude 3 Haiku model in terms of capabilities and performance?,1.0,1.0,1.0,True\n",
      "What is one key benefit of using examples when prompt engineering with Claude?,0.3333333333333333,1.0,1.0,True\n",
      "\"According to the Anthropic documentation, what is one key advantage of using prompt engineering instead of fine-tuning when it comes to adapting an AI model to new domains or tasks?\",0.3333333333333333,0.5,1.0,False\n",
      "How can I quickly get started using the Claude for Sheets extension with a pre-made template?,0.6666666666666666,1.0,1.0,True\n",
      "\"How does the \"\"index\"\" field in the \"\"content_block_delta\"\" event relate to the text being streamed in a response?\",0.3333333333333333,0.5,1.0,True\n",
      "\"How can you include an image as part of a Claude API request, and what image formats are currently supported?\",0.0,0.0,0.0,False\n",
      "What is the relationship between time to first token (TTFT) and latency when evaluating a language model's performance?,1.0,1.0,1.0,True\n",
      "How can providing Claude with examples of handling certain edge cases like implicit requests or emotional prioritization help improve its performance in routing support tickets?,0.3333333333333333,0.5,1.0,True\n",
      "\"How does the stop_reason of \"\"tool_use\"\" relate to the overall workflow of integrating external tools with Claude?\",0.3333333333333333,0.5,1.0,True\n",
      "\"According to the documentation, what error event and corresponding HTTP error code may be sent during periods of high usage for the Anthropic API when using streaming responses?\",1.0,1.0,1.0,True\n",
      "What are the two types of deltas that can be contained in a content_block_delta event when streaming responses from the Anthropic API?,0.3333333333333333,0.5,1.0,False\n",
      "\"On what date did Claude 3.5 Sonnet and tool use both become generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI?\",0.6666666666666666,1.0,1.0,False\n",
      "In what order did Anthropic launch Claude.ai and the Claude iOS app in Canada and Europe?,0.6666666666666666,1.0,1.0,True\n",
      "\"When the API response from Claude has a stop_reason of \"\"tool_use\"\", what does this indicate and what should be done next to continue the conversation?\",0.3333333333333333,0.5,1.0,True\n",
      "What Python libraries are used in the example code snippet for evaluating tone and style in a customer service chatbot?,0.0,0.0,0.0,True\n",
      "What are the two main ways to authenticate when using the Anthropic Python SDK to access Claude models on Amazon Bedrock?,0.6666666666666666,1.0,1.0,True\n",
      "\"When deciding whether to implement leak-resistant prompt engineering strategies, what two factors should be considered and balanced?\",0.6666666666666666,1.0,1.0,True\n",
      "How can selecting the appropriate Claude model based on your specific requirements help reduce latency in your application?,0.3333333333333333,0.5,1.0,True\n",
      "How can you stream responses from the Anthropic API using the Python SDK?,0.6666666666666666,1.0,1.0,True\n",
      "\"How can you guide Claude's response by pre-filling part of the response, and what API parameter is used to generate a short response in this case?\",0.0,0.0,0.0,False\n",
      "\"What is more important when building an eval set for an AI system - having a larger number of test cases with automated grading, or having fewer high-quality test cases graded by humans?\",0.3333333333333333,0.5,1.0,True\n",
      "What are the two required fields in a content_block_delta event for a text delta type?,0.6666666666666666,1.0,1.0,True\n",
      "\"What are two interactive ways to learn how to use Claude's capabilities, such as uploading PDFs and generating embeddings?\",0.0,0.0,0.0,False\n",
      "Why does breaking a task into distinct subtasks for chained prompts help improve Claude's accuracy on the overall task?,0.6666666666666666,1.0,1.0,True\n",
      "How does the streaming format for Messages responses differ from Text Completions streaming responses?,0.3333333333333333,1.0,1.0,True\n",
      "\"What are two ways to start experimenting with Claude as a user, according to Anthropic's documentation?\",0.0,0.0,0.0,False\n",
      "How can using chain prompts help reduce errors and inconsistency in complex tasks handled by Claude?,0.6666666666666666,1.0,1.0,True\n",
      "What HTTP status code does an overloaded_error event correspond to in a non-streaming context for the Anthropic API?,0.6666666666666666,1.0,1.0,True\n",
      "What are the two ways to specify the format in which Voyage AI returns embeddings through its HTTP API?,0.3333333333333333,1.0,1.0,True\n",
      "\"When streaming API requests that use tools, how are the input JSON deltas for tool_use content blocks sent, and how can they be accumulated and parsed by the client?\",0.6666666666666666,1.0,1.0,True\n",
      "\"What are the two interactive prompt engineering tutorials that Anthropic offers, and how do they differ?\",0.3333333333333333,0.5,1.0,False\n",
      "What are some of the key capabilities that make Claude suitable for enterprise use cases requiring integration with specialized applications and processing of large volumes of sensitive data?,0.3333333333333333,1.0,1.0,True\n",
      "\"As of June 2024, in which regions are Anthropic's Claude.ai API and iOS app available?\",0.6666666666666666,0.6666666666666666,1.0,False\n",
      "\"What are the two main approaches for integrating Claude into a support ticket workflow, and how do they differ in terms of scalability and ease of implementation?\",0.6666666666666666,1.0,1.0,True\n",
      "\"When did Anthropic release a prompt generator tool to help guide Claude in generating high-quality prompts, and through what interface is it available?\",0.0,0.0,0.0,False\n",
      "Which Claude 3 model provides the best balance of intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing?,0.0,0.0,0.0,True\n",
      "\"How can you calculate the similarity between two Voyage embedding vectors, and what is this equivalent to since Voyage embeddings are normalized to length 1?\",0.6666666666666666,1.0,1.0,True\n",
      "How can using examples in prompts improve Claude's performance on complex tasks?,0.3333333333333333,0.5,1.0,True\n",
      "\"What are the two types of content block deltas that can be emitted when streaming responses with tool use, and what does each delta type contain?\",0.6666666666666666,0.5,1.0,False\n",
      "What are two key capabilities of Claude that enable it to build interactive systems and personalized user experiences?,0.0,0.0,0.0,False\n",
      "\"What are the key event types included in a raw HTTP stream response when using message streaming, and what is the typical order they occur in?\",0.6666666666666666,1.0,1.0,True\n",
      "What is the maximum number of images that can be included in a single request using the Anthropic API compared to the claude.ai interface?,0.3333333333333333,0.5,1.0,True\n",
      "\"When Claude's response is cut off due to hitting the max_tokens limit and contains an incomplete tool use block, what should you do to get the full tool use?\",0.3333333333333333,1.0,0.5,True\n",
      "What two steps are needed before running a classification evaluation on Claude according to the documentation?,0.0,0.0,0.0,False\n",
      "How can you use the content parameter in the messages list to influence Claude's response?,0.0,0.0,0.0,True\n",
      "What are two key advantages of prompt engineering over fine-tuning when it comes to model comprehension and general knowledge preservation?,0.3333333333333333,0.5,1.0,True\n",
      "What are the two main steps to get started with making requests to Claude models on Anthropic's Bedrock API?,0.3333333333333333,0.5,0.3333333333333333,False\n",
      "How can you check which Claude models are available in a specific AWS region using the AWS CLI?,0.6666666666666666,1.0,1.0,True\n",
      "What argument can be passed to the voyageai.Client.embed() method or the Voyage HTTP API to specify whether the input text is a query or a document?,0.6666666666666666,1.0,1.0,True\n",
      "How do the streaming API delta formats differ between tool_use content blocks and text content blocks?,0.6666666666666666,1.0,1.0,True\n",
      "What are the image file size limits when uploading images to Claude using the API versus on claude.ai?,0.3333333333333333,1.0,1.0,True\n",
      "What is one key consideration when selecting a Claude model for an enterprise use case that needs low latency?,0.6666666666666666,1.0,1.0,True\n",
      "\"What embedding model does Anthropic recommend for code retrieval, and how does its performance compare to alternatives according to Voyage AI?\",0.6666666666666666,1.0,1.0,True\n",
      "What are two ways the Anthropic Cookbook can help developers learn to use Anthropic's APIs?,0.6666666666666666,1.0,0.5,True\n",
      "How does the size of the context window impact a language model's ability to utilize retrieval augmented generation (RAG)?,0.6666666666666666,1.0,1.0,True\n",
      "How can the Evaluation tool in Anthropic's Claude platform help improve prompts and build more robust AI applications?,0.3333333333333333,0.5,1.0,True\n",
      "Which Claude model has the fastest comparative latency according to the comparison tables?,0.6666666666666666,1.0,1.0,True\n",
      "How can you build up a conversation with multiple turns using the Anthropic Messages API in Python?,0.6666666666666666,1.0,1.0,True\n",
      "How can using XML tags to provide a specific role or context help improve Claude's analysis of a legal contract compared to not using a role prompt?,0.0,0.0,0.0,True\n",
      "What are the key differences between how Claude 3 Opus and Claude 3 Sonnet handle missing information when making tool calls?,0.0,0.0,0.0,True\n",
      "What steps should be taken to ensure a reliable deployment of an automated ticket routing system using Claude into a production environment?,0.6666666666666666,1.0,1.0,True\n",
      "How should you evaluate a model's performance on a ticket routing classifier?,0.3333333333333333,0.5,1.0,True\n",
      "What two methods does Anthropic recommend for learning how to prompt engineer with Claude before diving into the techniques?,0.3333333333333333,0.5,1.0,False\n",
      "What are the key differences between a pretrained large language model and Claude in terms of their training and capabilities?,0.6666666666666666,1.0,0.5,True\n",
      "What are some key advantages of using prompt engineering instead of fine-tuning to adapt a pretrained language model for a specific task or domain?,0.3333333333333333,0.3333333333333333,1.0,True\n",
      "How can you authenticate with GCP before running requests to access Claude models on Vertex AI?,0.6666666666666666,1.0,1.0,True\n",
      "\"What new capabilities and features were introduced by Anthropic on May 10th, 2024 and how do they enable users to create and tailor prompts for specific tasks?\",0.3333333333333333,1.0,0.5,True\n",
      "On what date did both the Claude 3.5 Sonnet model and the Artifacts feature in Claude.ai become available?,0.6666666666666666,1.0,1.0,True\n",
      "\"When putting words in Claude's mouth to shape the response, what header and value can you use in the request to limit Claude's response to a single token?\",0.3333333333333333,0.5,1.0,True\n",
      "What does the temperature parameter do when working with large language models?,0.3333333333333333,0.5,1.0,True\n",
      "What are two ways to specify API parameters when calling the Claude API using Claude for Sheets?,0.3333333333333333,0.3333333333333333,0.5,True\n",
      "How does prefilling the response with an opening curly brace ({ ) affect Claude's output when extracting structured data from text?,0.0,0.0,0.0,True\n",
      "What are some helpful resources provided by Anthropic to dive deeper into building with images using Claude?,0.0,0.0,0.0,True\n",
      "How do you specify the API key when creating a new Anthropic client in the Python and TypeScript SDK examples?,0.0,0.0,0.0,True\n",
      "What are two key benefits of using the Anthropic Evaluation tool when developing prompts for an AI classification application?,0.3333333333333333,0.5,1.0,True\n",
      "\"What are the key differences between a pretrained language model like Claude's underlying model, and the final version of Claude available through Anthropic's API?\",0.0,0.0,0.0,False\n",
      "What is the IPv6 address range used by Anthropic?,0.3333333333333333,1.0,0.5,True\n",
      "\"When using the Python SDK to create a message with Claude, what are two ways you can specify your API key?\",0.0,0.0,0.0,True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat evaluation/csvs/evaluation_results_detailed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
