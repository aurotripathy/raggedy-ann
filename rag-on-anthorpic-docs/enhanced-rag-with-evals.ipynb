{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4834a618-cb3c-4f71-b4f1-a18063afcc11",
   "metadata": {},
   "source": [
    "# RAG Retrieval Enhanced with Document Summaries\n",
    "In this section, we'll implement an improved approach to our retrieval system by incorporating document summaries. Instead of embedding chunks directly from the documents, we'll create a concise summary for each chunk and use this summary along with the original content in our embedding process.\n",
    "\n",
    "This approach aims to capture the essence of each document chunk more effectively, potentially leading to improved retrieval performance.\n",
    "\n",
    "Key steps in this process:\n",
    "\n",
    "1. We load the original document chunks.\n",
    "2. For each chunk, we generate a 2-3 sentence summary using OpenAI (or an OpenAI compatible API).\n",
    "3. We store both the original content and the summary for each chunk in a new json file: data/anthropic_summary_indexed_docs.json\n",
    "\n",
    "This summary-enhanced approach is designed to provide more context during the embedding and retrieval phases, potentially improving the system's ability to understand and match the most relevant documents to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b80e4-3558-445c-a17c-5a4b8db4cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## silent setup (-q), may take a while\n",
    "!pip install openai -q\n",
    "!pip install --upgrade tiktoken -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q\n",
    "!pip install seaborn -q\n",
    "!pip install -U scikit-learn -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af37b9a7-0878-4b8d-ae76-ad694cb512dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "embeddings_model = \"intfloat/multilingual-e5-large-instruct\"; generation_model = \"gpt-4o-mini\"; judge_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d1e786-d81b-411c-b2e0-8d618a5f5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "# print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262fa9ab-559b-41be-9de3-4ae757c2fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vast-jupyter/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6d08b86d9742bca5de1f6c9a30729c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e736fb1cb2b4422b75cea86e473522a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52812412ad9945789858deabb122c4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/140k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e45f21e44ed48be9f102f37758fcc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e84cc3687ae4f8ca1daf377d1502ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcc8db1cffb438abbccca144536ea32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd33906682a4a4d85e9e248d72f9ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fa5c62184f437cbe561dad290a8bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a635a54a99204daf97287f35d3183b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b321f7ad086444d8b01b6d2d8798f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddings_model = SentenceTransformer(embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54311b-bba4-40d4-b4fe-f17e54991e10",
   "metadata": {},
   "source": [
    "### Generating the Summaries and Storing Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b741b86c-f53e-4220-9c5b-bbbe6b7db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, this is for Claud-3-haiku, need to be changed to OpenAI or Llama\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summaries(input_file, output_file):\n",
    " \n",
    "    # Load the original documents\n",
    "    with open(input_file, 'r') as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    # Prepare the context about the overall knowledge base\n",
    "    knowledge_base_context = \"This is documentation for Anthropic's, a frontier AI lab building Claude, an LLM that excels at a variety of general purpose tasks. These docs contain model details and documentation on Anthropic's APIs.\"\n",
    "\n",
    "    summarized_docs = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Generating summaries\"):\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with creating a short summary of the following content from Anthropic's documentation. \n",
    "\n",
    "        Context about the knowledge base:\n",
    "        {knowledge_base_context}\n",
    "\n",
    "        Content to summarize:\n",
    "        Heading: {doc['chunk_heading']}\n",
    "        {doc['text']}\n",
    "\n",
    "        Please provide a brief summary of the above content in 2-3 sentences. The summary should capture the key points and be concise. We will be using it as a key part of our search pipeline when answering user queries about this content. \n",
    "\n",
    "        Avoid using any preamble whatsoever in your response. Statements such as 'here is the summary' or 'the summary is as follows' are prohibited. You should get straight into the summary itself and be concise. Every word matters.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=150,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        summary = response.content[0].text.strip()\n",
    "\n",
    "        summarized_doc = {\n",
    "            \"chunk_link\": doc[\"chunk_link\"],\n",
    "            \"chunk_heading\": doc[\"chunk_heading\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"summary\": summary\n",
    "        }\n",
    "        summarized_docs.append(summarized_doc)\n",
    "\n",
    "    # Save the summarized documents to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(summarized_docs, f, indent=2)\n",
    "\n",
    "    print(f\"Summaries generated and saved to {output_file}\")\n",
    "\n",
    "# generate_summaries('data/anthropic_docs.json', 'data/anthropic_summary_indexed_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cfea-b81d-4d06-9c8f-337f9ecc95c9",
   "metadata": {},
   "source": [
    "### Summary-Enhanced Vector Database Creation (heading + summary + chunk)\n",
    "Here, we're creating a new vector database that incorporates our summary-enhanced document chunks. This approach combines the original text, the chunk heading, and the newly generated summary into a single text for embedding.\n",
    "\n",
    "Key features of this process:\n",
    "\n",
    "1. We create embeddings for the combined text (heading + summary + original content) using the Voyage AI API.\n",
    "2. The embeddings and full metadata (including summaries) are stored in our vector database.\n",
    "3. We implement caching mechanisms to improve efficiency in repeated queries.\n",
    "4. The database is saved to disk for persistence and quick loading in future sessions.\n",
    "\n",
    "This summary-enhanced approach aims to create more informative embeddings, potentially leading to more accurate and contextually relevant document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351e638a-d09a-4295-8de6-6c54ac6e38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class SummaryEnhancedVectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/summary_indexed_vector_db.pkl\"\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        \"\"\"not called for now\"\"\"\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "        \n",
    "    def load_data(self, data_file):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Loading vector database from file: {self.db_path}.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "            \n",
    "        # well, if not...\n",
    "        print(f'file {self.db_path} does not exist')\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        texts = [f\"{item['chunk_heading']}\\n\\n{item['text']}\\n\\n{item['summary']}\" for item in data]  # Embed Chunk Heading + Text + Summary Together\n",
    "        # Embed more than 128 documents with a for loop\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            embeddings_model.encode(texts[i : i + batch_size])\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data  # Store the entire item as metadata\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=3, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            # print(f'found in cache!')\n",
    "            query_embedding = np.array(self.query_cache[query])  #\n",
    "            # print(f'type:{type(query_embedding)}')\n",
    "        else:\n",
    "            query_embedding = embeddings_model.encode(query)\n",
    "            print(f'query embedding:\\n {query_embedding}')\n",
    "            self.query_cache[query] = query_embedding.tolist()\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        # self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        print(f'Saving DB in: {self.db_path}')\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c306c5-1d54-4ca0-9a14-4947caae1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 3 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=3):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4e912-1419-4078-a063-c83d30b9a9de",
   "metadata": {},
   "source": [
    "### Enhanced Retrieval Using Summary-Enhanced Embeddings\n",
    "In this section, we implement the retrieval process using our new summary-enhanced vector database. This approach leverages the enhanced embeddings we created, which incorporate document summaries along with the original content.\n",
    "\n",
    "Key aspects of this updated retrieval process:\n",
    "\n",
    "1. We search the vector database using the query embedding, retrieving the top k most similar documents.\n",
    "2. For each retrieved document, we include the chunk heading, summary, and full text in the context provided to the LLM.\n",
    "3. This enriched context is then used to generate an answer to the user's query.\n",
    "\n",
    "By including summaries in both the embedding and retrieval phases, we aim to provide the LLM with a more comprehensive and focused context. This could potentially lead to more accurate and relevant answers, as the LLM has access to both a concise overview (the summary) and the detailed information (the full text) for each relevant document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a875906-ca83-4bb2-bdce-d8508e45025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl does not exist\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n",
      "Vector database loaded and saved.\n",
      "Saving DB in: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl\n",
      "query embedding:\n",
      " [-0.00210894  0.00373058 -0.01384532 ... -0.03834594 -0.0512418\n",
      "  0.02659795]\n",
      "ith:0\n",
      " {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic', 'chunk_heading': 'How to get embeddings with Anthropic', 'text': 'How to get embeddings with Anthropic\\n\\n\\nAnthropic does not offer its own embedding model. One embeddings provider that has a wide variety of options and capabilities encompassing all of the above considerations is Voyage AI.\\nVoyage AI makes state-of-the-art embedding models and offers customized models for specific industry domains such as finance and healthcare, or bespoke fine-tuned models for individual customers.\\nThe rest of this guide is for Voyage AI, but we encourage you to assess a variety of embeddings vendors to find the best fit for your specific use case.\\n', 'summary': 'Anthropic does not offer its own embedding model. Voyage AI is recommended as a provider of state-of-the-art embedding models, including customized and fine-tuned options for specific use cases.'}, 'similarity': np.float32(0.88499635)}\n",
      "ith:1\n",
      " {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/intro-to-claude#model-options', 'chunk_heading': 'Model options', 'text': 'Model options\\n\\n\\nEnterprise use cases often mean complex needs and edge cases. Anthropic offers a range of models across the Claude 3 and Claude 3.5 families to allow you to choose the right balance of intelligence, speed, and cost.\\n', 'summary': 'Anthropic offers a range of Claude 3 and Claude 3.5 models to cater to the complex needs and edge cases of enterprise use cases, allowing users to choose the right balance of intelligence, speed, and cost.'}, 'similarity': np.float32(0.86139673)}\n",
      "ith:2\n",
      " {'metadata': {'chunk_link': 'https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings', 'chunk_heading': 'Before implementing embeddings', 'text': 'Before implementing embeddings\\n\\n\\nWhen selecting an embeddings provider, there are several factors you can consider depending on your needs and preferences:\\nDataset size & domain specificity: size of the model training dataset and its relevance to the domain you want to embed. Larger or more domain-specific data generally produces better in-domain embeddings\\nInference performance: embedding lookup speed and end-to-end latency. This is a particularly important consideration for large scale production deployments\\nCustomization: options for continued training on private data, or specialization of models for very specific domains. This can improve performance on unique vocabularies\\n', 'summary': 'When selecting an embeddings provider, consider the dataset size and domain specificity, inference performance, and customization options. Larger or more domain-specific training data, faster embedding lookup, and the ability to fine-tune models can improve the quality and relevance of the embeddings for your use case.'}, 'similarity': np.float32(0.8572912)}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "def retrieve_similar_level_two(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n <document> \\n {chunk['chunk_heading']}\\n\\nText\\n {chunk['text']} \\n\\nSummary: \\n {chunk['summary']} \\n </document> \\n\" #show model all 3 items\n",
    "    return results, context\n",
    "\n",
    "def construct_prompt(query, context):    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def answer_query_from_context_level_two(query, db):\n",
    "    documents, context = retrieve_similar_level_two(query, db)\n",
    "    completion = client.chat.completions.create(\n",
    "    model=generation_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": construct_prompt(query, context)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Initialize the SummaryIndexedVectorDB\n",
    "level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "level_two_db.save_db()\n",
    "\n",
    "# # Load the Anthropic documentation\n",
    "# with open('data/anthropic_docs.json', 'r') as f:\n",
    "#     anthropic_docs = json.load(f)\n",
    "\n",
    "# test\n",
    "query = \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\"\n",
    "test_results, test_contexts = retrieve_similar_level_two(query, level_two_db)\n",
    "for i, test_result in enumerate(test_results):\n",
    "    print(f'ith:{i}\\n {test_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daa157-bd0c-4462-be43-f2a7d1f06bc4",
   "metadata": {},
   "source": [
    "### Defining Our Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51398e7-2da9-47ca-90f8-e4e565f6108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"For OpenAI models, returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db) # ??\n",
    "        \n",
    "        comparision_prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format (don't prefix with xml):\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_tokens = num_tokens_from_string(comparision_prompt, \"o200k_base\")  # note, this encoding name for gpt-4o, gpt-4o-mini\n",
    "        print(f'Number of tokens: {nb_tokens}')\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": comparision_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            response_text = str(response.choices[0].message.content)\n",
    "            print(f'Query:\\n{query}')\n",
    "            print(f'Correct answer:\\n{correct_answer}')\n",
    "            print(f'Generated anser:\\n{generated_answer}')\n",
    "            print(f'Response_text from judge LLM:\\n{response_text}')\n",
    "            \n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct_value = evaluation.find(\".//is_correct\").text\n",
    "            \n",
    "            is_correct = is_correct_value == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8852c4-15df-422c-bdb7-cccf25f8da08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector database from file: ./data/anthropic_docs_v2/summary_indexed_vector_db.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query embedding:\n",
      " [ 0.01853994 -0.02051672 -0.02005863 ... -0.04795236 -0.04596517\n",
      " -0.00204474]\n",
      "query embedding:\n",
      " [-0.00210894  0.00373058 -0.01384532 ... -0.03834594 -0.0512418\n",
      "  0.02659795]\n",
      "query embedding:\n",
      " [-0.01123418  0.00973828 -0.03412096 ... -0.02513294 -0.02512304\n",
      "  0.02933701]\n",
      "query embedding:\n",
      " [ 0.006573    0.02653212 -0.01462308 ... -0.03698858 -0.01842642\n",
      "  0.00694131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 5/5 [00:00<00:00, 40.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query embedding:\n",
      " [ 0.01077393  0.02323065 -0.0085004  ... -0.03344801 -0.04153727\n",
      "  0.00997741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 1/5 [00:07<00:30,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\n",
      "Correct answer:\n",
      "To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\n",
      "Generated anser:\n",
      "To create multiple test cases for evaluation in the Anthropic Evaluation tool, follow these steps:\n",
      "\n",
      "1. **Access the Evaluation Tool**: Open the Anthropic Console and navigate to the prompt editor. After composing your prompt, look for the ‘Evaluate’ tab at the top of the screen.\n",
      "\n",
      "2. **Prepare Your Prompt**: Ensure your prompt includes at least 1-2 dynamic variables using the double brace syntax (e.g., {{variable}}). This is necessary for creating evaluation test sets.\n",
      "\n",
      "3. **Add Test Cases**: In the Evaluation screen, you will see a single row for your test case. To add more test cases, click the ‘Add Test Case’ button.\n",
      "\n",
      "4. **Fill in Test Case Values**: For each new test case, fill in the values for each variable in your prompt. Repeat this process to create multiple scenarios.\n",
      "\n",
      "5. **Update and Re-run Evaluations**: If you need to update your original prompt text, you can do so and then re-run the entire evaluation suite against the new prompt to see how changes affect performance across all test cases.\n",
      "\n",
      "By following these steps, you can effectively create and manage multiple test cases for your evaluation.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it accurately describes the process of creating multiple test cases in the Anthropic Evaluation tool. It includes all the necessary steps: accessing the tool, preparing the prompt with dynamic variables, adding test cases, filling in values, and updating evaluations. These steps align well with the Correct Answer, which also emphasizes clicking the 'Add Test Case' button and filling in values for each variable. There are no critical pieces of information missing, and there are no contradictions. Therefore, the Generated Answer is deemed correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 2/5 [00:10<00:14,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\n",
      "Correct answer:\n",
      "Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\n",
      "Generated anser:\n",
      "Anthropic recommends Voyage AI as the embeddings provider for customized domain-specific models. Voyage AI offers state-of-the-art embedding models and provides options for customized models tailored to specific industry domains, such as finance and healthcare, as well as bespoke fine-tuned models for individual customers.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately reflects the information provided in the correct answer. Both answers mention that Anthropic recommends Voyage AI for embedding models and highlight the customization options available for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. There are no critical pieces of information missing, and there are no contradictions. Therefore, the generated answer is correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 3/5 [00:18<00:12,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\n",
      "Correct answer:\n",
      "When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\n",
      "Generated anser:\n",
      "Key success metrics to consider when evaluating Claude's performance on a classification task include:\n",
      "\n",
      "1. **Accuracy**: Measures the proportion of correct predictions made by the model. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
      "\n",
      "2. **F1 Score**: Balances precision and recall, providing a single score that reflects both the model's ability to correctly identify positive cases and its ability to avoid false positives.\n",
      "\n",
      "3. **Consistency**: Assesses whether the model produces similar outputs for similar inputs, indicating reliability in its predictions.\n",
      "\n",
      "4. **Structure**: Evaluates if the model's output adheres to the expected format, which is crucial for ease of interpretation and integration into systems.\n",
      "\n",
      "5. **Speed**: Measures the response time of the model, which is critical for applications requiring low latency. This metric directly influences the choice of model, as faster models can enhance user experience.\n",
      "\n",
      "6. **Bias and Fairness**: Important for ensuring that the model does not exhibit biases based on sensitive attributes, which can lead to ethical concerns and misclassification.\n",
      "\n",
      "When choosing the right model to reduce latency, it is essential to consider the trade-offs between speed, accuracy, and cost. Smaller models like Claude 3 Haiku are typically preferred for their speed and efficiency, making them suitable for classification tasks where quick responses are necessary. However, for tasks requiring more complex reasoning or specialized knowledge, larger models like Sonnet may be more appropriate, despite their higher latency and cost. Thus, the selection of the model should align with the specific requirements of the classification task, balancing the need for speed with the desired accuracy and complexity.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer accurately lists key success metrics for evaluating Claude's performance on a classification task, including accuracy, F1 score, consistency, structure, speed, and bias and fairness. It also discusses the importance of choosing the right model to reduce latency while balancing speed, accuracy, and cost. The mention of smaller models like Claude 3 Haiku for speed and larger models like Sonnet for complexity aligns with the correct answer's emphasis on model selection based on specific requirements. Therefore, the generated answer captures the essential points and does not omit any critical information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 4/5 [00:24<00:06,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What are two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts?\n",
      "Correct answer:\n",
      "Claude for Sheets enables testing prompts across evaluation suites in parallel, which is faster than running chained prompts sequentially. It also excels at office tasks like survey analysis and online data processing that may be more cumbersome with chained prompts.\n",
      "Generated anser:\n",
      "Claude for Sheets can improve prompt engineering workflows compared to using chained prompts in the following ways:\n",
      "\n",
      "1. **Parallel Testing of Prompts**: Claude for Sheets allows users to test multiple prompts across evaluation suites simultaneously. This capability enables more efficient experimentation and optimization of prompts, as users can quickly compare results and refine their approaches without the need to sequentially chain prompts.\n",
      "\n",
      "2. **Centralized Resource for Learning**: The Claude for Sheets prompting examples workbench serves as a centralized resource that houses example prompts and prompt engineering structures. This makes it easier for users to explore and learn about effective prompt engineering techniques, as opposed to relying on individual chained prompts that may not provide comprehensive insights or examples.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly identifies two ways that Claude for Sheets can improve prompt engineering workflows compared to using chained prompts: the ability to test prompts in parallel and the centralized resource for learning. However, it misses the specific mention of office tasks like survey analysis and online data processing, which is a critical aspect of the correct answer. Therefore, while the generated answer contains valid points, it lacks a key piece of information that makes it incomplete. </explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Number of tokens: 353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 5/5 [00:27<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What happens if a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns?\n",
      "Correct answer:\n",
      "If a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error.\n",
      "Generated anser:\n",
      "If a prompt for the Text Completions API is missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. The API requires these specific markers to properly identify the roles of the participants in the conversation. Without them, the prompt does not conform to the expected format, leading to an error in processing the request.\n",
      "Response_text from judge LLM:\n",
      "<evaluation>\n",
      "<content>\n",
      "<explanation>The generated answer correctly states that if a prompt for the Text Completions API is missing the required \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns, it will result in an API error. It also provides additional context about the necessity of these markers for identifying roles in the conversation, which aligns with the correct answer. Therefore, the generated answer is correct as it conveys the same essential information without omitting any critical details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "</evaluation>\n",
      "Detailed results saved to evaluation/csvs/evaluation_results_detailed.csv\n",
      "Average Precision: 0.4667\n",
      "Average Recall: 0.7000\n",
      "Average MRR: 0.8000\n",
      "Average F1: 0.5600\n",
      "End-to-End Accuracy: 0.8000\n",
      "Evaluation complete. Results saved to evaluation/json_results/evaluation_results_one.json, evaluation/csvs/evaluation_results_detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "level_two_db = SummaryEnhancedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the evaluations\n",
    "eval_data_range = eval_data[:5]\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_similar_level_two, eval_data_range, level_two_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_from_context_level_two, level_two_db, eval_data_range)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data_range],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "csv_dir = Path('evaluation/csvs')\n",
    "csv_file_name = Path('evaluation_results_detailed.csv')\n",
    "df.to_csv(csv_dir / csv_file_name, index=False)\n",
    "print(f\"Detailed results saved to {csv_dir/ csv_file_name}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "# Save the results to a json file\n",
    "json_dir = Path(\"evaluation/json_results\")\n",
    "result_file_name = Path(\"evaluation_results_one.json\")\n",
    "Path(json_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open('evaluation/json_results/evaluation_results_level_two.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Indexing\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {json_dir / result_file_name}, {csv_dir/ csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a0a91-3a1e-4b67-90e5-b38bbda11d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea28c0-4e1c-4398-ac76-0739a0a07553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
